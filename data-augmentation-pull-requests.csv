text;classification
"+1
Thanks @aokolnychyi!";non_debt
"ekhacoop@ welcome to the Apache Fineract community! It looks like you are trying to make your first contribution to this Git repository? However, this Pull Request is ""empty"" - there are no ""Files Changed"" (above).  I'm therefore closing this.  We look forward to receiving more contributions from you in the future!";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/3/;non_debt
Like setup, can probably do without the comments;non_debt
"can you change these params to `maxFailedTasks` and `maxBlacklistedExecutors`, and change the comment to:
This strategy adds an executor to the blacklist for _all_ tasks when the executor has too many task failures.  An executor is placed in the blacklist when there are more than [[maxFailedTasks]] failed tasks.  Furthermore, all executors in one node are put into the blacklist if there are more than [[maxBlacklistedExecutors]] blacklisted executors on one node.  The benefit of this strategy is that different taskSets can learn experience from other taskSet to avoid allocating tasks on problematic executors.";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/951/;non_debt
"  [Test build #24335 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/24335/consoleFull) for   PR 3649 at commit [`26aa58f`](https://github.com/apache/spark/commit/26aa58f951df85c279da844775dbee9d7a0953ff).
- This patch **passes all tests**.
- This patch merges cleanly.
- This patch adds the following public classes _(experimental)_:
  - `implicit class StringToSymbol(val sc: StringContext) extends AnyVal`";non_debt
run cpp tests;non_debt
" * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds the following public classes _(experimental)_:
  * `trait DataWritingCommand extends Command `
  * `case class DataWritingCommandExec(cmd: DataWritingCommand, children: Seq[SparkPlan])`";non_debt
@ankkhedia Is this good to go?;non_debt
"Okay, I get you. 
Wonder if we can find a solution in between. For eg. UUIDString is not a typedef, but publicly inherits SmallString, constructors are private and ID (where we create those) is a friend class?";non_debt
"# [Codecov](https://codecov.io/gh/apache/hudi/pull/1469?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/hudi/pull/1469?src=pr&el=continue).";non_debt
"In file included from ../src/kvstore/kvstore.cc:28:
../src/kvstore/./kvstore_local.h:281:23: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
    auto validator = [this](const int key, const NDArray& nd, bool ignore_sparse) -> bool {
                      ^
../src/kvstore/./kvstore_local.h:326:23: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
    auto validator = [this](const int key, const RSPVal& val_rowid, bool ignore_sparse) -> bool {
                      ^
In file included from ../src/c_api/c_api_profile.cc:35:
../src/c_api/../profiler/./profiler.h:1160:8: warning: 'mxnet::profiler::ProfileOperator::start' hides overloaded virtual function [-Woverloaded-virtual]
  void start(mxnet::Context::DeviceType dev_type, uint32_t dev_id) {
       ^
../src/c_api/../profiler/./profiler.h:870:8: note: hidden overloaded virtual function 'mxnet::profiler::ProfileEvent::start' declared here: different number of parameters (0 vs 2)
  void start() override {
       ^
../src/c_api/../profiler/./profiler.h:1212:8: warning: lambda capture 'this' is not used [-Wunused-lambda-capture]
      [this](OprExecStat *stat) {}, name_.c_str(), dev_type_, dev_id_,
       ^
See also #14940";non_debt
"@dongjoon-hyun, the problem that you pointed out was that the schema shouldn't require a reader. 
That's what I'm saying here, too: the table should have a schema and it shouldn't need to implement the read path to validate a write using the table schema.";non_debt
"GroupBy v2 doesn't cache on the broker, so it isn't actually testing
what the test was supposed to be testing. Also, the test failed due
to mismatched expectations.";non_debt
Use the Python Postinstall implementation by default;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/<h2>Failed Tests: <span class='status-failure'>1</span></h2><h3><a name='beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark' /><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/org.apache.beam$beam-runners-spark/testReport'>beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark</a>: <span class='status-failure'>1</span></h3><ul><li><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8052/org.apache.beam$beam-runners-spark/testReport/org.apache.beam.runners.spark.translation.streaming/ResumeFromCheckpointStreamingTest/testWithResume/'><strong>org.apache.beam.runners.spark.translation.streaming.ResumeFromCheckpointStreamingTest.testWithResume</strong></a></li></ul>
--none--";non_debt
"@ijuma i checked the cases where this test has failed and it seems to always be on the verification of the left join. I've ran this test plenty of times and i can't get it to fail. However in the interest of having stable builds, i've removed just the part of the test that is failing (which happens to be the last verification).
Thanks,
Damian";non_debt
.. except of course that when suspending a machine `JcloudsLocation` does not call the pre- and post-release methods of any attached `JcloudsLocationCustomizers`. I think this is fair given that the machine may be resumed.;non_debt
retest this please;non_debt
What is this for? Is `docs/_config_local.yml` used anywhere?;non_debt
Same about the log level.;non_debt
Fixed;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/4250?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/4250?src=pr&el=continue).";non_debt
There's definitely a more panda-esque way to do all this, but it's not shocking as is.;non_debt
Fixed;non_debt
issue is  https://github.com/apache/rocketmq/issues/375;non_debt
"This PR allows reporters to define their default delimiter. Previously we always defaulted to `.`.
Reporters may now implement the `DelimiterProvider` interface with which they may define a delimiter. This delimiter is used if no delimiter was explicitly configured for the reporter.
* Add DelimiterProvider interface
* modify Prometheus&JmxReporter to define the default delimiter
* modify delimiter setup in the MetricRegistry
* add a test to the MetricRegistryTest to verify functionality
This change added tests and can be verified as follows:
Run MetricRegistryTest#testDelimiterOverride.";non_debt
Any other comment? @cloud-fan Thanks!;non_debt
"This doesn't pass the system tests on my machine, hanging very early (with nginx-1.4.4):
With curl:
Requesting with `PageSpeed=off` on the querystring seems to works OK.
Requesting another url, `curl --raw -vv http://localhost:8050/mod_pagespeed_example/`
shows something else which looks concerning:
@jeffkaufman Can you reproduce this (perhaps with nginx-1.4.4)?";non_debt
Done.;non_debt
â€¦ow for updated data row;non_debt
"hi, i am using the dependency is
`
`
,which had been solved the bug?
Look forward to your favourable reply.";non_debt
Merge pull request #2 from apache/master;non_debt
/pulsarbot run-failure-checks;non_debt
"example ? Instead of exmaple ?
Message should say build with example external runtime and not relay vm profiler support .";non_debt
"@JoshRosen @MLnick the batch size here only affects transient data when writing and the re-serialization shouldn't be done if the data is already in pickle format (batch serialized or not). I'm uploading a patch to that effect. Since the batch size has no effect on the data persisted in files, I'm not exposing it to users. See you have any further comments and if you have a better suggestion for the default size 10, let me know.
@mateiz what you suggested is exposing the batch size for reading. I'll work on that next.";non_debt
NIFI-4516 FetchSolr Processor;non_debt
Agree with @ephraimbuddy , you are missing `.filter(DagRun.dag_id == dag_id)` that Ephraim suggested;non_debt
Thank you for the review. I'll try to address them tomorrow!;non_debt
done;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Fixed.  Re-ran tests.  Thanks.;non_debt
17165658-31052 review-553663753;non_debt
Why was this removed?;non_debt
revert this plesae;non_debt
Equivalent `for` loop takes fewer lines.;non_debt
cc @wuchong;non_debt
"@graceguo-supercat noticed `black==19.10b0` and `black==19.3b0` is treating function trailing commas differently. This [upgrades black to `19.10b0`](https://github.com/psf/black/compare/19.3b0...19.10b0) to bring in the fix (https://github.com/psf/black/pull/763).
Mypy is also updated because it's a dependency of black.
N/A
CI Successful build.
N/A
@graceguo-supercat @john-bodley";non_debt
cc/ @vanzin @ueshin @jiangxb1987 @erikerlandson @liyinan926;non_debt
fixed.;non_debt
36057260-1000 summary-0;non_debt
STORM-3632 reduce SimpleSaslServerCallbackHandler logging;non_debt
Merged build started.;non_debt
"  [Test build #31992 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/31992/consoleFull) for   PR 5867 at commit [`490d778`](https://github.com/apache/spark/commit/490d778b16934984acc577d19787268958ce5fd2).
- This patch **fails Spark unit tests**.
- This patch **does not merge cleanly**.
- This patch adds no public classes.";non_debt
"Hahaha, the bike shedder arrives!
I'm not sure. Is it worth the set up for a single file?
In any case, I don't like the "".a"" approach. If you want a change, I would create a header file, make the `Overridable_Map` not static, and grab the file via ""Makefile.inc"" reaching over to ""../shared/txn-vars.cc"".
What would the directory structure be?
-shared
--src
---txn_vars.cc
--include
---txn_vars.h
or
-shared
--txn_vars.cc
--include
---txn_vars.h";non_debt
ping @HyukjinKwon, @gatorsmile;non_debt
+1.;non_debt
Run Java PreCommit;non_debt
[SPARK-22665][SQL] Avoid repartitioning with empty list of expressions;non_debt
"Congratulations on your first Pull Request and welcome to the Apache Airflow community! If you have any issues or are unsure about any anything please check our Contribution Guide (https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst)
Here are some useful points:
- Pay attention to the quality of your code (flake8, pylint and type annotations). Our [pre-commits]( https://github.com/apache/airflow/blob/master/STATIC_CODE_CHECKS.rst#prerequisites-for-pre-commit-hooks) will help you with that.
- In case of a new feature add useful documentation (in docstrings or in `docs/` directory). Adding a new operator? Check this short [guide](https://github.com/apache/airflow/blob/master/docs/howto/custom-operator.rst) Consider adding an example DAG that shows how users should use it.
- Consider using [Breeze environment](https://github.com/apache/airflow/blob/master/BREEZE.rst) for testing locally, itâ€™s a heavy docker but it ships with a working Airflow and a lot of integrations.
- Be patient and persistent. It might take some time to get a review or get the final approval from Committers.
- Be sure to read the [Airflow Coding style]( https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#coding-style-and-best-practices).
Apache Airflow is a community-driven project and together we are making it better ðŸš€.
Mailing List: dev@airflow.apache.org
Slack: https://apache-airflow-slack.herokuapp.com/";non_debt
"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/24334/
Test FAILed.";non_debt
Why not do the conversion while inserting?;non_debt
the `* 1000` should be inside the bracket.;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
How is it done on the client side?;non_debt
@jiangxb1987 feel free to take a look at it. More eyes, more possibilities.;non_debt
"@vik5jagan yup. but this pull request seems to try to merge branch 4.0 to master. is that intended? There is a link describing how to contribute to BookKeeper : https://cwiki.apache.org/confluence/display/BOOKKEEPER/Contributing+to+BookKeeper .
Let me know if you have any questions.";non_debt
"  [QA tests have finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18681/consoleFull) for   PR 1819 at commit [`41ebc5f`](https://github.com/apache/spark/commit/41ebc5f912093fdf7b21808ce19da1bae514435e).
- This patch **fails** unit tests.
- This patch merges cleanly.
- This patch adds the following public classes _(experimental)_:
  - `abstract class Serializer`
  - `abstract class SerializerInstance`
  - `abstract class SerializationStream`
  - `abstract class DeserializationStream`
  - `class ShuffleBlockManager(blockManager: BlockManager,`
  - `case class OutputFaker(output: Seq[Attribute], child: SparkPlan) extends SparkPlan`
  - `implicit class LogicalPlanHacks(s: SchemaRDD)`
  - `implicit class PhysicalPlanHacks(originalPlan: SparkPlan)`
  - `class FakeParquetSerDe extends SerDe`";non_debt
"True, time unit won't be set as well.
The new code doesn't have the requirement of setting start/end before time unit.";non_debt
"# [Codecov](https://codecov.io/gh/apache/beam/pull/13905?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/beam/pull/13905?src=pr&el=continue).";non_debt
I agree that it should be configurable, but I don't think using a Builder pattern precludes this.  I'm about to push changes that adds support for using system properties, that's a good idea.;non_debt
R:@kennknowles;non_debt
@zentol the result need be sorted, otherwise the test would report error, because the result is not ordered, print this :;non_debt
updated copyright in heron/cli;non_debt
"1- Rebased and ran the tests again and updated the test results.
2- Done.";non_debt
Compat for opencl mode between cpu mode and gpu mode;non_debt
Hoping to remove this try/catch if [NIFI-4590](https://issues.apache.org/jira/browse/NIFI-4590) is implemented first.;non_debt
I understand the motivation here, could you show the benefit of this change for a real use case?;non_debt
Better to comment why `-3` needs a special process.;non_debt
change the url in index;non_debt
Fix broken unit tests and run go fmt;non_debt
If the PR isn't ready to be reviewed, why is it open?;non_debt
Sounds good to me.;non_debt
I think this can be no-op, since `RollbackOnly` is always `true`.;non_debt
I've rebased the patch on the current master. If there are no objects, I'd like to merge to master tonight or tomorrow.;non_debt
It does not. My initial implementation required this, then I forgot to change this back;non_debt
" * This patch **fails to generate documentation**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
STORM-3247 remove BLOBSTORE_SUPERUSER;non_debt
"[CALCITE-4106] Consider ""listCoerced"" in TypeCoercionImpl#inOperationCoercion (Jiatao Tao)";non_debt
"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 7f2e408128e5d7929dd4301362a20d8d90c69597 (Sat Mar 28 14:50:34 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* â“ 1. The [description] looks good.
* â“ 2. There is [consensus] that the contribution should go into to Flink.
* â“ 3. Needs [attention] from.
* â“ 4. The change fits into the overall [architecture].
* â“ 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier";non_debt
@eolivelli left my comments.;non_debt
" * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
"This PR contains the following updates:
---
-   **ux:** Disabled Execute button while request is in progress ([#&#82036776](https://togithub.com/swagger-api/swagger-ui/issues/6776)) ([2bf39e0](https://togithub.com/swagger-api/swagger-ui/commit/2bf39e0ad526336198490122c7978221487a1e35))
-   **sample-gen:** first oneOf or anyOf should be combined with schema ([#&#82036775](https://togithub.com/swagger-api/swagger-ui/issues/6775)) ([0f541a1](https://togithub.com/swagger-api/swagger-ui/commit/0f541a1ab055ce104da9758ddbf1a77dcd839c70))
-   **style:** response data flows off the screen ([#&#82036764](https://togithub.com/swagger-api/swagger-ui/issues/6764)) ([85a3ec9](https://togithub.com/swagger-api/swagger-ui/commit/85a3ec983e5822f87715d63720e511f8be65f6a9))
-   **examples:** Request Body examples should respect media-type ([#&#82036739](https://togithub.com/swagger-api/swagger-ui/issues/6739))  ([68e9b1b](https://togithub.com/swagger-api/swagger-ui/commit/68e9b1b43995a68d40813146aab174228e7a1257))
---
:date: **Schedule**: At any time (no schedule defined).
:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.
:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.
:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.
---
---
This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#github/apache/fineract).";non_debt
This documents the implementation of ALS in `spark.ml` with example code in scala, java and python.;non_debt
Pushed small refactors. Will merge once CI passes;non_debt
Hmm maybe such a type converter is not so idea when you require those stuff to be on the exchange property. Sometimes a type converter is not the best thing to add, they should really ideally just be for basic type conversions. Consider removing this if possible.;non_debt
20675636-466 summary-0;non_debt
"Explicit max and min heap size settings removed.
This file (changed in this PR) is the default `.conf` file your application will get when you create an application on top of NetBeans Platform.
See [NETBEANS-149](https://issues.apache.org/jira/browse/NETBEANS-149) for details.";non_debt
GEODE-1744: Probable bugs from == use fixed;non_debt
rebase latest master;non_debt
I would prefer `OZONE_SH_OPTS`.  However, this is an existing variable.  I kept its name to avoid breaking anyone's scripts.;non_debt
OK, Done;non_debt
Use `BDV.vertcat(BDV(Array(gradientLogSigmaSum / totalCnt.toDouble)), BDV(Array(gradientInterceptSum/totalCnt.toDouble)), gradientBetaSum/totalCnt.toDouble)` to combine three parts directly.;non_debt
"  [Test build #23698 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/23698/consoleFull) for   PR 3009 at commit [`6f17f3f`](https://github.com/apache/spark/commit/6f17f3f61102f5685d20cf42f79a049a5bbaad06).
- This patch **fails Spark unit tests**.
- This patch merges cleanly.
- This patch adds no public classes.";non_debt
Jenkins, retest this please;non_debt
@msb-at-yahoo - Addressed all your comments;non_debt
[Dubbo-1659] Optimize_hessian_desr_performance;non_debt
LGTM, will test and merge;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/3969?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/3969?src=pr&el=continue).";non_debt
@BLasan a thought: instead of using `busybox` and `nc`, why not just re-use `image: mysql:5.7` with `mysqladmin` ?;non_debt
Rename TableSegmentsAvailable to TableAvailable;non_debt
FreeBSD build *successful*! See https://ci.trafficserver.apache.org/job/freebsd-github/1602/ for details.;non_debt
@kou there are a couple of missing artifacts:;non_debt
Ok, that makes sense!;non_debt
/pulsarbot run-failure-checks;non_debt
Thanks @xiaoyuyao for review. I will merge it in.;non_debt
Or we can merge this test case with existing read/write test.;non_debt
"About fine grained configuration:
Yes, users should not need to think about tasks. It should be on operator basis. Thus, we would need to make standby tasks ""smarter"" and only populate some stores (as requested by the user) in the background. Would be a bigger change, but not impossible.
Yes, that is exactly what I had in mind.";non_debt
CAMEL-15896: Upgrade google-cloud-pubsub to 1.109.0;non_debt
"@rdhabalia @saandrews Added 1 commit on top of this PR. Take a look at https://github.com/merlimat/pulsar/commit/08de7c7c47fdb242074b08ddbb81a3fae8b2f72b 
Few things: 
 1. Use proper ref-count sematic for EntryImpl. EntryImpl owns a ref of the buffer (so, while we have an `EntryImpl`, the buffer is alive.
 2. Removed recycling on PositionImpl, this was long due since it's impossible to achieve on the current premises. Use ledgerId/entryId in entryImpl so that is more compact";non_debt
so we will never delete a segment in realtime for 5 days?;non_debt
"It's part of Dataflow VR test. 
I usually change the validatesRunnerV2 target to include just one test case and if you have write access to apache-beam-testing, you can invoke the test locally by ./gradlew :runners:google-cloud-dataflow-java:validatesRunnerV2 directly.";non_debt
adding windowing function CLI option descriptions;non_debt
I think we can just use `spark`.;non_debt
"On second thought, passing in the runtime context wouldnâ€™t work. To let user custom partitioners  be stateful, weâ€™ll essentially need to make FlinkKafkaPartitioner a CheckpointedFunction, and let the FlinkKafkaProducerBase invoke the checkpoint methods.
We should be able to avoid breaking the user API by having empty base implementations for the checkpoint methods on the FlinkKafkaPartitioner.";non_debt
"This isn't actually a defect - it's true that 11 vs 10 is not perfect, but the reason for ceil/floor vs round is that for small number (say 2 invokers):
The point for small N up to the reciprocal of the fraction, both sets overlap.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/1057/";non_debt
Do we want Scala 2.12 or 2.13 here?;non_debt
Yes, I will be adding ADTs today, along with tests. Thanks for the reviews;non_debt
This pull request has been marked as stale due to 60 days of inactivity. It will be closed in 4 weeks if no further activity occurs. If you think that's incorrect or this pull request should instead be reviewed, please simply write any comment. Even if closed, you can still revive the PR at any time or discuss it on the dev@druid.apache.org list. Thank you for your contributions.;non_debt
"Yes, that's ok. We'll put a notice about a breaking change to the release notes, so that users having there own indexer plugin know they have to adapt it.
We could try to only extend the IndexWriter interface and provide default do-nothing implementations for newly added methods as most index writers do not write data to the filesystem.";non_debt
[SPARK-19748][SQL]refresh function has a wrong order to do cache invalidate and regenerate the inmemory var for InMemoryFileIndex with FileStatusCache;non_debt
"Per above, it may be worth writing a ""large memory"" test with the `large_memory` pytest mark (which we can run locally, but not in Travis CI) where we have a field that overflows the 2G in a BinaryArray so we can test the rechunking / splitting of the null bitmap. I guess you'll have to pass a mask to get some nulls to make sure the logic is correct";non_debt
[approve ci Clang-Analyzer];non_debt
https://issues.apache.org/jira/browse/ARROW-11259;non_debt
"I've updated the patch. I've improved TajoTestingCluster to take system properties as follows:
If `CODEGEN` is a session variable, it will be applied to QueryContext instance used in all unit tests. So, in order to test the code generation feature, you should give `-DCODEGEN=true` when you execute `mvn install`. It can be used for other session variables too.
For test for real queries, you need to set a session variable CODEGEN  as follows:";non_debt
"Anyone know what's happening with this:
@shaneknapp";non_debt
If we change `inputIter`'s initial value to `null`, we need to also change it to a null check.;non_debt
A friendly ping~;non_debt
70746484-7260 review-556711333;non_debt
"It looks like only a couple tests classes are using this-- should they all?
Note that the transforms and tests we have in the core SDK are likely to get copy-pasted many times over their life, for future SDK transforms and by users. Your implementation here establishes the recipe for how to implement and test display data.";non_debt
Coverage increased (+0.003%) to 70.148% when pulling **bc981235946ce768f0ab469922ded96e5a76d3c0 on mariapython:ppp_final** into **af8f586b60853056a20b08d88f7dca72eac657bc on apache:master**.;non_debt
"right now, it's not supporting wild-card char such as `*` but 
- consumer can subscribe to any subscription if consumer's principal/role is authorized at namespace-level (`consume`) or per-topic (`consume`).
- however, if per-subscription auth is configured for a specific subscription-name then consumer's `principal/role` has to be authorized role as per `per-subscription` to connect to that specific subscription. so, in `pre-fix sub` usecase, user can grant access to set of roles for a specific subscription and only those roles can access the subscription. so, we can avoid sub-name conflict.";non_debt
Changing it because this way the tense is also in sync with the rest of the headings;non_debt
"Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/35513/";non_debt
Done;non_debt
@XuQianJin-Stars Could you please solve the conflicts and squash your commits into a single commit?;non_debt
I think after addressing the comments I just added, it's probably good to go.  I still don't love the name, especially since it extends BoolFunction it ought to now end with Function.;non_debt
Can we file JIRA for this? This is a low priority to have any acls user may want to provide.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/2642?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/2642?src=pr&el=continue).";non_debt
cc @Huyuwei @merrymercy one thing I find is that it is a bit tricky to do topi dispatching in the schedule, do you think if it is possible to fold the depthwise schedule and compute directly into the topi function of conv2d?;non_debt
Just discovered that it has been fixed with #12479 already.;non_debt
@zjffdu @Leemoonsoo I have handled the cases in https://github.com/apache/zeppelin/pull/1678;non_debt
"Hi, @morningman I have some question about olap_scan_node, could you plz help me figure it out?
1. what's the relationship between scanner and tablet, one tablet can be scanned by many scanner?they are n:1 or 1:1?
2. what's the relationship between TPaloScanRange and ColumnValueRange, is ColumnValueRange to deal with predicate case and TPaloScanRange can indicate that the range a scanner need to scan from StorageEngine's prefix index?
3. What's the usage of function `extend_scan_key` and the config variable `doris_scanner_row_num`?
I would appreciate your reply, thanks~";non_debt
[AIRFLOW-1236] SlackPostOperator using Slack Incoming WebHook;non_debt
"rdd.id() was returning an Attribute Error in some cases because self._id is not getting set.  So instead of returning the _id attribute, return the value of id() from the jrdd.  Fixes bug SPARK-2334.
Test with: sc.parallelize([1,2,3]).map(lambda x: x+1).id()";non_debt
20587599-10999 review-374455972;non_debt
@kiszk . Could you run for all?;non_debt
"It's an arbitrary name we give to the jQuery v3.4.1 we imported. It's like a namesapce. Note the magic happens in `window.jquery341 = jQuery.noConflict(true)`.
The problem here is that:
1. A frontend can connect to the kernel at any time: code executed by kernel in the past does not have any effect to new frontends.
2. Multiple frontends can connect to the same kernel: each frontend has its own state (browser: HTML and JS), the rendered HTML+JS cannot assume the existence of any global variable, function definition or libraries.
This ensures no matter how many jQuery gets imported at any time, the interactive notebook always checks and uses the single jQuery configured by interactive modules with Datatable plugin initialized.
And the `function($)` signature ensures that any customized script executed will use `$` as the singleton instance  `window.jquery341`. This ensures that code reading `$` as jQuery will always work.
The advantage of doing this isolation is:
1. The JS imported by interactive modules to any frontend does not alter their existing states. Everything in the notebook still works as it was no matter what libraries and global vars have been used.
2. HTML with JS rendered by interactive modules will have determined behavior because it always uses the same libraries.
3. Whether/when a frontend is connected to the kernel doesn't matter now. The visualization HTML contains everything it needs to setup and/or execute scripts.
4. Arbitrary DOM changes doesn't matter now. Even if the user screws the notebook's HTML, the data visualization broadcast from kernels will always be rendered correctly.";non_debt
Requested changes incorporated. Thanks for the review.;non_debt
Merging it into branch-0.6;non_debt
Should we update the comment to indicate this is the initial backoff time now?;non_debt
This PR has been approved for about 4 days with no further feedback so I am going to go ahead and merge. Thank you for this patch @a2l007 ... I know that my team is going to really enjoy this new API response dedicated to time outs.;non_debt
Fix #601: Use long when calling curl_easy_getinfo in tests;non_debt
I believe the ORT replacement will be installed via RPM -- the 5, 10, 20, however many binaries/things there are going to be, should all be included in the same RPM. At least, that's what I understand out of the blueprint review so far.;non_debt
"1. Yes CAST is required because the type of looked up value is Object. While we're saying based on only type on runtime, we might need to consider the type on schema.
2. Implicit cast is not possible because we can't determine the value type on compile time, especially we use ANY to represent its type.";non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22897/
Test PASSed.";non_debt
I think adding it to Array for consistency sounds good;non_debt
@snleee Please take a look and see if the interface change is okay for the LinkedIn internal tasks;non_debt
Removed;non_debt
"Could we consolidate actions in common for version 4 and 5?
Like";non_debt
done;non_debt
"Now allows to store the credentials in the keyring
of the OS. Retains backwards compatibility.
Testing Done:
- Tested with a PR
@jlowin";non_debt
This is checking length of encoded data, right? It looks like some Huffman code has 3 byte boundaries. Is this should be 4? ( e.g. `|11111111|11111111|11111110|0010         fffffe2  [28]`);non_debt
@ijuma Thanks! LGTM. Merged to trunk.;non_debt
"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4826?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4826?src=pr&el=continue).";non_debt
ONNX models are context agnostic. For running an ONNX model you will import ONNX model to ur framework model format i.e sym and params  for MXNet. Now you use mxnet api to set your context and either retrain this model(i.e ignore the params file ) or use it for inference.;non_debt
QA tests have started for PR 1505. This patch merges cleanly. <br>View progress: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16941/consoleFull;non_debt
"both planners have checked whether a sink is an `OverwritableTableSink`. 
I will add some tests about override mode";non_debt
Yea, I got the point. So, the case is when some files have different header.  I was thinking of a case when all files have the same header but the user provided a wrong order of schema (comparing to those all headers).;non_debt
"Meta data
Hash:ed7277689d9a82134d631fe144f0add5ace6380f Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/136299450 TriggerType:PUSH TriggerID:ed7277689d9a82134d631fe144f0add5ace6380f
Hash:25b3f3db03ffe6f5dbe243e4c1a53479093610bf Status:SUCCESS URL:https://travis-ci.com/flink-ci/flink/builds/136363611 TriggerType:PUSH TriggerID:25b3f3db03ffe6f5dbe243e4c1a53479093610bf
-->
* ed7277689d9a82134d631fe144f0add5ace6380f : SUCCESS [Build](https://travis-ci.com/flink-ci/flink/builds/136299450)
* 25b3f3db03ffe6f5dbe243e4c1a53479093610bf : SUCCESS [Build](https://travis-ci.com/flink-ci/flink/builds/136363611)
  The @flinkbot bot supports the following commands:
 - `@flinkbot run travis` re-run the last Travis build";non_debt
"# [Codecov](https://codecov.io/gh/apache/dubbo/pull/4202?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/dubbo/pull/4202?src=pr&el=continue).";non_debt
Merged build finished.;non_debt
"right.  
The Catch unit tests test only the v4 calculation. Your changes are in the plugin which will be hard to test from the unit-test we have. I guess you would need to create AuTest test which you will have to be created from scratch for this plugin (it will be great but it is up to you).";non_debt
Are the CompileStatic annotations here and in the following test necessary? BugsSTCTest extends StaticTypeCheckingTestCase.;non_debt
You can avoid `case` double-fold here with guards in condition, something like;non_debt
@gatorsmile Sure I'm fine with that. Thanks!;non_debt
Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/162/;non_debt
"  [Test build #30100 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/30100/consoleFull) for   PR 5031 at commit [`3e9ac16`](https://github.com/apache/spark/commit/3e9ac16a8e2e0bdd11f850428115bdc891874b1e).
- This patch **fails Scala style tests**.
- This patch merges cleanly.
- This patch adds no public classes.
- This patch does not change any dependencies.";non_debt
"Thanks for the review. Since we are adding a unit test, move the argument checking in checkArgs()
Edit: Since the existing tests are assuming that alter user entity type is valid with bootstrap server, revoke the change to the original place and test with the command entry point main()";non_debt
@xubo245 I thought that there is no need to add test cases for boolean type properties validation.Previous test cases is enough to this validation.;non_debt
Add new target style to apply clang-format style;non_debt
" * This patch **fails to build**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
s/if only cares about/if you need only/;non_debt
"Update testng ver
Update testcontainers ver
Remove arquillian references
Remove jmockit references";non_debt
How about?;non_debt
SDV Build Success , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3038/;non_debt
do you mean we support `DESCRIBE QUERY TABLE t`?;non_debt
"when add a parition with storage_cooldown_time property like this:
alter table tablexxx ADD PARTITION p20200421 VALUES LESS THAN(""1588262400"") (""storage_medium"" = ""SSD"", ""storage_cooldown_time"" = ""2020-05-01 00:00:00"")
and show partitions from tablexxx
the CooldownTime is wrong: 2610-02-17 10:16:40, and what is more, the storage migration is based on the wrong timestamp.
The reason is that the result of DateLiteral.getLongValue is not timestamp.";non_debt
"Apparent cause of failure
[incubator-brooklyn-pull-requests] $ /home/jenkins/tools/java/latest1.6/bin/java -Xmx26 -Xms256m -XX:MaxPermSize=512m -cp /home/jenkins/jenkins-slave/maven3-agent.jar:/home/jenkins/tools/maven/apache-maven-3.0.4/boot/plexus-classworlds-2.4.jar org.jvnet.hudson.maven3.agent.Maven3Main /home/jenkins/tools/maven/apache-maven-3.0.4 /x1/jenkins/jenkins-slave/slave.jar /home/jenkins/jenkins-slave/maven3-interceptor.jar /home/jenkins/jenkins-slave/maven3-interceptor-commons.jar 37905
Error occurred during initialization of VM
Incompatible minimum and maximum heap sizes specified
ERROR: Failed to launch Maven. Exit code = 1
Invalid configuration of Jenkins job";non_debt
will remove;non_debt
very cool.  however, as part of the simplification effort, i remove this altogether.  i'm only creating 1 dstream;non_debt
DISPATCH-1926: Added code to selectively allow deletion of http:yes lâ€¦;non_debt
"Restart JN with services up and running is not a good idea and can result in potential data loss.
Since we do not have server side ability to Stop All except ZK, here is new steps needed:
Stop All Services
Reconfigure
Install NN
Install ZKFC
Start ZK
Start JN
....
  21517 passing (34s)
  48 pending";non_debt
Merged build finished. All automated tests passed.;non_debt
Merged to Master;non_debt
Shouldn't you wait until all processors are running (have called onStart) before setting ApplicationStatus to running?;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/4737/;non_debt
LGTM too, the tests seem to have passed as well.;non_debt
Merged build finished. Test PASSed.;non_debt
" * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
LGTM. I just had one question in the tests.;non_debt
Ack.;non_debt
"Now, for all hadoop related dependencies, we just use the single jar of flink-shaded-hadoop-2-uber, instead of depending on individual hadoop jars. It's much more convenient for users to use now.
To summarize all flink-connector-hive's dependencies, which are all of provided scope:
hive-metastore
hive-exec
flink-shaded-hadoop-2-uber
- adds a few more dependencies for HiveCatalog in flink-connector-hive to connect to a remote hive metastore service
We need to add end-to-end test in order to cover such work. cc @lirui-apache @zjuwangg";non_debt
"add extMsg when reproteror for containerinfo
use case :";non_debt
Fix regression in MKLDNN caused by PR 12019;non_debt
I'd like to close this for now. Wait for necessary change on statistics.;non_debt
"[AMBARI-25194] Increase the Agent cert validity to 3 years
Manually tested by updating the ca.config file.";non_debt
"+1 LGTM
I've found that this patch ran as expected on testing cluster.";non_debt
Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/3125/;non_debt
The PR should be OK to be merged with just subset of tests as it does not modify Core of Airflow. The committers might merge it or can add a label 'full tests needed' and re-run it to run all tests if they see it is needed!;non_debt
"- Okay, I see why we don't want to have `println` lines in the example now. If it is just one line of code, I prefer putting it inside the example.
- This won't work on a distributed server. We need to collect them first.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6522/
--none--";non_debt
"- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.";non_debt
"@dandy10 I tried to use beam from this PR but I ended up with the same error still.
I simply cloned the repo, installed dependencies from build-requirements.txt and ran setup.py which completed successfully. Did I miss something or do I have to build anything else as well?";non_debt
" * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
"The changes to update to TS@3.5.1 were relatively minor changes to the type mapping interfaces, and are all in this commit: https://github.com/trxcllnt/arrow/commit/492d1f111bb78b75ca79790beb4654119af93d51#diff-3cc15e0c860bac3718152b2c143638ad
I don't disagree on preferring this to be split out and handled separately, but practically speaking I was working under a number of constraints that made it more difficult. First, TypeScript released v3.3+ with breaking changes to type resolution halfway through the process of working on these features (leading to [this issue]( https://github.com/apache/arrow/issues/4452)).
Second, I was using the new Builders in app code as they were being developed. The apps needed TS 3.5, but wouldn't compile without these fixes, so I had to update Arrow. But the Builders branch was sufficiently different and would have been difficult to track and constantly rebase with that change (and wasn't even done yet), it was easier to do it here.";non_debt
Run Dataflow ValidatesRunner;non_debt
I'm not sure I follow completely.  Do you mean the sections in common? As we can't test the entire topology description as it has changed from the optimization.;non_debt
Network-related parts LGTM;non_debt
Since `GROUPING SETS((year, month), (year), ())`  is valid SQL;non_debt
@tqchen So we actually pass fcreate(json, libmod, device_type_id[0], device_type_id[1], *device_type_id_others), right?;non_debt
 Merged build triggered.;non_debt
@mjsax, would appreciate your feedback on this. Thanks.;non_debt
"Kubernetes integration test status failure
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/41565/";non_debt
FreeBSD build _successful_! See https://ci.trafficserver.apache.org/job/Github-FreeBSD/385/ for details.;non_debt
IGNITE-5103 - Server drops client node from cluster when no metrics uâ€¦;non_debt
Then, I would suggest using `shuffle` as a prefix and omit the `netty` part. I tried to align the REST API response with the metrics naming here.;non_debt
retest this please;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1540/;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2677/;non_debt
[MXNET-1187] Added Java SSD Inference Tutorial for website;non_debt
" * This patch **fails Spark unit tests**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
This pull request has been closed due to lack of activity. If you think that is incorrect, or the pull request requires review, you can revive the PR at any time.;non_debt
I've changed the `TestUtils.createServer()` to include the `threadNamePrefix` when creating a KafkaServer in tests.;non_debt
retest this please;non_debt
merging if no more comment;non_debt
"df80c15 - copy-pasting sources from `iceberg-hive-metastore` 0.11.x, this commit **should not need a review**
**Please review:** fff81a7 - Changes needed compared to Iceberg repo to make tests pass:
- Extra conf setting in TestHiveMetastore#initConf: https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-454838f58a7e8537062a492232ebeaff66cb75051a4d2591ce54bbd1a1779fa5R223
- Database location URI behaves differently in upstream Hive: https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-70672eb7a795f778b743e1965d02f66a0d4f918c551dadcefd9b4864e9a93522R555
and https://github.com/apache/hive/commit/fff81a77632995b25d930b85f50a3bf49a46bc7f#diff-5173630509f49d2e68c1f0b06ea0d47166ff2e7dd0154bf24c763662546d90e6R405
Added the same checkstyle files as for iceberg-handler.
Moved `AssertHelpers` to iceberg-catalog test sources.
Deleted `TestHiveMetastore` from iceberg-handler (since it's already in iceberg-catalog) but ported over the changes from the previous commit.";non_debt
"Add version for CarbonData
Are there any other good idea for CarbonData version?
        - Whether new unit test cases have been added or why no new tests are required?
        - How it is tested? Please attach test report.
        - Is it a performance related change? Please attach the performance test report.
        - Any additional information to help reviewers in testing this change.";non_debt
Thanks, @aajisaka. I merged this.;non_debt
done, see #809;non_debt
feel free to mark this and all of the above as resolved;non_debt
"Hi @jihoonson 
Thanks for your review of great depth. 
As you commented, above case will be resolved automatically in TAJO-1346. I'll commit it to the master branch soon. :-)";non_debt
Above threshold?;non_debt
Update CD Jenkins config for include/mkldnn/oneapi/dnnl;non_debt
[approve ci];non_debt
"To the first point, I ended up splitting on protocol and then client/session.  So there is a ProxySesion.  Then there is a Http1Session and Http2Session that inherit from that.  Then there is Http1ClientSession  and Http1ServerSession, etc.  It turns out there was more commonality within the protocol, so you got more sharing by splitting on the protocol first.
To the second point.  I originally had a Client and Server versions of the protocol specific Transacation classes, but eventually there was really no difference between the two, so a trimmed that class hierarchy back.";non_debt
Oh I see, it gets caught and rethrown below;non_debt
"Kubernetes integration test starting
URL: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder-K8s/38790/";non_debt
It's rebased.;non_debt
Provide a more accurate size check for max_document_size limit;non_debt
ok, got it.;non_debt
"@apiri 
It sounds good. Thanks.";non_debt
"this might break the usage of GetName in exisitng usage.
Not sure if we would want to make the function an internal use only.";non_debt
@codelipenghui The `LedgerInfo` class is generated by protobuf, which is already an immutable class, we don't need to do extra things to make it immutable.;non_debt
Jenkins, test this please;non_debt
Done!;non_debt
Good catch, you are right.;non_debt
This PR is the only left in 0.10.1, so what we decide to do? @b-slim @gianm;non_debt
Build Failed with Spark 2.2.1, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.2/716/;non_debt
"+1, LGTM
Thanks for the contribution, @szaboferee!  Merged to master.";non_debt
Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/1852/;non_debt
Added a `__PLACEHOLDER__` suffix to the dummy table location path to fix case one. And made sure that we handle Hive compatible tables properly.;non_debt
"  [Test build #33266 has finished](https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/33266/consoleFull) for   PR 6323 at commit [`bd4a5dd`](https://github.com/apache/spark/commit/bd4a5dd52e0081ed08d934e9330790b86236a6b1).
- This patch **passes all tests**.
- This patch merges cleanly.
- This patch adds no public classes.";non_debt
R: @kennknowles;non_debt
"This utility method determines whether a Tree can be used in places
where a Primary is required (for instance, as the receiver expression of
a method invocation).";non_debt
"â€¦ils to start
RetriableException was added to the exceptions when we retry an hdfs operation.
Manually tested on my local cluster.";non_debt
"In https://ci.bigtop.apache.org/view/Test/job/Build-Deploy-Smoke-Test-Pull-Request-All-Distros/25/:
It seems the permission issues when it tried to delete the directory which was created by 'root'.";non_debt
I think maybe this is not a fatal error.;non_debt
2524488-329 summary-0;non_debt
"@HyukjinKwon 
Thank you again, and now windows machines can compile apache master.
No my java version is 1.8.";non_debt
@michaelandrepearce, fair enough.;non_debt
:-) Np. Thanks for taking the time to look over the PR;non_debt
Amended.;non_debt
"FileStatus is a Hadoop specific class. The return type of getFileStatus OM call should be Hadoop independent and a simple POJO can be used.
OzoneFileSystem can create the appropriate FileStatus implementation based on the information in this simple POJO.
https://issues.apache.org/jira/browse/HDDS-3501
With full CI build on my fork.";non_debt
Can one of the admins verify this patch?;non_debt
R: @tgroh Please review. Thanks;non_debt
When the sync process was notified more than 5 times, it will lead to early return before sync completely, so it seems that it should not depend on the number of notified times.;non_debt
[FLINK-19406][table-planner-blink] Casting row time to timestamp loseâ€¦;non_debt
@KurtYoung the failed test is fixed;non_debt
"I doubt this Jenkins error has anything to do with this PR:
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running streamer.ByteBufferTest
Tests run: 400, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.194 sec - in streamer.ByteBufferTest
Running streamer.BaseElementTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.231 sec - in streamer.BaseElementTest
Running common.ClientTest
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.575 sec - in common.ClientTest
Running rdpclient.MockServerTest
Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.055 sec <<< FAILURE! - in rdpclient.MockServerTest
testIsMockServerCanUpgradeConnectionToSsl(rdpclient.MockServerTest)  Time elapsed: 1.043 sec  <<< ERROR!
javax.net.ssl.SSLHandshakeException: No appropriate protocol (protocol is disabled or cipher suites are inappropriate)
	at sun.security.ssl.Handshaker.activate(Handshaker.java:529)
	at sun.security.ssl.SSLSocketImpl.kickstartHandshake(SSLSocketImpl.java:1492)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1361)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
	at rdpclient.MockServerTest.testIsMockServerCanUpgradeConnectionToSsl(MockServerTest.java:166)
Error in mock server: Received fatal alert: handshake_failure
javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:154)
	at sun.security.ssl.SSLSocketImpl.recvAlert(SSLSocketImpl.java:2038)
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1135)
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413)
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397)
	at streamer.debug.MockServer.run(MockServer.java:122)
	at java.lang.Thread.run(Thread.java:748)
Results :
Tests in error: 
  MockServerTest.testIsMockServerCanUpgradeConnectionToSsl:166 Â» SSLHandshake No...";non_debt
retest this please;non_debt
are we assuming the comment holder will always take an entire line?;non_debt
Null is for backward compatibility (for cases when a specific component is not needed);non_debt
Backport of #9195 to 0.17.0.;non_debt
"I think all DS v2 classes are still evolving. Shall we use `@Evolving` consistently? e.g. https://github.com/apache/spark/pull/27560/files#diff-ab2a72871faeb3ad8bae1d7f951899deR29
cc @brkyvz @rdblue";non_debt
"Can we add these options directly via `ConfogOptions` similar to that: https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/configuration/HighAvailabilityOptions.java
Maybe start a new class, `SecurityOptions`.";non_debt
Build Success with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/9737/;non_debt
@WeiZhong94 Thanks a lot for the view. Updated the PR.;non_debt
"This pull request fixed the bug that UDAF with Object Array return type (e.g. Row[]) will generate wrong result.
The problem is we reuse 'reuseArray' as the return value of ObjectArrayConverter.toBinaryArray(). It leads to 'prevAggValue' and 'newAggValue' in GroupAggFunction.processElement() contains exactly the same BinaryArray, so 'equaliser.equalsWithoutHeader(prevAggValue, newAggValue)' is always true.
return a copy of reuseArray in ObjectArrayConverter.toBinaryArray()
This change is a trivial rework / code cleanup without any test coverage.";non_debt
With some added changes, this seems to be working now. Running more tests today and then expect to immediately merge.;non_debt
"Run configurations startup order
https://issues.apache.org/jira/browse/HDDS-4785";non_debt
Packaging result: âœ”centos6 âœ”centos7 âœ”debian. JID-106;non_debt
"quickly and easily:
   `[BEAM-<Jira issue #>] Description of pull request`
       number, if there is one.
       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.pdf).
---
Sometimes during the cythonization of beam it finds other files in either target/tox which cause the compilation to fail. We should only pick beam files here.
R: @robertwb PTAL";non_debt
Do we really need to re-initialize committed offsets and task time during Resume?;non_debt
@JoshRosen I could imagine us introducing this same issue in the future with more Params in sharedParams.scala (like `HasAggregationDepth`).  As long as it's fine for us add MimaExcludes (and are sure Java users won't experience issues), then this seems fine.;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
This accounted for most of the test failures, and it's already fixed on trunk.;non_debt
Adding the command for update network partition;non_debt
Removed....;non_debt
"Then how do you think about activations requiring transactional supports?
For example, if an action updates data in DB, it can cause multiple updates with at least once semantic.
Do you keep transactional support in your mind as well?";non_debt
This LGTM. LMK if I should merge.;non_debt
tvm.abs !!;non_debt
@mboehm7 Can this be merged?;non_debt
It's a bug, thanks for pointing it out!;non_debt
"     with your text. If a section needs no action - remove it.
     Also remember, that CouchDB uses the Review-Then-Commit (RTC) model
     of code collaboration. Positive feedback is represented +1 from committers
     and negative is a -1. The -1 also means veto, and needs to be addressed
     to proceed. Once there are no objections, the PR can be merged by a
     CouchDB committer.
     See: http://couchdb.apache.org/bylaws.html#decisions for more info. -->
     what problem it solves or how it makes things better. -->
Add couch_stats tracking back to couch_log using stats defined in
https://github.com/apache/couchdb/blob/master/src/couch_log/priv/stats_descriptions.cfg
     Does it provides any behaviour that the end users
     could notice? -->
     repositories please put links to those issues or pull requests here.  -->
issue #832";non_debt
Thank you!;non_debt
Run Java_Examples_Dataflow PreCommit;non_debt
[FLINK-19823][table][fs-connector] Filesystem connector supports de/serialization schema;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1728/;non_debt
" * This patch passes all tests.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
R: @dhalperi;non_debt
31006158-7250 summary-0;non_debt
Actually, I take back my comment on validation that we don't use the same field name, seems that is separately being tracked in KAFKA-4855.;non_debt
Thanks for the PR it has been merged. Do you mind closing this?;non_debt
"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.";non_debt
study;non_debt
This needs to return the `Configuration` that was passed in when creating the `HadoopInputFile`, if there was one. The file system's `Configuration` may not contain the same configuration properties as the one that was passed in, and this `Configuration` is needed in some paths.;non_debt
ok. `CodegenFallback` does it.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8464?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8464?src=pr&el=continue).";non_debt
@d8tltanc Looks like we haven't add the new configs to Connect: https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3103/;non_debt
Jenkins, retest this please.;non_debt
Merged build finished. Test PASSed.;non_debt
Thanks for the quick review. I fixed groupby too. Please check it again :);non_debt
Also related to the grammar, Section 12.3.3 says that implicit lengths are not allowed for packed decimals. Probably binaryNumberKnownLengthInBits needs an SDE based on the binary rep if lengthKind is implicit.;non_debt
Upgrade to jclouds to 1.8.1;non_debt
Merged to master.;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
@mxnet-bot run ci [unix-gpu, windows-gpu];non_debt
"The file takes in data in dictionary format, for both predictions and ground truth. 
Ranking problems are very common in several areas of ML, and specifically computing metrics at a certain position is common in many applications. Usually for recommender systems, we don't want to compute the overall precision, but only, say precision@10, because only 10 items might be shown to the user. It makes sense to optimize your model that gets the top entries right, sacrificing overall precision. Similar arguments hold for recall, coverage etc. Having such a standard metric available in MXNet will let more people in the ML community make use of it, and increase it's adoptation.";non_debt
"Referring the ReferenceError - it was caused by a bug in randomsentence.js that happens only on case of a ""fail"" for one of the tuples. I fixed the bug (fix committed), still not sure why did you get ""fail"" in the split sentences example.";non_debt
Build Failed with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/413/;non_debt
@vectorijk Do you have time to update this PR? If not, I can help.;non_debt
Thanks @HeartSaVioR for the comments.;non_debt
" * This patch **fails to generate documentation**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
Change to sink's class name;non_debt
"Java characters (e.g. char and Character type) have internal UTF16 encoding, meaning that they will always be encoded as 16-bit numbers. But there are some symbols called surrogate pairs, which may require a pair of 16-bit characters to be encoded.
So from the wire format point of view, when you do `""ðŸŒ‰"".length()` you will get 2 (as there are two underlying 16-bit characters to encode this symbol), but with `""ðŸŒ‰"".codePointCount(0,""ðŸŒ‰"".length())` you will get 1 (as there only one single symbol). But the string serializer is not operating on symbols and UTF code points, it is operating on the underlying 16-bit numbers, so it must handle it properly out of the box.";non_debt
ðŸ‘;non_debt
@kishorekasi @eolivelli what is the plan for this? Are we including this in 4.5.0?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-test-coverage/359/";non_debt
I see, good point!;non_debt
Done.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8517?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/8517?src=pr&el=continue).";non_debt
I opened a PR for Calcite https://github.com/apache/calcite/pull/549. I will rebase this branch.;non_debt
[BEAM-1153] GcsUtil: use non-batch API for single file size requests.;non_debt
33884891-7038 review-368378012;non_debt
we can also add a new case for `backToBackFilterLong`, as we handle boolean type now.;non_debt
@sijie I did. Are you asking if it's possible to do so?;non_debt
"Fix: [#HIVE-21555](https://issues.apache.org/jira/browse/HIVE-21555).
The cached thread pool is proper when the task is light-weight and the number of sub-threads are not too many. Otherwise, fixed thread pool should be used to avoid OutOfMemoryError.";non_debt
keepalive-tests: add more keepalive tests;non_debt
Remove.;non_debt
"Yes (in an updated form of course), see https://github.com/apache/arrow/pull/5562#issuecomment-553782658
@kszucs what do you mean exactly with the task integration tests?";non_debt
Suggest workarounds for partitionBy in Spark 1.0.0 due to SPARK-1931;non_debt
[CARBONDATA-2825][CARBONDATA-2828] CarbonStore and InternalCarbonStore API;non_debt
"- This patch passes all tests.
- This patch merges cleanly.
- This patch adds no public classes.";non_debt
`java.time.Instant.class` -> `Instant.class`;non_debt
"Contributed by: Mehakmeet Singh
Tested by: mvn -T 1C -Dparallel-tests=abfs clean verify
Region: East US, West US";non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2092/;non_debt
"Yes.
True - that would also not break backwards compatibility.";non_debt
" * This patch **fails to build**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
31006158-2683 summary-0;non_debt
@wesm Here is a rebased PR (since part of this was used in ARROW-264).;non_debt
The @HyukjinKwon idea looks better and I think it's easier for users to use this new feature without any configuration.;non_debt
doesn't look like a good idea to use python. Why not build.bat?;non_debt
"[incubator-brooklyn-pull-requests #74](https://builds.apache.org/job/incubator-brooklyn-pull-requests/74/) SUCCESS
This pull request looks good";non_debt
Are these hard or soft dependencies?  There are newer versions of these.;non_debt
" * This patch **fails Scala style tests**.
 * This patch merges cleanly.
 * This patch adds no public classes.";non_debt
[Feature-3633][server,common] Let dependent nodes wait for a while before the pre-dependency starts;non_debt
https://issues.apache.org/jira/browse/PARQUET-1877;non_debt
good to go :+1:;non_debt
49876476-6757 summary-0;non_debt
Your query may be caused by my confusion above.  To answer your query, the prediction F(x) should be the raw prediction, not the discrete -1/+1 value.;non_debt
"@squito Yes, exactly. Nice summarization.
IMHO there're some cases we still want to get custom log URL while the status of application is ""in progress"", because according to the logic on determining whether the application is finished, app could be shown as ""inprogress"" when the app terminated unexpectedly.
https://github.com/apache/spark/blob/0b3abef1950f486001160ec578e4f628c199eeb4/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala#L98-L100
In this case, showing origin log URLs may not work but showing custom log URLs would work if the external log service gathers executor logs continuously rather than gathering logs when app is finished - I expect the behavior of external log service as former not latter. Even if app is still running, app UI will still provide origin log URLs so we can get it from there instead of SHS.
What I agreed to make change on only SHS was my prev. patch was making log urls being static again which is fragile if there's a change on external log service. Now we just require end users to modify the configuration and restart SHS (unless there's a cache mechanism to avoid re-reading events when restarting SHS.) IMHO, showing custom log URLs even for apps which are shown as ""inprogress"" still makes sense to me. 
Btw, I didn't add variables for other resource managers as they don't provide log URLs for now. (If possible we may be better to address it as well via separate PRs.)";non_debt
"# [Codecov](https://codecov.io/gh/apache/kylin/pull/1534?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/kylin/pull/1534?src=pr&el=continue).";non_debt
Used `luke` to look at the Lucene index created, and everything looked OK.;non_debt
Corrected other occurrences. Next PR update should reflect the changes.;non_debt
Can one of the admins verify this patch?;non_debt
?;non_debt
Looks good but you have to fix conflicts now.;non_debt
We probably don't want to do that because in the future the number of contains will change at runtime and `containerMap.size()` will no longer be a good indication.;non_debt
same as above.;non_debt
QA tests have started for PR 1337. This patch merges cleanly. <br>View progress: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/16529/consoleFull;non_debt
not sure I follow - why don't we `return r.code()` directly?;non_debt
LUCENE-9462: Fields without positions should still return MatchIterator.;non_debt
"@ijuma If I understand your comment correctly, you are talking about calling `RequestBuilder.build()` only once to address this issue without modifying the Request class itself. Here are my thoughts:
- Controller has separate request send threads talking to brokers and each of them contains a separate network client. Although we pass in the same request builder to each of the network clients, we cannot ensure that `RequestBuilder.build()` is called only once because we currently don't maintain states across these network clients. However, we can explicitly cache the request object in the RequestBuilder to avoid re-instantiation (this is what this PR did).
- Even if we can ensure `RequestBuilder.build()` is called only once and we use the same request object in the network clients, in the current implementation we will still create multiple Struct objects each time we serialize the request when network clients are trying to send out the request. To avoid that, this PR caches the Struct object and re-uses it when `toStruct()` is called.
Although this is a general optimization for all types of request, this PR only changes `UpdateMetadataRequest` instead of `AbstractRequest` because `UpdateMetadataRequest` is the only use case where we send out requests with the same payload for multiple times.";non_debt
I think for docs to be generated, you need to capture them as `#[doc=$doc]` like [this](https://github.com/rust-ndarray/ndarray/blob/master/src/impl_ops.rs#L51) for example.;non_debt
Agreed.;non_debt
nope, that should be fine enough.;non_debt
"Are you trying to handle the case where the map is empty? why not handle it directly then?
Just needs one new line of code, like: `require(distinctMap.nonEmpty)`";non_debt
Can one of the admins verify this patch?;non_debt
"We use web-socket api for creating and cloning notes from our servers. To know noteId of created or cloned note, we need some response message. I propose sending NOTE message to client on creating/cloning note.
There could be other ways to reach it. For example, we can send newId and note for creation an id, name, newId for cloning. If newId is not exists, we generate it on server.
Another way will be to create separate api for this, to not mix this with front-end api. It could be REST or smth.";non_debt
@eolivelli Looks like some tests will need refactoring.;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Given this is repeated several times, do you think it's worth disabling for the whole file?;non_debt
LGTM if tests pass.;non_debt
done via https://github.com/twitter/heron/pull/1629/commits/4fe8a8f834e984ba6cc1e3eff71e5932186cbaee!;non_debt
"I tried
The return result is `1`. I think you put `enddate` and `startdate` in the wrong order.
The format is `datediff(string enddate, string startdate)` (https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF).";non_debt
Thx @jackar;non_debt
woot! making perl better :D;non_debt
done in bae7ca548739e0d9fe501afba8b2c2559b7843f9;non_debt
 Merged build triggered.;non_debt
Will merge after tests pass. Thank you for the quick review.;non_debt
Do we need LongAdder still?;non_debt
Thanks guys!;non_debt
agreed;non_debt
So strange, l have handled it, but we can not see the change here;non_debt
I think we can make it inlined.;non_debt
@masaori335 Do we need this in 8.1.x as well ?;non_debt
Done.;non_debt
[AIRFLOW-7014] Add Apache Kylin operator;non_debt
run cpp tests;non_debt
@wu-sheng no tks!;non_debt
[SPARK-3816][SQL] Add table properties from storage handler to output jobConf;non_debt
Fixes #2142.;non_debt
This matches what I see locally, https://builds.apache.org/job/HBase%20Nightly/job/HBASE-24049-packaging-integration-hadoop-2.10.0/3//artifact/output-integration/hadoop-2/hadoop_cluster_command.err/*view*/;non_debt
@viirya Yeah, as @hvanhovell suggested, I'll use the resolve function of `LogicalPlan`.;non_debt
Having a default implementation will lead to people who add new expressions don't implement `withNewChildrenInternal` and we again be back to the same situation having many slow `withNewChildren` implementations, so I prefer to make have it like this to enforce `withNewChildrenInternal` implementation. Actually, even now, there are two expressions added to the master and I need to update this PR to implement the `withNewChildrenInternal` for them. The `legacyWithNewChildren` is here for a transition period, we have some expressions that are a bit hard to write `withNewChildrenInternal` for and probably need some refactoring. The goal is to remove `legacyWithNewChildren` altogether at some point.;non_debt
@rxin sure I'll put together a PR for the python API tonight;non_debt
Note on your aside: you will not need to rebase. The commits d510b4e, 8bce693, and 9f8dd18 will simply be added to GitHub and the master pointer updated. Then it will automatically look like the PR you intend.;non_debt
True. gfcpp.properties was changed to geode.properties with PR #13.;non_debt
Build Success with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/9627/;non_debt
* Modify WXStorage unit test;non_debt
Removed.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7329/
--none--";non_debt
"mxnet-mkldnn on mac requires that openmp be enabled (else performance is degraded). There are currently instructions specifying that the Makefile needs to be modified to address this however, that is not a good user experience. 
the issue is that mac by default does not include openmp by default and therefore a guard was added to prevent users from trying to build with this. the developer should instead not build with openmp enabled on mac and the Makefile should not attempt to catch this (the user may falsely believe they are using openmp when they are not).
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
"thanks for sharing the info!
I would like to use the default value 1 or something else, because if we force user to supply the shape, it's difficult for users who are not ml practitioner, since the user has to be familiar with the graph and figure out the related ops and shapes. We'd like to make the graphdef works even without specifying shape, then users could just use the model from tf community although they don't know the related graph structure.  
If user is willing to supply the related shape, then specified shape will be used instead of -1/1.";non_debt
"This moves parsing `CREATE TABLE ... USING` statements into catalyst. Catalyst produces logical plans with the parsed information and those plans are converted to v1 `DataSource` plans in `DataSourceAnalysis`.
This prepares for adding v2 create plans that should receive the information parsed from SQL without being translated to v1 plans first.
This also makes it possible to parse in catalyst instead of breaking the parser across the abstract `AstBuilder` in catalyst and `SparkSqlParser` in core.
For more information, see the [mailing list thread](https://lists.apache.org/thread.html/54f4e1929ceb9a2b0cac7cb058000feb8de5d6c667b2e0950804c613@%3Cdev.spark.apache.org%3E).
This uses existing tests to catch regressions. This introduces no behavior changes.";non_debt
`:type`;non_debt
31006158-4705 description-0;non_debt
...I wonder how that happened;non_debt
"Since enabling the strict dependencies on all Java modules is a large and critical change to be merged all at once. Therefore, task [[BEAM-10961](https://issues.apache.org/jira/browse/BEAM-10961)] is divided into sub-tasks and strict dependencies are enabled to each module separately which is comparatively easy to review and merge.
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.";non_debt
@yanboliang, just a friendly reminder please don't forget to review the PR when you have time. Thanks!;non_debt
"Fix #6386
Ran below command";non_debt
FWIW, I _believe_ that when we submit a job with the dispatcher `deployMode` is actually set to `client`, so this logic may not be invoked as expected.;non_debt
retest this please;non_debt
when there is no intrin func, using body for initialization. For issuâ€¦;non_debt
@leonardBang Sorry for the late response, I kind of lost sight of this PR :( I can't seem to find the test comment you mentioned, where would I find that?;non_debt
OK indexing throughput is noisy, but net/net I think there's no perf impact.  This is `wikimediumall` from `luceneutil` on 128 core box, JDK 11:;non_debt
I see makes sense;non_debt
feat(chart-data-api): make pivoted columns flattenable;non_debt
@hachikuji  Is this the expected result?;non_debt
FYI, no need for commit after DDL statements.;non_debt
"If I correctly interpret [this comment](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L181-L183), an exception will be thrown when `recoverFromCheckpointLocation` == `false` and `fs.exists(checkpointPath)` == `true` are satisfied.
Is [the comment](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/streaming/StreamingQueryManager.scala#L181-L183) wrong if this change is valid? (Note: since I made mistake, I have just update this comment)";non_debt
retest this please;non_debt
"I observed this while running a oozie job trying to connect to hbase via spark.
It look like the creds are not being passed in thehttps://github.com/apache/spark/blob/branch-2.2/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/security/HadoopFSCredentialProvider.scala#L53 for 2.2 release.
More Info as to why it fails on secure grid:
Oozie client gets the necessary tokens the application needs before launching. It passes those tokens along to the oozie launcher job (MR job) which will then actually call the Spark client to launch the spark app and pass the tokens along.
The oozie launcher job cannot get anymore tokens because all it has is tokens ( you can't get tokens with tokens, you need tgt or keytab).
The error here is because the launcher job runs the Spark Client to submit the spark job but the spark client doesn't see that it already has the hdfs tokens so it tries to get more, which ends with the exception.
There was a change with SPARK-19021 to generalize the hdfs credentials provider that changed it so we don't pass the existing credentials into the call to get tokens so it doesn't realize it already has the necessary tokens.
https://issues.apache.org/jira/browse/SPARK-21890
Modified to pass creds to get delegation tokens";non_debt
Yes, they are.;non_debt
"@hanm thanks for this! I pulled the code into a specialized branch on which I hope to land all of our jenkins related tooling:
https://github.com/apache/zookeeper/tree/jenkins-tools
I also changed the script around a bit to use out testReport from the jobs to identify failed tests. If folks have suggestions they can now use this to submit PRs in a similar way as the rest of our codebase
ps. I've also updated the jenkins job to reflect: 
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper-Find-Flaky-Tests/";non_debt
356066-2772 description-0;non_debt
Here's one organization that comes to mind:   ;non_debt
GEODE-6365: Add server group support for JDBC List Mapping and Destroy Mapping Commands;non_debt
DT output seems correct, it seems RT randomness was the cause. LGTM +1;non_debt
GEODE-6067: add list data-source gfsh command;non_debt
If its unbounded, a runner will never be able to execute it without expansion which requires the runner to at least inspect whether its capable of doing it.;non_debt
[BEAM-3326] Add a BundleProcessor to SdkHarnessClient;non_debt
actually, at this point we haven't persisted the message so, we don't have message-id so, we are not logging message-id.;non_debt
rebuild java11;non_debt
`tolerableCpFailureNumber <= UNLIMITED_TOLERABLE_FAILURE_NUMBER` always evaluates to `true` and can be removed.;non_debt
Do you know how do we include the yaml library inside lib/ts? @randall @bryancall;non_debt
Config should only be changed through the initialization stage. Nothing more.;non_debt
ping @junrao;non_debt
193065376-225 description-0;non_debt
Done. Now it is changed to iterate ArrowRecordBatch rather than VectorSchemaRoot.;non_debt
retest sdv please;non_debt
The schema inferring is replaced with metastore schema completely in #14690. I think we can close this now? cc @cloud-fan @liancheng;non_debt
"`write.df`/`read.df` API require path which is not actually always necessary in Spark. Currently, it only affects the datasources implementing `CreatableRelationProvider`. Currently, Spark currently does not have internal data sources implementing this but it'd affect other external datasources.
In addition we'd be able to use this way in Spark's JDBC datasource after https://github.com/apache/spark/pull/12601 is merged.
**Before**
- `read.df`
- `write.df`
**After**
- `read.df`
- `write.df`
Unit tests in `test_sparkSQL.R`";non_debt
[Doc] TVM release process;non_debt
@johnsgill3 @walshb -- any progress here?;non_debt
47246081-915 description-0;non_debt
"Maven build fails due to invalid file name character, this PR fix it by renaming the note file name.
[Bug Fix ]
* https://issues.apache.org/jira/browse/ZEPPELIN-3788
* CI pass";non_debt
[SPARK-17590][SQL] Analyze CTE definitions at once and allow CTE subquery to define CTE;non_debt
As part of this PR, the docker push target has been moved to a public ECR repository.;non_debt
"Thanks @wchevreuil for reviewing.
[ReplicationSinkManager.chooseSinks](https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSinkManager.java#L151) will be called to re-fetch the sinks when HBaseInterClusterReplicationEndpoint.replicate [catches ConnectException or UnknownHostException](https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.java#L563), It's same as before.
And I've added a config ""hbase.replication.fetch.servers.usezk"" to optionally still use ZK impl.";non_debt
Because we already have exception handling, this can be done from front end;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/4106/;non_debt
Thanks for testing and fixing this. A couple of minor comments.;non_debt
Add the class member variables with 'this';non_debt
"When a `SparkSession` is stopped, `SQLConf.get` should use the fallback conf to avoid weird issues like
a new test suite";non_debt
[AIRFLOW-4797] Fix zombie detection;non_debt
Re-opening PR after some manipulations around local git branches;non_debt
I didn't test this myself, but this diff could be sufficient for testing your change:;non_debt
Merged;non_debt
"fixed ""error=Algorithm HmacSHA1 not available"" when use ACL on windows.";non_debt
"Not the same thread, but:
Before invoking them, the caller will take a same lock.";non_debt
Added test for it.;non_debt
This actually needs to pass the current metadata location to `CustomService`. Otherwise, `CustomService` can't guarantee that the location update is an atomic swap, and the atomic swap is required for consistency. Otherwise, two processes could each start a change, validate that metadata hasn't changed and both call this update at the same time. The second one would win because the update doesn't know to reject it because it isn't based on the other process's changes.;non_debt
Yea, very nice PR description.;non_debt
done;non_debt
I had fix this. maybe you can try;non_debt
"[incubator-brooklyn-pull-requests #1062](https://builds.apache.org/job/incubator-brooklyn-pull-requests/1062/) SUCCESS
This pull request looks good";non_debt
"KAFKA-5374. AdminClient gets ""server returned information about unknoâ€¦";non_debt
How is this going to work with assign?  It seems like it's just avoiding the problem, not fixing it.;non_debt
I think we will need to discuss how to pack files and whether we need to look at the min/max stats on the sort key, for example. Does not have to be done now, though.;non_debt
"Also add (optional) access to the contexts on serializing and
deserializing pipelines.
---";non_debt
Similar cleanup https://github.com/apache/kafka/pull/6598;non_debt
"Seems the bracketedCommentLevel could not handle this case:
WIP";non_debt
"I got this working when I tested 0.10.0-rc4. In Hive, I had to set `iceberg.mr.catalog=hive` so that the correct catalog was used. Unfortunately, that broke the Hadoop table.
I think we might want to add support for Hadoop tables in our catalogs using an identifier that contains a path to fix this.";non_debt
There is a default one for each actor, yes -- from its actorSystem.;non_debt
Although we're making this public, let's not make all of its APIs public.  Can you please make the constructor private and make this class final?;non_debt
Thanks @kevinthesun;non_debt
Before merging, please share a link to the archived copy of the old website.;non_debt
"YW @iverase.
Right, that case should write a whole new segment, using the latest Codec, holding the just indexed document with vector field.";non_debt
@mistercrunch almost there! Can I help?;non_debt
retest this please;non_debt
[SPARK-22203][SQL]Add job description for file listing Spark jobs;non_debt
+1;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
R: @bsidhom @ryan-williams @tweise;non_debt
Merged build finished. Test FAILed.;non_debt
"@erikerlandson @skonto @liyinan926 for review
Do you see additional places where we would want to leverage the newer version for optimality?";non_debt
[BEAM-6726] explicitly specify signing key;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/988/;non_debt
"It seems this is a regression assuming from https://issues.apache.org/jira/browse/SPARK-16698.
Field name having dots throws an exception. For example the codes below:
throws an exception as below:
This problem was introduced in https://github.com/apache/spark/commit/17eec0a71ba8713c559d641e3f43a1be726b037c#diff-27c76f96a7b2733ecfd6f46a1716e153R121
When extracting the data columns, it does not count that it can contain dots in field names. Actually, it seems the fields name are not expected as quoted. So, It does not have to consider whether this is wrapped with quotes because the actual schema (inferred or user-given schema) would not have the quotes for fields. 
For example, this throws an exception. (**Loading JSON from RDD is fine**)
as below:
Unit tests in `FileSourceStrategySuite`.";non_debt
Should we consider making this fields `public` too?;non_debt
"LGTM thanks. 
It could be better if you can have a rebase locally and solve conflicts so people can easy merge your PR.";non_debt
"R: @chamikaramj 
Re-opening https://github.com/apache/beam/pull/1848 as a new PR.
Added a unit test and removed the change in `wordcount.py`. Adding `WordCountOptions` work when `wordcount` is run only by itself but fails when running with `tox`. All tests after the wordcount test start requiring the `output` flag as defined in the wordcount options. This happens with or without the current change to the `pipeline_options`. We need to fix that later.";non_debt
@jbertram although I can see an issue by inspecting the code. not just for AMQP  but noConsumers is only increased.. if you open a consumer... close it.. and open it again... maxConsumers is ignored.;non_debt
ARROW-3582: [CI] fix incantation for C++/Java detection tool;non_debt
Yes, we can use that dataset to verify.;non_debt
if these are host and port entries, can they use `HostAndPort` objects?;non_debt
Shouldn't this be `public` and not `package-private`? (The same for the other methods);non_debt
Ah, I didn't realize that, let me update it. Thanks!;non_debt
sure. we can remove that. For my last test, i found test is failed and have trouble to find the log for the test that just ran.;non_debt
"Makes perfect sense, thank you for explanation. 
I believe that is the case where changed one is distributed under the same licence, as per [#4 in licence doc](http://www.apache.org/legal/src-headers.html#3party).";non_debt
"DAVID SMITH  on dev@nifi.apache.org replies:
Hi Guys
st.=20
www.avast.com  |
As you may remember I have developed some processors that publish/subscribe=
 to AMQP brokers, but I was having problems writing Junit tests for these p=
rocessors.=C2=A0I was interested to see that you have been working on NiFi =
Pull Request 865.=C2=A0I have looked at your code for these processors, we =
are both using different property descriptors to allow messages to be publi=
shed and pulled. I also noticed that you are using RabbitMQ libraries to co=
nnect to the broker, whereas I connect to the AMQP broker using the QPID JM=
S libraries. I can still see a use for my processors and I would still be i=
nterested getting my processors uploaded to run alongside yours in a future=
 release of NiFi.I have tidied up my code and pushed it back to github:
https://github.com/helicopterman22/nifi_amqp_processors.git
I would appreciate your feedback=C2=A0Dave
=20
=20
 Github user asfgit closed the pull request at:
=C2=A0 =C2=A0 https://github.com/apache/nifi/pull/200";non_debt
Why some benchmarks use large num rows, others use small num rows?;non_debt
"Yes thanks @adriancole 
And thanks for the heads-up on that meeting. I dont know my calendar at that time in 2018.";non_debt
LUCENE-9514: Include TermVectorsWriter in DWPT accounting;non_debt
done both changes;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/9519/<h2>Build result: FAILURE</span></h2>[...truncated 2.31 MB...]	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoExecutionException: Command execution failed.	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:302)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)	... 31 moreCaused by: org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1)	at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:404)	at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:166)	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:764)	at org.codehaus.mojo.exec.ExecMojo.executeCommandLine(ExecMojo.java:711)	at org.codehaus.mojo.exec.ExecMojo.execute(ExecMojo.java:289)	... 33 more2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] For more information about the errors and possible solutions, please read the following articles:2017-04-14T02:39:05.915 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException2017-04-14T02:39:05.915 [ERROR] 2017-04-14T02:39:05.915 [ERROR] After correcting the problems, you can resume the build with the command2017-04-14T02:39:05.915 [ERROR]   mvn <goals> -rf :beam-sdks-pythonchannel stoppedSetting status of 82ba164b6f0ca69abbc707163232fa5b5791dc9a to FAILURE with url https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/9519/ and message: 'Build finished. 'Using context: Jenkins: Maven clean install
--none--";non_debt
Bigtop 2312;non_debt
I couldn't figure out what this comment meant, till I realized you were copying it from `addPendingTask`, though I coudln't figure out what it meant there either, till I looked in history and realized it was long obsolete -- added [here](https://github.com/apache/spark/commit/5e91495f5c718c837b5a5af2268f6faad00d357f), then removed [here](https://github.com/apache/spark/commit/3535b91ddc9fd05b613a121e09263b0f378bd5fa) (but the comment was left).  Can you delete both comments?  (unless you can see a way it still has some relevance ...);non_debt
You're `using` them but still using the full qualified names below :-);non_debt
Not possible if we want to share code. We call `decodeTasks` that does not know if it's decoding prevTasks or standbyTasks -- alternative, we can do two methods. Let me know what you think;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5798/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
Thanks for your contribution @houmaozheng . Looks good to me.;non_debt
"    - TESTS=""smoke/test_accounts
             smoke/test_affinity_groups
             smoke/test_affinity_groups_projects
             smoke/test_deploy_vgpu_enabled_vm
             smoke/test_deploy_vm_iso
             smoke/test_deploy_vm_root_resize
             smoke/test_deploy_vm_with_userdata
             smoke/test_deploy_vms_with_varied_deploymentplanners
             **smoke/test_diagnostics**
             smoke/test_disk_offerings
             smoke/test_dynamicroles
             smoke/test_global_settings
             smoke/test_guest_vlan_range""";non_debt
"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---";non_debt
Added mockito library.;non_debt
@nsivabalan What is the reason to change the name from `UserDefinedBulkInsertPartitioner` to `BulkInsertPartitioner`. I see that you added a new method to the interface, which is fine, but want to understand the reason for the name change;non_debt
"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
* â“ 1. The [description] looks good.
* â“ 2. There is [consensus] that the contribution should go into to Flink.
* â“ 3. Needs [attention] from.
* â“ 4. The change fits into the overall [architecture].
* â“ 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier";non_debt
jenkins, add user;non_debt
rebased;non_debt
than, can you approve please @andrijapanicsb;non_debt
if the intention is to use it only for batch, you would need to add {..};non_debt
Removed it;non_debt
Agree, this is one of the key changes in this PR.;non_debt
does `prettyString` work for it?;non_debt
Looks good. Please squash commits.;non_debt
Needs to be restored;non_debt
"In `BytesToBytesMap.MapIterator.advanceToNextPage`, We will first lock this `MapIterator` and then `TaskMemoryManager` when going to free a memory page by calling `freePage`. At the same time, it is possibly that another memory consumer first locks `TaskMemoryManager` and then this `MapIterator` when it acquires memory and causes spilling on this `MapIterator`.
So it ends with the `MapIterator` object holds lock to the `MapIterator` object and waits for lock on `TaskMemoryManager`, and the other consumer holds lock to `TaskMemoryManager` and waits for lock on the `MapIterator` object.
To avoid deadlock here, this patch proposes to keep reference to the page to free and free it after releasing the lock of `MapIterator`.
Added test and manually test by running the test 100 times to make sure there is no deadlock.";non_debt
"@elsloo Fixed up the JSON.org dependency.
Also I think at this point this can go into 2.3, so updating the milestone";non_debt
"follows: https://github.com/dmlc/mxnet/blob/master/python/mxnet/module/sequential_module.py
training log:
and load checkpoint:";non_debt
done.;non_debt
Merged build started.;non_debt
Why do we use a pre-baked key and certificates here? I though all the SSL tests generated certs on the fly but maybe I am wrong. In the past this has caused issues with hostname validation.;non_debt
"@StephanEwen @fhueske - apologies, I forgot to call collect. 
A dumb question - why do we need to sort the records before combining them? Should I just call collect on the oversized record or still pass it to the combiner? What do combiners do(or what are they suppose to do in general)?";non_debt
@dguy When I was adding `LockException` in another PR, I was explicitly requested to add more ctors even if I did not use them (see https://github.com/apache/kafka/pull/2233/files#r92231489);non_debt
Question*;non_debt
Merged to master. @JoshRosen you can make any other Jenkins changes as needed;non_debt
There could be multiple joins within each reporting period, so this should be a `+=`, right?;non_debt
Is it the password?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5149/
Test FAILed (JDK 8 and Scala 2.12).";non_debt
"logic here is sleep 1s and reconnect when connect failed?
Is there a need to add max_retry_num?";non_debt
Set split node's range to minimum of ext and split factor or split npâ€¦;non_debt
Add timeout to shutdown request to middle manager for indexing service;non_debt
Why do we need to fetch it? Seems we can always use `executorMemoryMiB.toString` instead.;non_debt
Move to checkWriteStatus's javadoc.;non_debt
"nit: `isolate ... form other client configs` -> 
Ditto below for other two.";non_debt
KAFKA-4484: Set more conservative default values on RocksDB for memory usage;non_debt
78186814-1794 description-0;non_debt
Nice catch. I meant `===` here. Thanks.;non_debt
done;non_debt
By the way, I also found another issue of the current foreach sink due to the API limitation in Data Source V2: https://issues.apache.org/jira/browse/SPARK-28605;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2127/
Test PASSed (JDK 8 and Scala 2.11).";non_debt
Merging to master and branch 1.5. Thanks @yjshen;non_debt
"We could make it abstract. And add Prometheus as a one of the implementations. It would be nice to have something out of the box.  Enabled during server creation. 
Using the argument of external repo could be used against everything.  Web server for instance. 
I donâ€™t really think it applies the same thing. 
We should move the discussion to the dev list.  Itâ€™s bexomong out of the scope of code review now.";non_debt
This will be very useful! Once this lands I'll see about wiring this up to the gRPC async APIs.;non_debt
LGTM, merged.;non_debt
Shell we backport this to 3.0 too? SPARK-30541 was merged to master so the flakiness should be there in branch-3.0 too.;non_debt
+1;non_debt
@michaelandrepearce ok all fix now, let me know if you want me to change anything else!;non_debt
I'm not sure about having this be a static method since it would end up constructing an instance of the object anyways, but I don't have strong feelings here. I lean towards giving it a name like `validateReferencedColumns` personally.;non_debt
Run Whitespace PreCommit;non_debt
@RongtongJin Splitting the large PR into many small ones maybe a good choice.;non_debt
"The inputBuffer is a safeRow in SortAggregateExec
`inputBuffer.getBinary(inputAggBufferOffset)` and `getField[Array[Byte]](inputBuffer, inputAggBufferOffset)` are equivalent.
Yes, it is better to use `inputBuffer.getBinary(inputAggBufferOffset)` directly";non_debt
for batch ingestion, I would expect if user submitted supervisor task fails then no segments are generated. however, from this it appears that segments are published as subtasks finish rather than at the end.;non_debt
Ok for now.;non_debt
Implicit CastExpr can't call analyze() outside , so targetTypeDef will not throw NullPointerException.;non_debt
@payert please review this PR;non_debt
emm. i agree with u.so i deleted it. btw, i think its time to review @YunaiV @wu-sheng @JaredTan95;non_debt
Excellent!;non_debt
@fayeshine #9814 was merged. Do you mind closing this PR manually? Thanks for your contribution!;non_debt
I spoke too soon it doesn't decide just tells you that it supports both so maybe if it supports html then give them html otherwise not, should work.;non_debt
@hetong007 can you please merge this PR.;non_debt
LGTM;non_debt
This was not working before #18704 . I just reverted the changes on that.;non_debt
Thanks, merged on master;non_debt
Refactor cmake to use modern feature;non_debt
cc: @g-boros;non_debt
@ewencp ï¼Œ thanks a lot for your comment, ~I modified this again, please check;non_debt
"Hi @mans2singh Seems this sensor only checks whether there is message in the queue, or not.
Do you think adding feature to check if specific body contents is in the queue would be a good idea?";non_debt
done;non_debt
See: https://github.com/apache/incubator-mxnet/pull/10486/files;non_debt
retest this please;non_debt
They are not added as allowed values of the UPGRADE_FROM_CONFIG yet, is it intentional?;non_debt
changed in a9572d8;non_debt
THRIFT-3770 Implement Python 3.4+ asyncio support;non_debt
retest this please;non_debt
"This can be configured on the oidc implementation side.  For example, in Keycloak you can configure it to just return the email and username in the token.
Edit: We may just need to introduce a configuration options that lists out what the expected claims are, and put this method call behind that.";non_debt
"fix #4929 
fix ClassCastException when run hive sql with udf";non_debt
"I think we can avoid the new field by doing
here and
Allowing the removal of the other connectionTerminated().";non_debt
[CARBONDATA-3142]Add timestamp with thread name which created by CarbonThreadFactory;non_debt
cc @rxin @srinathshankar @cloud-fan;non_debt
Done;non_debt
"Thanks for sending a pull request!  Here are some tips for you:
  3. Ensure you have added or run the appropriate tests for your PR: 
  5. Be sure to keep the PR description updated to reflect all changes.
  6. Please write your PR title to summarize what this PR proposes.
  7. If possible, provide a concise example to reproduce the issue for a faster review.
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
-->
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If possible, please also clarify if this is a user-facing change compared to the released Hive versions or within the unreleased branches such as master.
-->
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->";non_debt
I saw it as false for a moment for some reason. Nvm. Lgtm;non_debt
17165658-9610 review-44743137;non_debt
No translation...;non_debt
Oof, sorry, can't believe I left that in!;non_debt
"To add a nightly build Jenkins pipeline for v1.x, we need to create a new Jenkinsfile capable of building all the components without the ""Publish"" step.
- A ci/jenkins/Jenkinsfile_website_nightly
Tested successfully on Jenkins dev instance (see build output [here](http://jenkins.mxnet-ci-dev.amazon-ml.com/job/docs/job/connor-website-build-master/38/)).
Pipeline is available [here](http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-nightly-website-build-1.x/).";non_debt
"R: @emilymye 
cc: @chamikaramj 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.";non_debt
Thanks! Merged to master.;non_debt
This is still 'required' right? we're not making it an error, but it won't have any effect if not in inputCols.;non_debt
Simple code cleanup to remove full package name for FlatMapOperatorBase for FlatMapOperator#translateToDataFlow method;non_debt
ack;non_debt
Woops wrong repo. Sorry!;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7107/
--none--";non_debt
@merlimat Thanks for your comment, I will fix it later. I have created an issue to track this problem. https://github.com/apache/pulsar/issues/7161;non_debt
@JoshRosen Thanks for pointing out the compatibility issue. I have fixed it now.;non_debt
For the AIP I would expect building on top of FAB or equivalent. That gives OpenAPI endpoints out of the box.;non_debt
done in 141917b;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
Run Java PreCommit;non_debt
graph view mouseover task should increase stroke width of upstream / downstream;non_debt
I think we can just leave it without other options. It's rather corner case and I think it's fine to break such stuff since we're moving to Spark 3.;non_debt
Why is it not a Map here;non_debt
what about `1 + 2`? The requirement is a foldable int type expression, not have to be int literal.;non_debt
356066-469 description-0;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
@chickenlj Done;non_debt
KYLIN-3505 Fix wrong usage of cache in DataType;non_debt
TE: Refactor code to support more metric functions such as 'average' and 'count';non_debt
"# Summary
* Add functionality to wrap SQL Lab queries with a limit
A common problem people have is that they input a query in SqlLab to preview their data before hitting visualize to create a table, and then it takes forever to run. This can be a pain (on them and potentially on the database).
Currently one workaround is to manually enter a limit in the query editor, visualizing (which creates the datasource), and then later removing it from the base query (by editing the created datasource). With this in mind, an additional/alternative solution might be to allow users to edit the sub-query in the visualize modal before creating their datasource.
So here's a way to have a limit only in the context of SQL editor. The limit is only applied as a wrapper for select queries, and not saved in any persistent database.
https://github.com/apache/incubator-superset/pull/4834 - I like this idea, but not sure if every database supports prefetching. If going with this route, I'd have a configurable page size.
https://github.com/apache/incubator-superset/issues/4588";non_debt
"**RunningContext** designed for in thread propagation to avoid one object accessed by multiple threads. **Async APIs**  designed for the span across threads to finish and set tag.
Choose the right API fitting your scenario.";non_debt
"@sunjincheng121 Thanks a lot for your review. I have addressed all your comments.
@twalthr @sunjincheng121 I have also rebased to the deprecating window pr and delete the deprecated methods and classes in this one. Would be great if you can take a look. 
Best, Hequn";non_debt
33884891-528 description-0;non_debt
Update eventhub client dependency. Move Storm-Eventhubs dependency vesion definitions to parent pom. Introduce new schemes for message handling;non_debt
@zentol @fhueske I am merging the change with the extra space since this looks to have been the original intent. I've looked at both forms without finding a strong preference.;non_debt
TAJO-1013: A complex equality condition including columns of the same table is recognized as a join condition.;non_debt
"Ah, for PRB, it will be disabled by default unless it's manually enabled like I did in this PR:
https://github.com/apache/spark/pull/23117/files#diff-9657d18e98a7dc82ca214359cfd6bdc4R630
I'm gonna try to fix it anyway. I know it's discouraged for other people excpet the build manager (you) to touch the job like that in general so wanted to have a way to test it in PRB.
So, setting `SPARK_TEST_KEY` environment variable alone in PRB should be okay .. It won't affect the other PRs if you don't manually change the build script as i did here to enable it.";non_debt
"How do nested records end up being represented in the Solr document? Not saying anything is wrong here, just asking to understand how it works.
Lets say we have a person schema with top-level fields for ""firstName"" and ""lastName"", and ""address"", and the address field is of type record and then has it's own fields ""street"", ""city"", ""zip""...  
Does the resulting Solr document contain ""firstName"", ""lastName"", ""street"", ""city"", ""zip""? 
Would it make sense to have an option to include the parent field in the field names, so it ends up being ""address_street"", ""address_city"", and ""address_zip"" so that you know where those fields came from?";non_debt
you can set the defaultValue in the java code. PathParam can have the default value as well.;non_debt
"I agree service creation should be done by the scheduler backend code so to get rid of dependency on the submission client. 
Yes, I agree that we eventually should allow client mode including use cases that directly create `SparkContext`. But until we have a solid well-tested solution, I think we should disable it for now. We can always revisit this once we have a good solution. Regarding `kubernetes.default.svc`, yes, it's a good indication. But again, the driver service must exist. Unless we change to have the backend create that service, this still won't work.";non_debt
Thanks @HeartSaVioR;non_debt
32199982-213 description-0;non_debt
@aljoscha Thanks. This time it is green.;non_debt
`outputTopic2`;non_debt
MINOR: Pin to system tests to ducktape 0.3.10;non_debt
good move.  i think @andreaturli 's fixes from this class were incorporated in 1.8 so no more need for this.  can you confirm @andreaturli ?;non_debt
GROOVY-8855: Matcher.asBoolean() does not rely on matchers internal search index anymore;non_debt
Thanks @jackye1995!;non_debt
Hi @pnowojski , thanks for fixing the problem. Looks good to me. Same error exist in comments of `org.apache.flink.table.functions.TableFunction`.;non_debt
Run Java_Examples_Dataflow_Java11 PreCommit;non_debt
"Removes ""disallow"" pattern support
Addresses issue #1645";non_debt
I close this issue. Thank you again.;non_debt
Done.;non_debt
THRIFT-3758 TApplicationException::getType and TProtocolException::getType should be const;non_debt
NUTCH-2581 Caching of redirected robots.txt may overwrite correct robots.txt rules;non_debt
@jkbradley I've fixed it up. Thanks!;non_debt
[perf logging] Add timing event when browser tab is hidden;non_debt
This code file is copied from https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/balancer/ewma.lua, so we MUST keep the oirgin license and copyright, this is respect for other open source projects and a rule that Apache project must follow.;non_debt
I thought you said it would be `TSRemapConfigLoadStart` for future symmetry concerns.;non_debt
@weberxie this seems to be opened by mistake. Would you mind closing this patch?;non_debt
fix dubbo-rpc-http resouces setting name;non_debt
You need not use Persistent\* classes. Instead you can return lists/map etc. and in the clojure code call clojurify-structure on the result e.g.;non_debt
"I see : 
We use a tree-set to redeliver in order, though it shouldn't be strictly necessary. 
I guess we could either: 
 * Use ConcurrentLongPairSet and forget about order
 * Implement a `Queue< Pair<Long, Long> >` that is actually implemented as an array of primitive longs";non_debt
I just noticed that my IntelliJ code formatter may have been a little over-zealous with the TestSupport file. I can fix that if it bugs anyone.;non_debt
Yeah, it's a newly added method which will be called in kafkaStreamingExtractor.shutdown(), and the kafkaStreamingExtractor.shutdown() method did not call super.shutdown(), so I did not add it here as well. Also super.shutdown is meaning to check it's a decorator, which is not applied to this case as well.;non_debt
can this be `camelCase`?;non_debt
I don't think we can do this because `maximum_frame_size` can be smaller than the frame popped. We don't need to think about a window for flow control in this case, but we still need to think about a size of packet. MTU may be changed. You wrote `split()` for it, right?;non_debt
Build Failed with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/3012/;non_debt
Run Python PreCommit;non_debt
[WIP]Spark: Bump Spark 3 version to 3.1.1;non_debt
"I am afraid that we misguide people to think we or zipkin traces an OP, which needs 1ms but actually it is just a simple `+`(or others). Anyway, both ways should be fine. SkyWalking sometimes duration is 0ms too.
You can make the decision, I just ask and give you suggestions.";non_debt
I'm not sure if RetryPolicy and BackoffStrategy apply to LimitExceededException / ProvisionedThroughputExceededException but I can look into that. If so I think it makes sense to just configure this in the Kinesis client instead of having a BackoffRateLimitPolicy. What do you think @aromanenko-dev and @lukecwik ?;non_debt
Ok to test;non_debt
Okay. Let me have a simple test DS v2 locally and capture some screenshots of the web UI.;non_debt
"Updated the sonar report. Will see how does it work after the next build on **apache** repo:
https://github.com/apache/hadoop-ozone/actions?query=branch%3AHDDS-3710";non_debt
Sorry, I thought this was in a `private[ml]` trait that could be inherited by a public class.;non_debt
@beemarie `exec` deals with the set of env. variables specified in execOptions, its nothing to do with `wskdeploy`. Its more about injecting those env. variables (cloudant username etc) into env. where `wskdeploy` is being called:;non_debt
Minor, but I think we do not need to `setup-blob-permission` here, since we are doing it next unconditionally.;non_debt
31006158-2068 description-0;non_debt
"May I ask how to solve this issue in spark2.3?
I see that this flag has been removed in PruneFilters and EliminateOuterJoin. @viirya @gatorsmile";non_debt
Test failure is unrelated (FLINK-21647). Merging. Thanks again.;non_debt
I'm excited to see this merged, because it will help with registering a suite of objects (e.g. the taxi tables and functions) succinctly.;non_debt
why not do the check for post-aggregators naming here, before adding it to combinedAggNames?;non_debt
19961085-522 comment-246064224;non_debt
[ios] callNativeModule support unicode characters;non_debt
We need use them in `.cu` file.;non_debt
This is another test I think could be broken into distinct cases with a `@Before` to set up the default branch and table.;non_debt
increase the counter after successfully created the pipeline ? Might throw exception during the creation.;non_debt
Jenkins, test this please.;non_debt
For accuracy sake - my example snippet above will fail much earlier - due to `OpenHashSet. MAX_CAPACITY`. Though that is probably not the point anyway :-);non_debt
Yes, sure.;non_debt
LGTM ðŸ‘ make this cache field explicit, instead of implicit property;non_debt
206424-200 description-0;non_debt
[SPARK-9052][SparkR] Fix comments after curly braces;non_debt
"This introduced a new Coverity issues:
Assuming we don't revert this change, we need to fix this.";non_debt
That's sad ...;non_debt
KAFKA-8599: Use automatic RPC generation in ExpireDelegationToken;non_debt
Then maybe we should augment this to say that this stack trace recursively includes the causes of the exception;non_debt
add a newline after. thanks;non_debt
we don't do the put here. what if implementation changes and there will be two puts? do we need to keep these in sync?;non_debt
[BEAM-9865] Cleanup Jenkins WS on successful jobs;non_debt
LGTM +1, merging. Thanks @mattyb149!;non_debt
The `TestKubernetesClient` is shared with `KubernetesContainerTests` - so for now, changed to implicit ActorSystem (and left object/classes in current places). Good?;non_debt
Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/4811/;non_debt
Implement GET /api/1.1/roles handler;non_debt
Any reasons to have these empty methods?;non_debt
[doc] fix the wrong file extension name;non_debt
"This PR ads two additional metrics:
- loadbalancer_activations_<namespaceId>_count : counter for activation starts in load balancer
- invoker_container_start_<containerState>_<namespace>_<action>_ count
I also adds another info level log statement with container start information";non_debt
So its one test being failed in a different Hive version. I assume the test is being failed as expected because Hive fixed it in 2.0. Why didn't we just fix the test? Seems root cause was clear, easy to fix the test, and didn't break PR build.;non_debt
[BEAM-1286] DataflowRunner handling of missing filesToStage;non_debt
Thanks, @shardulm94! Great to have this fixed!;non_debt
31006158-1914 description-0;non_debt
From the code in waitUntilConnected, if false is returned, it indicates the deadline has elapsed(timed out).;non_debt
If create VM snapshot fails immediately after instance creation, the issue might exists.;non_debt
"suggest change ""dmClassName"" to ""providerName""";non_debt
"SPARK-2710 Build SchemaRDD from a JdbcRDD with MetaData
and a small bug fix on JdbcRDD, line 109
it seems conn will never be closed";non_debt
"@chie8842 Please review the commit guidelines:
My commits all reference JIRA issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from ""How to write a good git commit message"":
- Subject is separated from body by a blank line
- Subject is limited to 50 characters
- Subject does not end with a period
- Subject uses the imperative mood (""add"", not ""adding"")
- Body wraps at 72 characters
- Body explains ""what"" and ""why"", not ""how""
Your commit should start with `[AIRFLOW-1331]` and it should be a single commit (not two as now).";non_debt
Yes, most of the time it is, but now always. There other places includes happen like this, might worth the investigation why, but I don't think it is relevant for this PR.;non_debt
Fixes image tag readonly failure;non_debt
"Very good.
+1 to merge";non_debt
As mentioned in person, LGTM.;non_debt
@Premik, can you please squash your commits and rebase? A defect was recently fixed that prevented the PR test build from finishing properly. Thanks!;non_debt
Should the thread count be related to number of cores for the processor ?;non_debt
@yrqls21 PTAL;non_debt
last commit fix it;non_debt
No? It is the Table implementation for rest.;non_debt
This is some routine scrubbing that deals with some remnants of the prior dual py2/3 codebase.;non_debt
"Well, this is one step in the right direction. 
But we also have https://issues.apache.org/jira/browse/ARROW-2136
If we're going to respect the `nullable` flag, we also should not permit nulls to pass through silently
I also opened https://issues.apache.org/jira/browse/ARROW-5668 as a usability improvement";non_debt
@prabhjyotsingh @zjffdu @gss2002 Can you please help review this ?;non_debt
"what about 
#immutable";non_debt
"done.
On Tue, Jul 14, 2015 at 1:15 PM, Steve Varnau notifications@github.com
wrote:
Regards, --Qifan";non_debt
The two K8S tests failed again after my restarting.;non_debt
This property descriptor is missing a `displayName()` field.;non_debt
"I think this should be using the ""hadoopConfiguration"" object in the SparkContext. That has all the hadoop related configuration already setup and should be what is automatically used. @marmbrus should have a better idea.";non_debt
70746484-2232 review-244103695;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8959/
--none--";non_debt
@mengxr fixed!;non_debt
"Bumps [angular](https://github.com/angular/angular.js) from 1.5.11 to 1.7.9.
*Sourced from [angular's changelog](https://github.com/angular/angular.js/blob/master/CHANGELOG.md).*
- Additional commits viewable in [compare view](https://github.com/angular/angular.js/compare/v1.5.11...v1.7.9)
This version was pushed to npm by [petebacondarwin](https://www.npmjs.com/~petebacondarwin), a new releaser for angular since your current version.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/qpid-dispatch/network/alerts).";non_debt
Run Java PostCommit;non_debt
"@lexburner @kimmking i think three for statement can be combined too,my impl:
    public <T> List<Invoker<T>> route(List<Invoker<T>> invokers, URL url, Invocation invocation) throws RpcException {
        // filter
        List<Invoker<T>> result = new ArrayList<>()
        // all invokers that don't have ""tag"" parameter in url is a normal invoker
        List<Invoker<T>> normalResult = new ArrayList<>()
        try {
            // Dynamic param
            String tag = RpcContext.getContext().getAttachment(REQUEST_TAG_KEY)
            for (Invoker<T> invoker : invokers) {
                if (StringUtils.isEmpty(invoker.getUrl().getParameter(TAG_KEY))) {
                    // all invokers that don't have ""tag"" parameter in url is a normal invoker
                    normalResult.add(invoker)
                } else {
                    if (invoker.getUrl().getParameter(TAG_KEY).equals(tag)) {
                        result.add(invoker)
                    }
                }
            }
            // If no invoker be selected, downgrade to normal invokers
            if (result.isEmpty()) {
                return normalResult
            }
            return result
        } catch (Exception e) {
            logger.error(""Route by tag error,return all invokers."", e)
        }
        // Downgrade to all invokers
        return invokers
    }";non_debt
You're right.  That one is my bad.  I forgot to go and update all of these after I created the sign_in function.;non_debt
@binlijin for example, under docs/content, you can run `jekyll serve` to render the docs and the html page should be rendered;non_debt
"Changes include:
- Add `KafkaCommitOffset` transform, which expands to 
PCollection<KV<KafkaSourceDescriptor, KafkaRecord>> -> Map to KV<KafkaSourceDescriptor, (Long)offset> -> WindowInto(5 min FixWindow) -> Max.longsPerKey() -> CommitOffsetDoFn
- Change `ReadFromKafkaDoFn` to output KV<KafkaSourceDescriptor, KafkaRecord> instead of KafkaRecord only.
- Add commit offset expansion to KafkaIO.ReadSourceDescriptors:
ParDo(ReadFromKafkaDoFn) -->
Reshuffle() --> Map(output KafkaRecord) --> output
        - -> KafkaCommitOffset
**Note** that this expansion is not supported when using x-lang on Dataflow.
r: @aromanenko-dev 
cc: @lukecwik 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.";non_debt
ARROW-2265: [Python] Use CheckExact when serializing lists and numpy arrays.;non_debt
You may add comments why we need such non-trivial project, due to Calcite's behavior of ProjectRemoveRule. Also, it might be worth mentioning that the added Project may not be sitting right under Screen/Writer operator in the physical plan, as we would add additional Projects, for case likes * column expansion, or partition by column processing.;non_debt
hi @mimaison understand that this is not super critical, but since it is a simple fix, just want to check in and see if you think this approach is acceptable. would like to hear your thoughts and comments.;non_debt
I think this is fine, as it only gets called by the unordered parser, which means it will only sort the children in the unordered sequence.  In the tests I have, including tests where the unordered sequence is part of a larger sequence, childNodes only contains the elements of the unordered sequence.;non_debt
can we put this in SparkSessionBuilderSuite?;non_debt
Why are we removing this?;non_debt
Coverage decreased (-0.2%) to 38.742% when pulling **c818e364fe910821913ffcc83414c1b77241a374 on lindzh:fix_test_exeception** into **ccc2235ae9509f101971915ba0521109a82894b0 on apache:develop**.;non_debt
let's validate that all terms have `field` as a field, or directly take a list of BytesRefs?;non_debt
Why is opt_level here? Isn't defining it in PassInfoNode sufficient?;non_debt
Can one of the admins verify this patch?;non_debt
Removed `catch InterruptedException` and added to the method signature after not converging in an offline discussion.;non_debt
Good point, I take my suggestions back.;non_debt
"The prob is that here we want change the multiplier everyday when the current hour switch between peak and offpeak, there will not a config changed event.
Thanks. @virajjasani";non_debt
"@viirya Sure. Here's complete graphs
Before:
After:";non_debt
ARROW-1674: [Format, C++] Add support for byte length booleans in Tensors;non_debt
I think it might be worth `assert!` ing that there are no child data arrays so we don't have a silent (and hard to debug failure);non_debt
I do not think it makes sense to merge SupervisorActionsDialog and TasksActionsDialog into one high level TableActionDialog. You should have a TableActionDialog that is like the snitch dialog (never directly used but used by a `SupervisorActionsDialog` and a `TasksActionsDialog`. The `TableActionDialog` should take care of the (grid) layout and the tabs, the specific dialogs should add their own views.;non_debt
+1;non_debt
[FLINK-15416][task][network] Retry connection to the upstream;non_debt
Done;non_debt
Mixed-type mx.np.power;non_debt
"Thx a lot for your review~
Got it, because @SneakyThrows is the feature of lombok, I will suggest community to introduce the lombok in the future.";non_debt
"Do we need the stateChangedCounter in-addition to the stateChangedLatch? It looks like stateChangedLatch.countDown is exactly equivalent to ""stateChangedCounter == 1""";non_debt
There is no global table changelog topic -- this test really only write into the global input topic and your verification step was to verify that this write is picked up. Will add a corresponding test in a follow up PR.;non_debt
My C++ skills are not so great. After university I haven't done much, so I'm not familiar with the different versions. My preference would be to update #335 after this one gets merged and then enable the test again ðŸ‘;non_debt
"1. Defines Enum ServiceRole: `CONTROLLER`/`BROKER`/`SERVER`/`MINION` for the Pinot Components to manage
2. Defines interface: `ServiceStartable`, so each role could implement its own way to start/stop it.
3. Make `ControllerStarter`,`HelixBrokerStarter`, `HelixServerStarter`, `MinionStarter` to implement interface `ServiceStartable`.
4. Implement `PinotServiceManagerStarter` as a new entry point to manage all pinot roles lifecycles.
5. Make `ServiceStatus` could handle multiple Pinot roles.
6. Provide REST API to start/stop Pinot Roles with default configs.
7. Move `ControllerStarter`/`BrokerStarter`/`ServerStarter` commands to use new `ServiceManagerStarer` command. Hence all Quickstarts will use it transparently.
* Start PinotServiceManager along with controller/broker/server by default.
 * Start PinotServiceManager along with Broker&Server with default configs
 * Start PinotServiceManager along with Broker&Server with bootstrap configs.
* Once PinotServiceManager is up, it exposes APIs using swagger.
- `HelixBrokerStarter`, method `shutdown()` is replaced as `stop()`
- `HelixServerStarter` requires an explicitly call of `start()` to start it. In
the old behavior, there is no `start()` method and the constructor will also take care of start server.";non_debt
Thanks for review and merge, I'll delete the branch.;non_debt
Tried my best ðŸ˜„ Please check the new one.;non_debt
"idk what ""my-directive"" is or why it's in there. I'll get rid of it.
as far as ""acting weird,"" no, I don't think I have noticed that, but I'll look at it.";non_debt
changes in https://github.com/apache/incubator-streams-master/pull/2 should merge first;non_debt
@tqchen;non_debt
/pulsarbot run-failure-checks;non_debt
Thanks for rebasing!;non_debt
So what's a good name? I am not attached to Reflect, but I think Reflect should be in the name, if the function is called reflect.;non_debt
fixed;non_debt
Has this fixed your issue locally?;non_debt
@patricker All LGTM, +1. Merged to master, thank you!;non_debt
ARROW-759: [Python] Serializing large class of Python objects in Apache Arrow;non_debt
Let me find the correct code. (It's weird I attached a wrong code example.);non_debt
"Hi, @klion26 
This PR is ready to be reviewed. Could you help to review it at your convenience?
Thanks~";non_debt
`getSchema` will throw an exception when the schema contains an unsupported type. Now we use it to check if the table exists. Does it change current behavior? E.g., the insertion working before now fails.;non_debt
"Just to understand: is there a fundamental reason to not allow it there as well, or is it just ""didn't tackle that here""? 
Currently, in the python datasets code, I am not passing each path through `FilesystemFromUri`, so ideally a path passed to eg `GetTargetStats` would also be sanitized IMO.";non_debt
OK, I'm working toward using the `RexToLixTranslator` starting with #5544 to move to calc and make it easier. It will probably drag on for a while, so I'm thinking I'll revive this. But instead of the painstaking work of keeping these low-level unit tests, I will move to `SqlStdOperatorTable` or `SqlFunctions` at least, and just delete the tests and transform to SQL DSL smoke tests only.;non_debt
[BEAM-9547] Provide some top level pandas functions.;non_debt
HAWQ-1381. fix Core dump when execute 'select * from hawq_toolkit.__hâ€¦;non_debt
99919302-4984 review-557010119;non_debt
"What I settled on is that the `validVersions` would be a range encompassing all the supported protocol versions. The individual fields would use an open range when they're added (like `3+`), and then use a closed range when they are removed. For example, in KIP-441, we're going to set `validVersions: 1-6` and then set (for example) `prevTasks: 1-5`, while adding new fields `6+`.
This scheme would give us the smallest diff when we do have to bump the protocol version without changing any fields (which happens with some regularity). It also makes the most intuitive sense, IMHO, to use open ranges for the fields when you add them, because you would just assume they'll remain part of the protocol indefinitely.";non_debt
"Adding a dynamically updatable log config is currently error prone, as it is easy to set them up as a val not a def and this would result in a dynamically updated broker default not applying to a LogConfig after broker restart.
This PR adds a guard against introducing these issues by ensuring that all log configs are exhaustively checked via a test.
For example, if the following line was a val and not a def, there would be a problem with dynamically updating broker defaults for the config.
https://github.com/apache/kafka/blob/4bde9bb3ccaf5571be76cb96ea051dadaeeaf5c7/core/src/main/scala/kafka/server/KafkaConfig.scala#L1216";non_debt
"Okay, that makes sense.
The major difference from the better-known clients is, that their error behavior is more principled (i.e. controllable via a `SupervisionStrategy`).
In any case: If we make sure that:
1. Errors we know of  are handled
2. Errors we don't know of just result in a restart of the consumer/producer
I guess we are fine.
I'll review more thoroughly tomorrow. Testing new clients/completely moving away from our own ones is a much larger task that should be seen orthogonal.";non_debt
[SPARK-32808][SQL] Fix some test cases of `sql/core` module in scala 2.13;non_debt
Run Python PostCommit;non_debt
Remove this line;non_debt
"Ok cool can you add a comment here with the first thing that you said (""For locality preferences, ignore preferences for nodes that are blacklisted"")";non_debt
14135470-1113 description-0;non_debt
GUACAMOLE-116: Replace minified AngularJS with non-minified AngularJS.;non_debt
"Sounds good. Will take a closer look sometime this week. 
What I meant was, have you been able to test this out on any real production datasets yet?";non_debt
"The logic same to SizeInBytesOnlyStatsPlanVisitor#default, just add rowCount:
https://github.com/apache/spark/blob/711d8dd28afd9af92b025f9908534e5f1d575042/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala#L50-L58";non_debt
OBE'd by cached thread pool;non_debt
Was this supposed to be a scope resolution operator?;non_debt
YARN-10036. Install yarnpkg and upgrade nodejs in Dockerfile;non_debt
The partitioner comes into play during operations like `add` and `multiply`. This will later call the repartition method which does nothing if the properties match, but partitions the matrix otherwise.;non_debt
"Domain admins should not be able to assign the role of root admin to new users. Therefore, the role â€˜root adminâ€™ (or any other of the same type) should not be visible to domain admin accounts.
I created unit test that cover all of the modified code. Moreover, I tested locally.
Testing";non_debt
Is there no way to smoke test anonymous authentication with this PR, as there are no available anonymous authorization providers?;non_debt
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/15998/;non_debt
Could we rename the method to sth like getTransactionState?;non_debt
It seems Travis is killing jobs due to some memory issues. I'm not sure how to resolve this... but the branch builds locally just fine.;non_debt
"@aaltay like I said I don't know HadoopInputFormat enough to do a good review so I left general comments. I just took a look at last modifications regarding my previous comments and they are fine. 
For a more in depth review I suggested @iemejia last time, but he seems busy. @timrobertson100 I think you know HadoopInputFormat well, can you take another look?";non_debt
Correct. This patch uses a method which was introduced in that one.;non_debt
Run Dataflow RunnableOnService;non_debt
retest this please.;non_debt
"@limited - what types of delivery services does this relate to? You should update the Traffic Portal as well.
There are 4 html forms in TP for a delivery service. One for any_map, dns*, http* and steering*. You should add this field to each form where it applies
https://github.com/apache/incubator-trafficcontrol/tree/master/traffic_portal/app/src/common/modules/form/deliveryService
also, you'll need to add an entry for fqpacing to:
https://github.com/apache/incubator-trafficcontrol/blob/master/traffic_portal/app/src/traffic_portal_properties.json#L124";non_debt
@carlvine500 I think @ascrutae 's question is for you. Can you provide some answers?;non_debt
@jihoonson @clintropolis Could you let me know your comments if there's any? Thanks!;non_debt
Add perl-modules as install dependency for cloudstack-agent;non_debt
Are we sure the input is always unsafe-backed array? If it is `GenericArrayData`?;non_debt
Fixed;non_debt
Got it. I missed that during check this part only. Thanks.;non_debt
OK;non_debt
@Leemoonsoo @jongyoul @felixcheung  Please help review.;non_debt
"oh, yes, sorry, trivial error !!!
So it should be ""c >= 0"" strictly speaking";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR/1584/
Test PASSed.";non_debt
"Motivated by the fix in https://github.com/apache/incubator-brooklyn/pull/472 by @michaeldye - tests that scenario, and various others.
@michaeldye If you get a chance to review + comment on this PR, that would be great!";non_debt
#25794 for branch-2.4;non_debt
Merged to master.;non_debt
@sijie Key_shared subscription is enabled by default.  so i think we need to tell users can disable it at broker.config;non_debt
"This backports #8168 and #8162.
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.";non_debt
TINKERPOP-1391 issue with where filter;non_debt
Oh I see thanks @nandorKollar I will move this there then.;non_debt
@Leemoonsoo thanks. I will check it tomorrow. And what do you think of making a profile for yarn-pyspark?;non_debt
Modify the default value of `druid.server.http.numThreads`  to `max(10, (Number of cores * 17) / 16 + 2) + 30`;non_debt
Merged. Thanks the review @adoroszlai and @jojochuang;non_debt
ping @cloud-fan @jiangxb1987 @xuanyuanking Please take a look, thanks!;non_debt
Could this be renamed to GatewayReceiverXmlParsingValidationsJUnitTest.java;non_debt
actually we need `Option[IntervalRow]`?;non_debt
2 cents. feel free to take a call. Should we explicitly check only if spark version  is 2 or 3 we will proceed, if not, will throw an exception. just in case, in future, when we have spark 4, we don't need to make any code fixes.;non_debt
that happens during CheckAnalysis when we report errors.;non_debt
"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 1055410a5888dcada8045f57f5100dc09f095db9 (Wed Oct 16 08:32:58 UTC 2019)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* â“ 1. The [description] looks good.
* â“ 2. There is [consensus] that the contribution should go into to Flink.
* â“ 3. Needs [attention] from.
* â“ 4. The change fits into the overall [architecture].
* â“ 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier";non_debt
"On Jun 27, 2017 11:59, ""necouchman"" <notifications@github.com> wrote:
Okay, merged it to staging, now just need to merge staging to master. Is
this going to require a force?
No - never ever ever force push to the main repos.
Or create duplicate entries?
Yep.";non_debt
4710920-1327 comment-362996179;non_debt
Which part does not seem right? For single-value string-type virtual columns, the default null values are specified during initialization. e.g. for $segmentName, getValue should return the name of the segment. Previously in SegmentNameVirtualColumnProvider, getValue returns context.getSegmentName(). Now the segment name is initialized in addBuiltInVirtualColumnsToSegmentSchema for each segment and stored in the segment schema, so fieldSpec.getDefaultNullValue should still return the sgment name.;non_debt
We should clarify somewhere that bootstrap file scheme and the real base path cannot be different. We can bootstrap fine but while querying donâ€™t we need them to be the same hdfs, hdfs or s3,s3;non_debt
"This filesystem should be same as where we want to store the segment tars.
If we use s3a path as segment tar dir. Then this should be s3a filesystem.";non_debt
I will check the shard iterator type in the new method by merging these two 'getShardIterator' methods.;non_debt
"Missing Hyphen after `#service`
`#service` > `#service-`";non_debt
Sounds good. I'll do some benchmark in a follow-up pr.;non_debt
"@HuangZhenQiu thanks for your effort on this. And @bowenli86 the syntax is consistent with FLIP-69.
@walterddr  I think ddl syntax proposed in FLIP-69 is less possible to be challenged, while the changed API to TableEnvironment to support returned ddl eg `show tables` maybe need to discuss further. We are discussing in our inner team and plan to complete this flip before 1.10 release. I estimate there maybe two weeks or longer before the flip ready to vote.";non_debt
"Now server starts fine on Jenkins
However one test fails due  to older docker version
@houshengbo Whats the docker version on Jenkins machine? Per [docker docs](https://docs.docker.com/engine/reference/commandline/logs/#options) support for `--until` flag was part of engine api v1.35. So we may need to update the Docker version on Jenkins";non_debt
Coverage increased (+0.04%) to 69.79% when pulling **b0b91c842e09aa7fdb5c1dc216574daa43b437ea on dhalperi:dataflow-runner-speedup-2** into **1c6e667414788fe99f583fac39d458a4984ae162 on apache:master**.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2194/;non_debt
Maybe we could harmonize the null checking as you've done it.;non_debt
I'll put this lf in;non_debt
"With the change to check the boolean, I still get the following 3 tests in error. Illegal State error occurs when the test tries to start the aggregationProcessor again after a stop
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTimeoutExpressionRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTimeoutExpressionRestart:132 Â» IllegalState
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTimeoutRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTimeoutRestart:87 Â» IllegalState
[ERROR] org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest.testAggregateProcessorTwoTimeoutExpressionRestart(org.apache.camel.processor.aggregator.AggregateProcessorTimeoutCompletionRestartTest)
[ERROR]   Run 1: AggregateProcessorTimeoutCompletionRestartTest>TestSupport.runBare:58->testAggregateProcessorTwoTimeoutExpressionRestart:189 Â» IllegalState";non_debt
Do we need this? If we are in interpretation mode, we might as well not do anything Unsafe.;non_debt
"Currently, all background deletion services use the same intervals for deletion. It should ideally use different configs and the default value can be the same for all the configs.
https://issues.apache.org/jira/browse/HDDS-4367
Tested Manually";non_debt
Good point. Instantiating in every iteration is wasteful. I'll fix it!;non_debt
Where do we call this method?;non_debt
Let's elaborate the part of comment : `if is a kill`. I don't get it.;non_debt
â€¦tions;non_debt
LGTM too;non_debt
Compatible, Services Lookup is recursive.;non_debt
Sure;non_debt
"CLOUDSTACK-9092: L10n fix in ""Add LDAP Account page""";non_debt
If `v` has been used, then it will be `v`. Otherwise, it will be `v_1`.;non_debt
"Bumps [maven-compiler-plugin](https://github.com/apache/maven-compiler-plugin) from 3.8.0 to 3.8.1.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually";non_debt
[FLINK-14683] Fix RemoteStreamEnvironment's constructor;non_debt
I asked on #508 that several references are changed to 2.x. Also, review comments are to be addressed on the same PR, not by opening another one.;non_debt
206317-594 description-0;non_debt
[ZEPPELIN-4987]. Fix the interpreter dependency conflict caused by the CLASSPATH environment variable;non_debt
The subscription to ZK change notification path was made in ZkNodeChangeNotificationListener. So, in order not to miss any changes, we should apply the bootstrap logic after ZkNodeChangeNotificationListener is initialized.;non_debt
"[Reef-pull-request-ubuntu #109](https://builds.apache.org/job/Reef-pull-request-ubuntu/109/) SUCCESS
This pull request looks good";non_debt
SPARK-13450, commit ID: 02c274eaba0a8e7611226e0d4e93d3c36253f4ce#diff-9a6b543db706f1a90f790783d6930a13;non_debt
206459-60 description-0;non_debt
Renamed;non_debt
LGTM;non_debt
The community doesn't have any technical writer. We rely only on the contributions of other people and they are mostly developers.;non_debt
I'm not if it is though. in your link, there is no 'SYNONYM';non_debt
310611-1223 description-0;non_debt
v1.3.x - backports nightly test fix;non_debt
KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values();non_debt
Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1138/;non_debt
CAMEL-11954: Renamed ha packages to cluster;non_debt
The constraints are not deep copied though, you are right. Those should not change in any case I will change the comment accordingly.;non_debt
"Thanks for sending a pull request!  Here are some tips for you:
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
The `TableProvider` only accepts table schema and properties. It should accept table partitioning as well.
This is extracted from https://github.com/apache/spark/pull/25651, to only keep the API changes and make the diff smaller.
-->
Although `DataFrameReader`/`DataStreamReader` don't support user-specified partitioning, we need to pass the table partitioning when getting tables from `TableProvider` if we store tables in Hive metastore with v2 provider.
-->
not yet.
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->
existing tests";non_debt
the same as  https://github.com/apache/spark/pull/12574 ?;non_debt
Ditto.;non_debt
This is the problem of upstream, it shall be fixed by https://github.com/dmlc/tvm/pull/332;non_debt
ok to test;non_debt
looking at the code right now i may have figured out why the files aren't copied, but i find it odd that it supposedly works with hdfs. it actually should never copy additional files if no parameters are given.;non_debt
@advancedxy . We need a test case which failed without this PR. Please add a test case to show the validity of your PR.;non_debt
@pvary @lcspinter could you please review when you get the chance? Thanks!;non_debt
I've rebased this PR again. Can someone test and merge it before new conflicts arise?;non_debt
Why do we need this line? Have called `cluster.addMemberChild(spec)` to create `node1` originally.;non_debt
[hotfix][docs][udfs.md]fix the example of user-defined function in udfs.md;non_debt
"Thanks for doing this.
+1";non_debt
[sqllab/cosmetics] add margin-top for labels in query history;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1380/;non_debt
Yes, @neumarcx, I just wanted to establish the exact code you are using from Commons Math. What I linked is what you are importing in the current state of this PR.;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
we might not be able to set this here - we need to have it working with multiple versions of scala;non_debt
High level comment because I have not yet had time to look at this, but I think this pull request should directly be committed into Druid 0.6 to save the time to port things over.;non_debt
"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.";non_debt
[HELIX-717] Add api for get / set quota type, ratio and participant capacity;non_debt
"Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.";non_debt
/pulsarbot run-failure-checks;non_debt
Oops, i didn't saw that ~;non_debt
reviewing;non_debt
@ustcweizhou we will pick up your change ne try to fix the sort of the project list with jQuery.;non_debt
Update to 1.11.0-SNAPSHOT;non_debt
I found there's an existed Python test for multiple topics, I'll fix the test.;non_debt
@gatorsmile @liufengdb Anything else? Thanks!;non_debt
31006158-1909 description-0;non_debt
17165658-5992 description-0;non_debt
"Bumps [junit-pioneer](https://github.com/junit-pioneer/junit-pioneer) from 0.9.2 to 1.0.0.
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)";non_debt
"The `NoSuchMethodError` was caused by wrong datanucleus-core version. The target method is defined in datanucleus-core 3.2.2 but not in 3.2.10. The former is used when compiling against Hive 0.12.0, while the latter is used when compiling against Hive 0.13.1. Currently on Jenkins we first do a clean compile against 0.12.0, then build the assembly jar _without_ clean against 0.13.1 and run the tests.
This behavior left both versions of datanucleus-core in the `lib_managed` directory, and may sometimes mess up class paths. I'll open a PR to clean `lib_managed` before compiling Hive 0.13.1 in `dev/run-tests`.
I've seen this error once several days before right after the most recent Jenkins upgrade. Not sure why this issue wasn't detected before.";non_debt
Run Dataflow PostRelease;non_debt
@jihoonson let me know if you agree with the proposed wording and general page structure.;non_debt
"hi @zhztheplayer @michaelmior  I don't know if all the considerations are incomplete. I hope you can review and give me some Suggestions. Thank you very much.
best
qianjin";non_debt
should add payload => `action.payload.key`;non_debt
"@erikerlandson  thanks for looking at this.
A few questions:
1. After this pull request, does anything still use SimpleFutureAction?
3. This is not always lazy still right? See a test case";non_debt
I ran the failed test thousands of times to collect a few failures. I checked the core dumps. It seems it always fails exactly in the same place. Please check my comment in https://github.com/apache/incubator-mxnet/issues/11171. It's unclear why there is such a memory error. Since it always fails in the same operator, it's less likely that the error was introduced by this PR.;non_debt
"Let's focus on ""glob path"" here - from the quick look on org.apache.hadoop.fs.FileSystem javadoc, both `isDirectory()` and `exists()` seem to require the exact path, not glob path. The method which accept glob path is `globStatus()`, and there the API clearly names the parameter as `pathPattern`.
https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/FileSystem.html
And intuitively, it sounds very odd to me if someone can say the glob path is a directory while matching paths can be both files and directories, and same for checking existence as well. I don't think it's feasible to ""expect"" the meaningful value from calling these methods with glob path. That sounds to me as ""undefined behavior"" even any weird file system could return true for the case.
That said, I'd rather consider the input of glob path as ""wrong one"" and always return false (some sort of debug log message is fine if we really like to log). If there's a code relying on such behavior, I don't think that is correct. I'd rather say the possible paths should be populated before, and this method should be called per each path.
I still need to hear voices from others, but if the consensus goes to just disallow the glob path here, we won't need to introduce the new configuration.
@tdas @zsxwing @jose-torres @viirya @gaborgsomogyi Would like to hear your voices on this. Thanks!";non_debt
Does that mean no chance of 2.12 support for ~6 months?;non_debt
"Hello
I've rebased the branch to current master and modified the CLI to use the new property so that everything is clean.
LGTM now, tested OK";non_debt
"@hachikuji Ok, that sounds like we might have a bug in the packaging scripts in connect-hdfs and connect-jdbc then, although the development and packaged versions differ. Probably worth reviewing.
Based on this update and that we confirmed system tests are passing, LGTM.";non_debt
Upgrade Activemq to version 5.11.12;non_debt
interesting, so each reducer no longer just generates one file, how do we determine that?;non_debt
Yeah I definitely understand we probably don't want to introduce new functionality at the moment. I'll close this for now. @xtinec let me know if you want to chat about how it works if you decide to integrate cascading filter options!;non_debt
As discussed in the mailing list thread, this PR removes the AdminClient changes pertaining to `deleteTopicsWithIds` and `DeleteTopicsWithIdsResult`;non_debt
Does this mean only bulk formats can work with checkpoints?;non_debt
70746484-7612 review-589839657;non_debt
Jenkins, test this please;non_debt
If `prop_name` doesn't exist, we need to handle the failure of creating a subgraph property.;non_debt
This also needs to check that the type returned by `read(...)` is no longer bytes?;non_debt
Add CI building status and Apaches license icon;non_debt
Sure. I'll take a look next week.;non_debt
I'm closing this as to-be-preempted.;non_debt
"Marvin test code PEP8 & PyFlakes compliance:
CloudStack$
CloudStack$ pep8 test/integration/plugins/nuagevsp/_.py
CloudStack$
CloudStack$ pyflakes test/integration/plugins/nuagevsp/_.py
CloudStack$";non_debt
"In #5288, we moved Inline after the second fusion to avoid fusion checks the OpPattern of certain ops that can be eliminated by simplify inference pass, i.e. batch_norm, as external codegen needs to keep them. However, as memory passes should happen as a whole, this may introduce bugs. For example, we may have to bind constants first, like the change in the unit test. This fix reverts the change, but stopping fusing the function that should be handled by external codegen in fuse_op. 
This indicates that we may need to think about a more systematic way to skip functions inside a pass. Although outlining and inlining was used for the functions that are control by pass manager, it is not sufficient in such case.
@vegaluisjose @jroesch @comaniac @tqchen";non_debt
"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit cbf96e7bb990402b6e591d1f829d55d1e3b2cd01 (Fri Nov 13 18:55:08 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* â“ 1. The [description] looks good.
* â“ 2. There is [consensus] that the contribution should go into to Flink.
* â“ 3. Needs [attention] from.
* â“ 4. The change fits into the overall [architecture].
* â“ 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier";non_debt
"sample config insideï¼š
  ""tableIndexConfig"" : {
    ""invertedIndexColumns"" : [],
    ""loadMode""  : ""MMAP"",
    ""lazyLoad""  : ""false"",
    ""sortedColumn"" : [""secondsSinceEpoch""],
    ""starTreeIndexSpecConfigs"" : {
      ""enableStarTree"": ""true"",
      ""maxLeafRecords"": ""50000"",
      ""skipStarNodeCreationForDimensions"": ""uuid"",
      ""skipMaterializationCardinalityThreshold"": ""10000""
    },
    ""streamConfigs"" : {
      ""stream.kafka.consumer.type"": ""highLevel"",
      ""stream.kafka.decoder.class.name"": ""com.linkedin.pinot.core.realtime.impl.kafka.KafkaJSONMessageDecoder"",
      ""stream.kafka.hlc.zk.connect.string"": ""localhost:2181/kafka"",
      ""stream.kafka.topic.name"": ""pinot-test"",
      ""stream.kafka.consumer.prop.auto.offset.reset"":""smallest"",
      ""streamType"": ""kafka"",
      ""realtime.segment.flush.threshold.size"":""500000"",
      ""realtime.segment.flush.threshold.time"":""18000000""
    }
  },";non_debt
Can you replace this class by `ShardingScalingJob`?;non_debt
Handling the IO Exception in InternalTopicManager.close() now.;non_debt
"Moving WaitingForRegistration from constructor to Call method in tasks
Add Cancellation token to WaitingForRegistration method
Enable test case TestFailedMapperOnLocalRuntime to trigger the cancelation scenario in WaitingForRegistration
JIRA: [REEF-1549](https://issues.apache.org/jira/browse/REEF-1549)
This closes #";non_debt
"1. DEFAULT_FAIL_RETRY_SIZE maybe changed to DEFAULT_FAILBACK_TIMES
2. 100 is also too large. maybe 3 is enough.";non_debt
IGNITE-12701 : Disallow silent deactivation in CLI and REST.;non_debt
"The URL was contained in this doc.
The brokers method of the {@inject: javadoc:PulsarAdmin:/admin/org/apache/pulsar/client/admin/PulsarAdmin.html} object in the Java API";non_debt
Updated with url_len removed.;non_debt
Fix script to build native libs;non_debt
"Summary of changes:
   * Make `ProtocolSerializer` constructor injectable
   * Create named parameters for the constructor's input
   * Fix the `ProtocolSerializerTest` unit tests to use injection
   * Bug fixed: register the `Header` class regardless of the message namespace parameter
   * Minor improvements and refactoring
**P.S.** We'll need to merge those changes back into [REEF-1763](https://issues.apache.org/jira/browse/REEF-1763) at some point
JIRA: [REEF-1936](https://issues.apache.org/jira/browse/REEF-1936)";non_debt
19961085-857 description-0;non_debt
This is formatted message which worker node will send to coordinator to synchronize ndarray allreduce order across all nodes.;non_debt
Monomorphic processing of TopN queries with 1 and 2 aggregators (key part of #3798);non_debt
"Handle heartbeat function if owner-worker is not available. 
Function scheduling handles heartbeat function if owner-worker is not available.";non_debt
ping @cloud-fan;non_debt
@hogepodge is working on a fairly extensive refactor of the TVM tutorials--I think we should ensure there is a place for stuff like this. it would still be helpful to see your tutorial or perhaps the python scripts you're using with this code! perhaps there are other pieces we are missing or should checkin, or perhaps there is a particular way we should structure our docs to make this use case more straightforward.;non_debt
"Hi @jliwork , thanks for working on it!
But sorting on array of null type doesn't make sense to me, can you check the behaviour of other SQL system like Hive? And how about struct type? It's also order-able.";non_debt
@clintropolis - can you provide me a snippet of what needs to be done. I don't see a method available in `NilVectorSelector` that would taking in `ReadableVectorOffset`;non_debt
Done.;non_debt
"In the PR, I propose to use existing expressions `DayOfYear`, `WeekDay` and `DayOfWeek`, and support additional parameters of `extract()` for feature parity with PostgreSQL (https://www.postgresql.org/docs/11/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT):
1. `dow` - the day of the week as Sunday (0) to Saturday (6)
2. `isodow` - the day of the week as Monday (1) to Sunday (7)
3. `doy` - the day of the year (1 - 365/366)
Here are examples:
Updated `extract.sql`.";non_debt
52039373-4921 review-447727733;non_debt
@srowen I've reverted the previous change and inlined the relative tolerance logic, thank you!;non_debt
@dobozysaurus you can just force push your branch and keep this PR open if you want.;non_debt
[SIP-6] Refactor WordCloud;non_debt
LGTM;non_debt
Oh my. Good catch.;non_debt
"    âš ï¸ Please make sure to read this template first, pull requests that don't accord with this template
    maybe closed without notice.
    Texts surrounded by `<` and `>` are meant to be replaced by you, e.g. <framework name>, <issue number>.
    Put an `x` in the `[ ]` to mark the item as CHECKED. `[x]`
-->
     ==== ðŸ› Remove this line WHEN AND ONLY WHEN you're fixing a bug, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ”Œ Remove this line WHEN AND ONLY WHEN you're adding a new plugin, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ“ˆ Remove this line WHEN AND ONLY WHEN you're improving the performance, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ†• Remove this line WHEN AND ONLY WHEN you're adding a new feature, follow the checklist ðŸ‘† ==== -->";non_debt
Looks like there are conflicts. Are we still trying to merge this to 2.0?;non_debt
"Environment: kvm-centos7 (x2), Advanced Networking with Mgmt server 7
Total time taken: 24863 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr1659-t168-kvm-centos7.zip
Test completed. 45 look ok, 3 have error(s)";non_debt
s/give/given/;non_debt
Yes, I should refine this :-);non_debt
Good catch. I'm fixing.;non_debt
"Thanks a lot for your contribution to the Apache Flink project. I'm the @flinkbot. I help the community
to review your pull request. We will use this comment to track the progress of the review.
Last check on commit 02744178a91af131c0be0337a18e7cc65303495d (Thu Mar 26 17:11:23 UTC 2020)
**Warnings:**
 * No documentation files were touched! Remember to keep the Flink docs up to date!
* â“ 1. The [description] looks good.
* â“ 2. There is [consensus] that the contribution should go into to Flink.
* â“ 3. Needs [attention] from.
* â“ 4. The change fits into the overall [architecture].
* â“ 5. Overall code [quality] is good.
 The Bot is tracking the review progress through labels. Labels are applied according to the order of the review items. For consensus, approval by a Flink committer of PMC member is required <summary>Bot commands</summary>
  The @flinkbot bot supports the following commands:
 - `@flinkbot approve description` to approve one or more aspects (aspects: `description`, `consensus`, `architecture` and `quality`)
 - `@flinkbot approve all` to approve all aspects
 - `@flinkbot approve-until architecture` to approve everything until `architecture`
 - `@flinkbot attention @username1 [@username2 ..]` to require somebody's attention
 - `@flinkbot disapprove architecture` to remove an approval you gave earlier";non_debt
LGTM ðŸ‘;non_debt
It seems that the last one is enough?;non_debt
I don't think you need the `@JsonProperty` annotations here since only the main constructor is `@JsonCreator`, this legacy constructor would only be used when directly instantiating;non_debt
Done;non_debt
I am going to go ahead and merge. The build failures are due to a test regression in streams scala.;non_debt
Messed up the PR, closing this and reopening under new PR.;non_debt
"@davidyan74 : Do we need to care about following scenario
Operator C has two upstream A and B. Current latency of A > B and based on that C adds A as it's slowest upstream but then B's stats come and now latency of B > A but C is still pointing A as it's slowest upstream...";non_debt
"With a fresh install of NB11.1 and enabling all Java + JavaEE features, where the Amazon Beanstalk entry is missing is:
New Project -> Java with Maven -> Java Web -> Server & Settings -> JavaEE7 -> Add
Amazon Beanstalk should be in the list of Server types to add to the Project, but isn't anymore (Only Tomcat, Glassfish & Payara).  
I remember seeing something on the NB Mailing list about duplicate entries in this dialog after Payara was merged in, so suspect it may be related to this (rather than JavaEE8), but without tracing the related commits/PRs and going through the code I can't be certain at this point.
I will have a look myself if I get time.";non_debt
changed;non_debt
@fmcquillan99 New commit should return the original symbols;non_debt
"It's a pretty standard way in Druid of differentiating realtime and non-realtime servers. See CoordinatorBasedSegmentHandoffNotifier, DruidSchema, and CachingClusteredClient, all of which use this method to determine if segments are served by realtime servers or not. Maybe we could make this clearer by adding a new ""isRealtimeServer"" method.";non_debt
This was just an attempt. With the standard ASF infra approach we may need to use a personal access token of a personal account to be able to scan the github repo and check for PR. So it's not really feasible. I'm testing a different approach and probably we may remove the jenkinsfile later.;non_debt
LGTM.;non_debt
[BEAM-8156] Add convert_to_typing_type;non_debt
Fixed. Thx.;non_debt
added thread name;non_debt
Nicely done - I like how you validate the objects at the start.;non_debt
"Implemented in terms of std::runtime_error
Thank you for submitting a contribution to Apache NiFi - MiNiFi C++.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
just reading through your description here all the yarn pieces aren't in place so you have an admin type command to signal spark that a node is being decommissioned. But that means someone has to run that command on every single spark application running on that cluster, correct?  That doesn't seem very feasible on any relatively large cluster.  One big question I have is, are there enough use cases where just the command is useful?  Otherwise we are temporarily adding a command that we will have to keep supporting forever (or perhaps only the next major release).    Or does it make sense to wait for YARN (or another resource manager) to have full support for this?;non_debt
missing `|` at the beginning of the line;non_debt
`TestUtils.alterClientQuotas` called before `adminClient.close` can throw an exception. Admin extends AutoCloseable, so you can use try-with-resources.;non_debt
"Rebased for conflict resolving.
Travis passed: https://travis-ci.org/xintongsong/flink/builds/610258110";non_debt
"This PR is to revert the changes made in https://github.com/apache/spark/pull/16700. It could cause the data loss after partition rename, because we have a bug in the file renaming. 
Not all the OSs have the same behaviors. For example, on mac OS, if we renaming a path from `.../tbl/a=5/b=6` to `.../tbl/A=5/B=6`. The result is `.../tbl/a=5/B=6`. The expected result is `.../tbl/A=5/B=6`. Thus, renaming on mac OS is not recursive. However, the systems used in Jenkin does not have such an issue. Although this PR is not the root cause, it exposes an existing issue on the code `tablePath.getFileSystem(hadoopConf).rename(wrongPath, rightPath)`
--- 
Hive metastore is not case preserving and keep partition columns with lower case names.
If SparkSQL create a table with upper-case partion name use HiveExternalCatalog, when we rename partition, it first call the HiveClient to renamePartition, which will create a new lower case partition path, then SparkSql rename the lower case path to the upper-case.
while if the renamed partition contains more than one depth partition ,e.g. A=1/B=2, hive renamePartition change to a=1/b=2, then SparkSql rename it to A=1/B=2, but the a=1 still exists in the filesystem, we should also delete it.
N/A";non_debt
"@piiswrong , thanks. Yes, the use case is to manage a HybridBlock with changing graph topology. The idea is to provide an easy interface so that the user does not has to keep track of a HybridBlock for each graph topology but instead manage the graph topologies internally/within the mxnet framework code.
Instead of modifying the definition of HybridBlock, what about introducing a separate class that provides this functionality:";non_debt
ARROW-10885: [Rust][DataFusion] Optimize hash join build vs probe order based on number of rows;non_debt
Of course. I have created a JIRA https://issues.apache.org/jira/browse/FLINK-19412 to track it.;non_debt
"Yes, well that bit of code does dnsmasq's job. I think we should merge this and then find a way to call a release on a single lease by mac or ip.
LGTM ^^";non_debt
"# [Codecov](https://codecov.io/gh/apache/skywalking/pull/4740?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/4740?src=pr&el=continue).";non_debt
"Let's have `S3` in there as ""bucket"" is unclear, as in ""The S3 bucket...""";non_debt
Looking clean and green ðŸ’š;non_debt
@rhtyd Done!;non_debt
"Hello,
As far as I can see, and understand, the option `--enable-docs` doesn't work as expected in 7.1.2.
In `configure.ac`:
`enable_doc_build` is set to `no` if option is missing. If option is provided, nothing is done.
Then, `BUILD_DOCS` is set to true only if `enable_doc_build` is set to `yes` which seems to be unlikely to happen.
In `configure`:
If `--enable-docs` option isn't provided, `enable_doc_build` is set to `no` and never set to `yes` if option is provided.
When testing `enable_doc_build` to set `BUILD_DOCS_TRUE` and `BUILD_DOCS_FALSE`, first case is never triggered, leading to `enable_doc_build` to be always empty.
This means BUILD_DOCS_TRUE will be always equal to '#', thus leading man pages generation to be disabled in `doc/Makefile.in`:";non_debt
@chia7712 @guozhangwang I updated the code which using KafkaConsumer.metrics() to get rebalance time.  Could you please review it?;non_debt
It forces cluster scope, which still works locally if there is no cluster state management.  Is there a different way that you'd like this to be addressed?  Take a look at: https://github.com/apache/nifi/pull/1636#discussion_r109812402;non_debt
This sounds great, thanks!  I'll need to finish up with QA for 1.5 before taking a look, but please ping me if I don't return to review before long.;non_debt
Remove since the same as the default implementation.;non_debt
@viira can you review for me again please;non_debt
the relevant change is going from `FileSystem.get(conf)` to `FileSystem.getLocal(conf)` the formatting changes were done by maven.;non_debt
Add graphviz to all jenkins nodes. INFRA-17902;non_debt
Actually, at second thought: Could we separate the mk.sh and cp commands into two lines and just ignore the exit code of cp? It would be good to catch checkins that break the GUI build. We had that a while ago and had a hard time getting it back to work when we didn't notice the problem immediately.;non_debt
also update the scaladoc to give an example, and explain what the acceptable values are for dayOfWeek;non_debt
I was assuming we'd just merge this to trunk since we tend not to do new test development on the older branches.  I don't have a strong opinion, though.;non_debt
NIFI-5166 - Deep learning classification and regression processor witâ€¦;non_debt
"These leading space arise in Eclipse. After a closing ""}"" the cursor is padded to the same depth as the ""}"". Is there a way to change that without using a save action?";non_debt
173335706-1187 description-0;non_debt
i dont know about this. Could you explain briefly how does this annotation help?;non_debt
Although this is not related to this refactoring, ping @rxin and @kiszk because @kiszk seemed to want to fix the root cause of the failure.;non_debt
"Thanks Imesh. This is a improvement. We will merge this improvement after the Alpha release.Hence closing the PR
Thanks,
Gayan";non_debt
@tqchen updated. PTAL. Thanks.;non_debt
Updated the PR.;non_debt
â€¦Scanner.;non_debt
Thanks for fixing this! `ENUM` is not specified in the Parquet format spec, but according to parquet-mr 1.7.0, it is only used for converting Avro, ProtoBuf, and Thrift files. Double checked that in all cases `ENUM` is mapped to UTF8 string. So I think it's OK to always map `ENUM` to `StringType` regardless the value of `assumeBinaryIsString`.;non_debt
Adding event callbacks to BaseOperator (all tasks)!;non_debt
Cut them out will keep maintenance down;non_debt
[SPARK-23045][ML][SparkR] Update RFormula to use OneHotEncoderEstimator.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/3300/
Test FAILed.";non_debt
@vvcephei this PR lgtm.;non_debt
CAMEL-11881: Handling special arguments when declaring queues;non_debt
Closing this as it was since fixed by: https://github.com/apache/activemq/commit/4450c17c1c836a1daa7541dcb944f35798258197;non_debt
Thanks for suggestion but I find it harder to read.;non_debt
Don't the time and date cases need to be handled differently (calling TreeExprBuilder_MakeInExpressionDate etc.?);non_debt
retest this please;non_debt
retest this please;non_debt
This locking strategy looks like it could block multiple event threads while a new config is being loaded.  I suggest letting transactions use the old config while the new one is being loaded, using https://godbolt.org/z/3bq43z or something like it.;non_debt
why not use `dig +short $HOSTNAME` everywhere?;non_debt
Only if there is no rc2 :);non_debt
Yep and this is I.A.private, so..;non_debt
206417-169 description-0;non_debt
LGTM. Merging to master/2.1. Thanks!;non_debt
[MXNET-331] Single machine All Reduce Topology-aware Communication;non_debt
"Meta data
{
  ""version"" : 1,
  ""metaDataEntries"" : [ {
    ""hash"" : ""de081e28e7377785ea255b466988d4f0419c31f2"",
    ""status"" : ""DELETED"",
    ""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15818"",
    ""triggerID"" : ""de081e28e7377785ea255b466988d4f0419c31f2"",
    ""triggerType"" : ""PUSH""
  }, {
    ""hash"" : ""12851b775e442319ddb5ada5e2f1f6cf5df73ef4"",
    ""status"" : ""SUCCESS"",
    ""url"" : ""https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15841"",
    ""triggerID"" : ""12851b775e442319ddb5ada5e2f1f6cf5df73ef4"",
    ""triggerType"" : ""PUSH""
  } ]
}-->
* 12851b775e442319ddb5ada5e2f1f6cf5df73ef4 Azure: [SUCCESS](https://dev.azure.com/apache-flink/98463496-1af2-4620-8eab-a2ecc1a2e6fe/_build/results?buildId=15841) 
  The @flinkbot bot supports the following commands:
 - `@flinkbot run travis` re-run the last Travis build
 - `@flinkbot run azure` re-run the last Azure build";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2955/;non_debt
Packaging result: :heavy_multiplication_x: centos7 :heavy_multiplication_x: centos8 :heavy_multiplication_x: debian. SL-JID 242;non_debt
This will only be called when it's a new segment. You can call getAssignedInstancesForSegment() when assignedInstances are not provided in addNewSegment(), but it should not be null here. I would recommend add a Preconditions.checkNotNull() because when we call this, the segment ZK metadata has been updated.;non_debt
"Take your time. SkyWalking is a large ecosystem, even PMC member is not familiar all parts of the project. So, don't worry. With your further contributions, you will learn more.
And welcome to be 151th contributor of this repo.";non_debt
R: @Ardagan;non_debt
"I'm not sure if oracle can be associated with anything _reasonable_, but sometimes you have to play the hand you are dealt. :)
I can only answer your question with a question... Would there ever be a use case in the Decimal() class where the precision and/or the scale would be set to a negative value?
I'd have to assume that there isn't a use case for negative values given the way precision and scale are used and defined, but you'll have to forgive any ignorance on my part as I'm still fairly new to scala. I hadn't even browsed the source for spark until about one week ago. I'm still in the alpha stages of even testing spark in general, so while it's seemingly solved the problem for me in my testing, I could easily be overlooking something.";non_debt
retest this please;non_debt
Updated the LICENSE.txt in [3dba6a8](https://github.com/apache/nifi-minifi-cpp/pull/979/commits/3dba6a8440d962cd546a7975e4fa9b21386ab6d0);non_debt
@mjeelanimsft Just to clarify my latest comment: I'd to see a new class `ConfigUtilsTest` in which you explicitly verify the behaviour of `splitServerConfig`. Thanks.;non_debt
Put the If in the guard.;non_debt
"seems useless to record context for web container?  
and i don't know what is the context for web container, ðŸ˜‚";non_debt
Sure! Done.;non_debt
Seems unrelated.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/3974/
Test FAILed.
Test FAILured.";non_debt
I think the standard way to do this is to use `strings.Join`.;non_debt
"@bessbd thank you!
Would you be able to fetch the logs in `src/couch_views/.eunit/couch.log` and share in gist or txt attachment.";non_debt
[approve ci];non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
new zk client use native zk;non_debt
retest sdv please;non_debt
That depends on whether they share a filename prefix, which would be reasonable though. Could be a problem at the point of rolling though as we could miss some messages ðŸ¤«;non_debt
Are we planning to use this in different snapshots? So far it's only used for `CopyableValue`.;non_debt
"If invokers have been deployed success.
Then, it will report error when execute `ansible-playbook -i environments/<env> invoker.yml ` again.
because the invoker0 or invoker1 alreay exist, so below errors will appear.
So it is necessary to add the `judge condition` that whether the invoker has been deployed,
if already deployed, there has no need to deploy again.";non_debt
ok - it wasn't a big deal - i could have just merged your stuff into a local branch and tested.;non_debt
ha, good catch;non_debt
removed;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
Feature/merge apache 0.37;non_debt
ping on API review?;non_debt
Background job for running async chart queries in the context of the new `/api/v1/chart/data` API;non_debt
"Instead of using `_.find`, consider using the Javascript built-in [`Array.find`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/find) method.
Also, though, you could avoid iterating the whole array of assigned parameters for every parameter if it's made into just a Set of ids first:";non_debt
Add a strategy that allows slot restrictions for tasks with different prefixes, avoiding the task of different data sources to interact with each other because of slot resources. Although the current affinity strategy can achieve similar functionality, but in the cloud environment, there are still many restrictions, such as expansion or shrinkage will lead to ip and port changes, need to re-configure. Restrictions based on slots can be more flexible.;non_debt
@srowen @vanzin @squito @HyukjinKwon +Potetial reviewers, could anybody give some suggestions?;non_debt
Thanks a lot for the explanation. +1 for the PR.;non_debt
retest this please;non_debt
Fixed.;non_debt
Add default sort to phys_locations;non_debt
Delete SingleHopClientExecutorWithLoggingIntegrationTest_log4j2.xml;non_debt
JavaDoc;non_debt
Hey @mjsax @guozhangwang, before we start the discussion thread, maybe you could give me some initial feedback on the code or KIP?  Especially the logic has been a little deviated from our initial discussion on Jira, and I reflected those in the KIP. Let me know if you need more clarification, thank you!;non_debt
[KARAF-6226] Expose org.apache.felix.utils.properties package in Karaf config core bundle;non_debt
retest this please;non_debt
ditto;non_debt
sure;non_debt
no problem!;non_debt
"Thanks @GayathriMurali for the PR. I think we'll need to override the default behavior of getAndSetParams. Meanwhile, we need to invoke both convertVectorColumnsToML and convertMatrixColumnsToML. 
I'll send a PR to your repository for reference.";non_debt
Packaging result: âœ”centos6 âœ”centos7 âœ”debian. JID-1588;non_debt
Fine. Gonna change accordingly.;non_debt
"@piiswrong In kvstore, 
it requires optimizer to be dumped, but current ccSGD doesn't support. So revert first.";non_debt
@srowen @ajbozarth I created this PR without adding any new API. Just rewrote the way getApplicationList constructing the iterator. Can you guys take a look? Thanks!;non_debt
"""{}"" syntax";non_debt
Good point. Thanks for pointing this. I will update implementation for rpc.;non_debt
endpoint.get != null;non_debt
Also fix the dependency relationship between Bison and Flex;non_debt
Done.;non_debt
So, this is not currently written to the image?;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/1493/;non_debt
@srowen here is the companion PR to #16037.;non_debt
Move this before DELETE, because insertions are more than deletions.;non_debt
"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
 Merged build triggered.;non_debt
20587599-178 description-0;non_debt
"+1
Looks great to me!";non_debt
add available query granularity strings;non_debt
[FLINK-14543][FLINK-15901][table] Support partition for temporary table and HiveCatalog;non_debt
cc @yhuai @chenghao-intel;non_debt
It's the formatted explained plan. So it doesn't really have `Partition Statistics` and `codegenStageIds`.;non_debt
"I will file a ticket for the rightsubnetwithin configuration.
I will run the BVT tests and update the results here.";non_debt
Addresses the problem pointed out in [this comment](https://github.com/apache/spark/pull/2441#issuecomment-55990116).;non_debt
"This PR implements https://issues.apache.org/jira/browse/ZEPPELIN-74.
Plus, changing existing interpreter's name
spark.spark -> spark.scala
spark.pyspark -> spark.py
hive.hive -> hive.hql
tajo.tajo -> tajo.tql
All changes are backward compatible except for spark.pyspark -> spark.py.
User need either %spark.py, %py (when Spark is selected first) instead of %pyspark.";non_debt
To complete the thought here: I discovered that most of the Pyspark API methods accepted a `java.lang.Long`, in order to only optionally accept a seed. A few did not however. I also made those consistent, so that the Pyspark default of seed=None results in a null to this argument, which results in a default seed in the JVM, which means a random seed.;non_debt
Adapting partial code(file name start with H) to the sonar cloud rule;non_debt
+space;non_debt
OK, great.;non_debt
[mkldnn-v1.0] Add MKL-DNN LRN;non_debt
For instance, what if a Cell implementation had data members that cached all lengths... a column family length data member and a row length data member, etc. These methods wouldn't make sense to it?;non_debt
ðŸ‘  the if expression on the `filter` method could also be removed then;non_debt
Add a blank before and after the `>`;non_debt
Ok I will try to create a new component camel-slack-sdk;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Jenkins, retest this please;non_debt
"Rather than describing the attributes in prose, could we represent them in the schema as we did above for the old message format:
The explanation of the use of the timestamp type is useful though.";non_debt
"@clebertsuconic I fully understand your concerns, I could also just check for presence of characters on the right side of the separator. This would strengthen the validation by always checking for the queue part at least but ignoring the address which would still allow that test to succesfuly pass.
I guess I'd like to hear more opinions though, such as @jbertram's and @michaelandrepearce's, before closing the PR as something that would cause more harm than good ðŸ˜œ
Regarding the PR, I do not ""want"" the PR, I was just looking to contribute. I picked up this Jira issue as it seemed like a good first issue in a large project such as this one. If you can direct me to other good Jira's for starting up artemis contributions I'd be delighted x)";non_debt
"Hmm where does the ""token"" here come from? It ought to be the retrieval token, obtained from CommitManifest. I'm surprised that this argument even existed here, before the PR that introduced the retrieval token.";non_debt
@yhuai ping;non_debt
@merlimat After second thought, I found you are right! Excuse me!;non_debt
Thanks for your reviews!;non_debt
I can easily do a simpler getOrElse as is done in [spark-xml](https://github.com/databricks/spark-xml/blob/9f681939d16508abf4a12a129469ffebf87a2fa4/src/main/scala/com/databricks/spark/xml/XmlRelation.scala) which has more of a benefit of being lazier. But if an error does occur due to a mismatch, then the error is further from the original issue. I'm fine with either scenario, but at least wanted to give the other side for this one. Thoughts?;non_debt
"This reverts commit 1de3fc42829187c54334df1fb2149dc4aeb78ed9.
https://github.com/apache/spark/pull/30643#issuecomment-740454543
Thanks for sending a pull request!  Here are some tips for you:
  7. If you want to add a new configuration, please read the guideline first for naming configurations in
     'core/src/main/scala/org/apache/spark/internal/config/ConfigEntry.scala'.
-->
If possible, please consider writing useful notes for better and faster reviews in your PR. See the examples below.
-->
-->
Note that it means *any* user-facing change including all aspects such as the documentation fix.
If possible, please also clarify if this is a user-facing change compared to the released Spark versions or within the unreleased branches such as master.
-->
If tests were added, say they were added here. Please make sure to add some test cases that check the changes thoroughly including negative and positive cases if possible.
If it was tested in a way different from regular unit tests, please clarify how you tested step by step, ideally copy and paste-able, so that other reviewers can test and check, and descendants can verify in the future.
If tests were not added, please describe why they were not added and/or why it was difficult to add.
-->";non_debt
LGTM;non_debt
I think it might be worth calling this out on the user group too in order to ask if anyone's life has ever been saved by the `InsertSegmentToDb` functionality.;non_debt
Make 1000 configurable .;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/600/
Test FAILed (JDK 7 and Scala 2.10).";non_debt
This lands at `branch-3.0` now.;non_debt
"@cloud-fan 
In this PR, I set a unique output dir for partition overwrite operation, both dynamic and static partition overwrite.
cc @advancedxy @wangyum";non_debt
In that case you should probably fix it in `flink-kafka-connector/pom.xml`;non_debt
I'd rather choose stick to convention.;non_debt
"Thank you for submitting the pull request to the Apache Ignite.
In order to streamline the review of the contribution 
we ask you to ensure the following steps have been taken:
The description explains _WHAT_ and _WHY_ was made instead of _HOW_.
The following pattern must be used: `IGNITE-XXXX Change summary` where `XXXX` - number of JIRA issue.
the `green visa` attached to the JIRA ticket (see [TC.Bot: Check PR](https://mtcga.gridgain.com/prs.html))";non_debt
No throw exception? `dbTransactionMgr` could be `null`;non_debt
"Like `ConcreteRecordBatchColumnSorter`'s `next_column_`?
It would work.";non_debt
Yes. added support for null values.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1254/;non_debt
No. Those are not random numbers. Those empirical number took me a long time to achieve the best balance among speed, retries and reliability. Check details at this PPT https://iwww.corp.linkedin.com/wiki/cf/display/DWH/Google+Search+Console+Data+Ingestion#GoogleSearchConsoleDataIngestion-PPTPresentation;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/3202/;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Closes #550;non_debt
A `reset` in `TaskWriter` is just clear complete files?;non_debt
"Actually this branch has merge conflicts with the master. May be that is the reason? Please update your branch and test again. 
Also I saw python streaming tests handing in another PR of mine last night. I wonder if this is a Jenkins issue. Nonetheless please update your branch.";non_debt
"`DataStreamWindowJoin` should support outer joins, but at the moment it does not. 
Until it supports outer joins, I would not translate to `DataStreamWindowJoin`.";non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/3046/;non_debt
"Indeed it changes the behavior of how we close the producer as it won't be a force close after the refactor, however I don't think it changes the behavior of the test itself. Since we're not setting any response on the mock client, the initTransaction should timeout regardless of how do we close the connection at the end.
@huxihx as you're the author of the original code, would you please help us whether this change is correct or are there any details I'm missing?";non_debt
"I think I have addressed all the comments, please review this again.
@gatorsmile @HyukjinKwon @mengxr @dongjoon-hyun @WeichenXu123";non_debt
This class is audience private so presuming it ok changing this public static's name.;non_debt
fixed;non_debt
Done;non_debt
"          UT Added
Issue:
When having duplicate values in the data, filter results are wrong
Scenario:
Load data like below
a,11234567489.7976
b,11234567489.7976000000
Filter query on double_column = 11234567489.7976
Result - only either one of the row is selected
Actual Result - all the two rows should be selected(both the values will be same while parsing)
Logic of binary search while applying filter is changed (in case of duplicates in DOUBLE column)";non_debt
aren't these used by a test?;non_debt
"@franz1981 did you send a pr to @qihongxu branch so he can merge it and this pr picks it up?
Be great to see a final stat in @qihongxu test env";non_debt
Extension points for authentication/authorization;non_debt
Added execution command for prestoUI in the readme;non_debt
Thanks for the comments @aljoscha. I integrated them and I will merge soon.;non_debt
@dragon512 Is this crashing too ? Do we have some issue on 7.1.x branch ?? @bryancall;non_debt
The PR title doesn't match the changes;non_debt
Thanks for the review @ajayydv , the checkstyle issue is not introduced by this patch. I've opened a separate JIRA: https://issues.apache.org/jira/browse/HDDS-1350 so that we can get a clean cherry-pick for ozone-0.4 here.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/6936?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/6936?src=pr&el=continue).";non_debt
I did not get the meaning of index. it is supposed to be independent of other indexes. I think onBlockEnd event is enough for writing the index file.;non_debt
Add option for hybrid global and thread session pools;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22315/
Test PASSed.";non_debt
"@ijuma Sure! It seems that the last commit `e70a191d3038e00790aa95fbd1e16e78c32b79a4` in trunk caused `./gradlew checkstyleMain checkstyleTest` to fail. It prevents this and future PR from running tests. 
Looking at that patch, the check style failure seems to be due to the following change. Can we remove the log.isDebugEnabled() check?";non_debt
@xuanyuanking Thanks.;non_debt
@squito what do you think of this much of the change? Don't know if it just works on Hive 2 but I think this much is necessary in any event;non_debt
Same as above;non_debt
"[ActiveMQ-Artemis-PR-Build #583](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/583/) UNSTABLE
Looks like there's a problem with this pull request";non_debt
There is a handy method to define format out-of-box, please see https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/physical/impl/writer/TestTextWriter.java#L250 as example.;non_debt
"The map key could be like ""UTC+01:00"". ""American/Los Angeles"", ""PST"", etc., they are already cached in `getTimeZone`, but the method itself is a synchronized one.";non_debt
"[kafka-trunk-git-pr #521](https://builds.apache.org/job/kafka-trunk-git-pr/521/) FAILURE
Looks like there's a problem with this pull request";non_debt
@freeman-lab It's alright. Thanks!;non_debt
Pass linkers to the gtest cmake. This might fix the trusty failure maybe:;non_debt
HIVE-23786: HMS server side filter with Ranger;non_debt
Retest this please;non_debt
that api would work for now when we only have addresses but I don't think will be as nice if we add in count and the addresses are optional. Since its private we could have this and change later if needed but just thinking ahead perhaps we call them acquireAddresses and releaseAddresses;non_debt
Coverage increased (+0.08%) to 70.191% when pulling **4e9263e804d4297a94edf81aaeaf1b1f39cfd04d on afernandez:afernandez_impersonate** into **52a9f2742b5003e81604da1494a452f677cbe33c on apache:master**.;non_debt
Perhaps it's a moot point with the [switch to docsy](https://lists.apache.org/thread.html/r7fa6d710c0a1959cce5108e460d71c306ce5756cf96af818b41cb7ca%40%3Cdev.beam.apache.org%3E) going on;non_debt
Proposal of commit author validation using a GitHub action: https://github.com/marketplace/actions/check-author-name-and-email;non_debt
"`SaveDLTensor` function declaration is `const DLTensor* tensor`, but implementation is `DLTensor*`, if we pass `const DLTensor*`, we could compile successfully. However, when we run tvm, we will meet symbol can not find. We need to unify the interface and implementation so that we could avoid this runtime error.
@zhiics @tqchen please help to review";non_debt
:+1:;non_debt
ditto;non_debt
"""It is used to prevent denial of service type of attacks, to prevent filling up the heap or disk space."" is not applicable here I believe. The statement is for ""Max Request Size"".";non_debt
Looks good ! Merging it in now...;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4802/";non_debt
Updated PR after discussion with @zregvart and @jeremyross . I will add `rawPayload` support in other composite operation.;non_debt
ok let's leave it.;non_debt
This is minor fix to correct debug info.;non_debt
[Improvement-4187][Master] Add log exception stack information;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1353/
Test PASSed (JDK 8 and Scala 2.11).";non_debt
This is a refactor oversight, it used to be size / 2.;non_debt
retest this please;non_debt
INFRA-15440: allow self-serve to update qmail files;non_debt
"I see. I will update this PR with only `expression[Cardinality](""cardinality"")`. I will also update the description and JIRA later.
Is it OK with you? @gatorsmile cc: @ueshin";non_debt
AMBARI-24762. Ambari server continues to send request updates after all commands were completed.;non_debt
Looks good to me, but changing `?term_size/1` scares me. @davisp - second opinion, please?;non_debt
fixed;non_debt
"it built with java 8.  switched to 11.  nifi starts up and appears to be working great.
I did notice this on startup
nifi.sh: JAVA_HOME not set results may vary
Java home: 
NiFi home: /Users/joe/development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT
Bootstrap Config File: /../development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT/conf/bootstrap.conf
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.nifi.bootstrap.util.OSUtils (file:/../development/nifi.git/nifi-assembly/target/nifi-1.9.0-SNAPSHOT-bin/nifi-1.9.0-SNAPSHOT/lib/bootstrap/nifi-bootstrap-1.9.0-SNAPSHOT.jar) to method java.lang.ProcessImpl.pid()
WARNING: Please consider reporting this to the maintainers of org.apache.nifi.bootstrap.util.OSUtils
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release";non_debt
I think it's fine, I think I was seeing it a lot because I was running tests using JDK 8 but I had built with JDK 11 , so the metadata calls were constantly erroring out. Normally I would expect this to log a few times and then resolve;non_debt
@markrmiller can you take a look?;non_debt
"This adds a fine tuning example for BERT based off of the following Gluon NLP example
[https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html](https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html)
There is a PR in the GluonNLP repo for the export script https://github.com/dmlc/gluon-nlp/pull/672
The original BERT infer example directory has been changed from `bert-qa` to `bert` to accomodate both the QA example and this one.
A walkthrough in the form of a Jupyter notebook has been provided for the sentence pair classification based off of the Gluon NLP one. A markdown version of the notebook has also been provided for those who don't have the `lein-jupyter` plugin.
Example results for sentence pair classification after 3 epochs:
which is comparable to the sentence embedding results in the gluon nlp notebook
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- Move BERT QA infer example within a broader BERT example
- Add BERT Sentence Pair Classification Example along with notebook
- Tweak the Callback Speedometer so that the results will be printed out in the notebook instead of having to look at the console for the logging.
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
sure;non_debt
I might miss something obvious here: why this line (used for exclusion) is no longer required?;non_debt
@Kh4L @wkcn  What is the reason for moving the exisiting function to mshadow?;non_debt
My suggestion is to only remove things that have been deprecated in 0.11.0 or older. One year seems like the minimum we should provide for users to migrate.;non_debt
@zhreshold In Horovod case, each training process is attached to a GPU. If we do not specify device_id for the cpu_pinned context, all processes will use the memory in GPU 0 (because the default device_id for cpu_pinned context is 0) and cause out of memory error. I had a similar [enhancement](https://github.com/apache/incubator-mxnet/pull/13980) for ImageRecordIter.;non_debt
I see, sounds good.;non_debt
@gianm thanks I have added tests and the start intervals;non_debt
Yes, that is what I also expected. Just wanted to be really sure.;non_debt
"- Traffic Ops
- Traffic Portal
- Run the topologies API tests in CDN-in-a-Box, make sure they pass.
- In TP, create a topology where one or more edge_loc cache groups have a edge_loc parent or secondary parent. Ensure the warnings are displayed and can be save successfully. Also ensure that edge_loc cache groups can only parent edge_loc children.
Includes a bug fix. `GET /topologies` used to return `null` for 0 topologies, now it returns an empty array.
- master (2dd9a0cdf1)
Otherwise, not a bug fix.
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License"") you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->";non_debt
"One the other hand I can get my PR submitted tonight provided I can get the test suite to run some sane way. I'm an Eclipse user and I guess I found the place where to run the desired test suites, problem is that when I do right-click ""Run as"" -> ""JUnit test"" for the stomp package in integration-tests I get a lot of errors about classes not found, some ExceptionInInitializerErrors due to <code>Caused by: java.lang.IllegalArgumentException: Invalid logger interface org.apache.activemq.artemis.tests.integration.IntegrationTestLogger (implementation not found in sun.misc.Launcher$AppClassLoader@409a44d6)</code> etc, so I suppose my setup needs some changes, unfortunately I'm not sure what...";non_debt
Remove moot `version` property from bower.json;non_debt
Ah okay, makes sense.;non_debt
"Hi @automaticgiant , unfortunately, 'exec' command added into nifi.sh by #966 broke `nifi.sh restart` to work properly. I tried fixing it with this PR.
I confirmed that 'exec' is taking its effect, but I am not sure if it still work as you expected, i.e. whether `nifi.sh run` can be supervised by runit ... etc. Would you try this PR and check if it works?
Thank you in advance.";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/2340/;non_debt
I fixed the problem you mentioned, what should I do next, do I check the ci again~;non_debt
if `static row` is the only outdated data, then `BTreeSet.Builder<Clustering> toFetch` would be empty..so `ClusteringIndexNamesFilter` will has no clusterings and query entire partition in `querySourceOnkey()`..;non_debt
Sure, I will change the default to be `connectionTimeoutMs`;non_debt
70746484-6274 review-483721402;non_debt
"The tests came back with good and bad news. The good news is that the newly added tests passed on the build server. The bad news is that the VS 2013 build failed at the compilation stage:
Do we still care about VS 2013 as a target?";non_debt
Build Success with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/3296/;non_debt
"@cloud-fan (Also left on the JIRA ticket) - Sorry this has dropped off my radar for so long - work + life took me away from it for a while. So looking at the PR review comments and better understanding Broadcast Variable behavior (and some of the changes that took place in the 2.X series), it seems like simply trying to close Broadcast variables won't work as intended. However, I believe the underlying concept (driver-scoped shared variables, where the variable lives until the job is done or the driver removes it) is still worth pursuing. Being able to scope shared resources (like DB connection pools, which may need to change per phase of a job, or be able to be disposed of early in a process, which makes static variables not useful). Given that, I'd like to propose we add a new concept, similar to Broadcast Variables, called, perhaps, Scoped Variables. The intent would be for these to be scoped by the driver, be relatively small from a memory-consumption perspective (unlike broadcast variables, which can be much larger), and to be held in memory until explicitly removed by the driver. Most of the infrastructure work for broadcast variables supports this use-case, but we'd need to have either a ""non-purgable"" type in the MemoryStore, or some other store specific to these new scoped variables, in order to prevent them from being evicted like cached items are.
Thoughts on this? I'll start working on updating the PR to support something like this sometime today, but it might still take a while to get something workable put together, so I'd appreciate any feedback when someone has the time.";non_debt
Mostly because of the checkpoint stuff :(;non_debt
20089857-1747 description-0;non_debt
[SQL Lab] Wrap more logic with feature flag;non_debt
LGTM. I will merge after tests. Thank you.;non_debt
revert or fix forward @GabrielBrascher your choice for my part,, but with priority please.;non_debt
"@shaneknapp TRDL: Now SparkR supports both testthat 1.0.2 and 2.0.0. I will backport this to branch-2.4 at #27379 as well.
So, as a reminder:
- We should upgrade testthat in Jenkins to 2.0.0 at any earliest time you're available
- We should upgrade R version to 3.4.x after Spark 3.0 release.";non_debt
"Here, val is ""non-negative"" rather than ""positive""
Let's rename methods.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2831/
Test FAILed (JDK 7 and Scala 2.10).";non_debt
that's why I said we need to change the delay (e.g. 5s) instead of 0 for both submiting and killing.;non_debt
"Here are my numbers, and no I didn't do it headless but I was not doing anything on the laptop at the time it was running except looking at Activity Monitor to be sure nothing else was using too much CPU.
The setup is
MacBook Pro (Retina, 15-inch, Mid 2015)
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
macOS 10.12.6
java version ""1.8.0_121""
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)
The versions tested are 1.2.0-SNAPSHOT (b24f5d87cbcd4faeed651e45a8e673db52469a46)
Metrics V2 (78d3de44cf0705efd199fb64b7ef092e23c3285b) which is 1.2.0-SNAPSHOT + 99bcf68 merged in.
The Numbers:
1.2.0-SNAPSHOT
Metrics V2:
At 50,000 sentences per second on a single worker both can keep up, but the mean latency appears to be about 200 to 300 microseconds slower 9.6 ms vs 9.9 ms.  The bigger issue in my mind is the CPU time as it went up from about 79 seconds (max 30 second period) every 30 seconds to 92 seconds (min 30 second period after stabilizing). That was at least a 16% increase in CPU usage.
At 75,000 sentences per second metrics v2 was just able to keep up with that throughput while 1.2.0-SNAPSHOT could handle it easily. I will not compare the latency because metrics v2 didn't stabilize enough for me to feel good reporting the numbers, but the CPU time went up from 118 seconds every 30 seconds to 130 seconds.  That was an increase of at least 9%.
When I tried to do 100,000 sentences per second neither of the two could keep up, but 1.2.0-SNAPSHOT hit a maximum of 97k while metrics V2 hit a maximum of 78k or 20% less maximum throughput.  Only here did we see the CPU usage be lower for metrics V2 with 134 seconds vs 141 for 1.2.0-SNAPSHOT, but if we take into account the actual throughput and look at CPU seconds needed to process 1k sentences per second it is 1.7 for metrics v2 vs 1.5 for 1.2.0-SNAPSHOT.
I am not saying that these changes are a blocker for the code going in, but I really want to understand what is happening here";non_debt
not even for statics?;non_debt
"The test is good! +1
https://ci.bigtop.apache.org/view/Test/job/Build-Deploy-Smoke-Test-Pull-Request-All-Distros/";non_debt
4766638-436 description-0;non_debt
Will do.;non_debt
I will fix this.;non_debt
cfg points to broken link https://airflow.apache.org/howto/enable-dag-serialization.html;non_debt
20587599-10502 review-359579267;non_debt
Can one of the admins verify this patch?;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/735/;non_debt
41348333-234 summary-0;non_debt
31006158-5761 description-0;non_debt
"Updates to scalatest 3.2.0. Though it looks large, it is 99% changes to the new location of scalatest classes.
3.2.0+ has a fix that is required for Scala 2.13.3+ compatibility.
No, only affects tests.
Existing tests.";non_debt
31006158-269 description-0;non_debt
"This PR is dependent on https://github.com/apache/beam/pull/13495 - do not merge it before this one is merged.
Added integration test for Bigtable for BeamSQL and done some refactor to test utils to avoid package and classes errors in GcpApiSurfaceTest.
I've tested  BigtableTableIT with real Bigtable - everything seems to be working.
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.";non_debt
Removed.;non_debt
/pulsarbot run-failure-checks;non_debt
remove this;non_debt
ARROW-1589: [C++] Fuzzing for certain input formats;non_debt
"Several fixes applied:
The auto-creation loop was skipping over segments if consecutive partitions had missing segments.
The auto-creation logic was always taking the earliest kafka offset (assuming the error is
that of offset not being found), but this may not be the case. So, if we have an older segment whose
end offset is higher than the earliest available kafka offset, we should pick the higher one.
Added tests for these cases.";non_debt
Isn't an empty delegation block enough? Any reason to have a second kill switch?;non_debt
Topologies support for assigning ORG servers to Delivery Services;non_debt
"quickly and easily:
   `[BEAM-<Jira issue #>] Description of pull request`
       Travis-CI on your fork and ensure the whole test matrix passes).
       number, if there is one.
       [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---
R: @dhalperi";non_debt
LGTM. Testing locally and gonna merge this :+1:;non_debt
Should it not be directing people to Apache download as official place to get distribution;non_debt
"`mktest` generate them for convenience when we need to use them (not find and add them manually).
I think it's OK to leave them in a test file.";non_debt
I'm guessing this is the new way to apply migrations. Never mind my comment above;non_debt
ping @ijuma @guozhangwang;non_debt
Why are we removing `compaction.force.reprocess`? It seems to me it is a reasonable use case that users want to re-compact the input every run, even if the output was already compacted.;non_debt
why this has to be done from lifecycle handler and not directly after creation of the provider ?;non_debt
"Thanks! LGTM.
R: @aaltay, can you please merge?";non_debt
50904245-12828 comment-691328440;non_debt
"# [Codecov](https://codecov.io/gh/apache/jmeter/pull/377?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/jmeter/pull/377?src=pr&el=continue).";non_debt
"Update ActiveMQConnection to change behaviour, getMetaData and stop methods.
Add test to avoid regression.";non_debt
Can we put all these types in an array or other kind of data structure and do lookups to determine the least-common-ancestor?;non_debt
LGTM and merged to trunk.;non_debt
@amithadke Thanks for making these changes. Unfortunately, there have been major changes to HashJoin since it now does spilling. So this code will have to be refactored significantly. Additionally there will have be a design discussion on how to handle schema changes with spilling, since I suspect handling that will be non trivial. Since this PR has been inactive for a few years and would require a good amount of work to update. I will close this PR. Please feel free to reopen this PR if you would like to continue forward.;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3534/;non_debt
I've pushed a HOTFIX commit to address this issue, cc @mjsax @ableegoldman;non_debt
New Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/2308/;non_debt
LANG-1168: Add SystemUtils.IS_OS_WINDOWS_10 property;non_debt
Doc: [TS-4113] Add value 4 to proxy.config.http.cache.cache_responsesâ€¦;non_debt
62117812-7421 review-449485668;non_debt
@marmbrus Can we trigger a test for this?;non_debt
Will remove.;non_debt
"With introduction of interpreterGroup following code from the above example doesn't work anymore. I could track upto different InterpreterContextRunnerPool getting created for multiple interpreters and only one have interpreterContextRunners populated with all my paragraphs. So, sparkInterpreter when sends event to another RemoteInterpreter, it finds the list empty there.
z.angularWatch(""selectedTable"", (before:Object, after:Object) => {
    z.run(2, selectedTableContext)
    z.run(3, selectedTableContext)
    z.run(4, selectedTableContext)
})
It will be great if somebody can fix it asap (because we have a demo pending in another day or two).";non_debt
Wow, I see your point. Then a single abort message back to the Coordinator is enough in this case. Why 10 min btw?;non_debt
retest this please;non_debt
@rxin Please help verify whether this change for R is correct, thanks!;non_debt
"This is used only in AbstractManagedStateInnerJoinOperator.createStore().
In the interest of keeping the number of files less, can the content of method be directly used in createStore?
It just seem to create a new object of type ManagedTimeStateMultiValue.";non_debt
I'm using whatever cast as decimal is using here, but I think it is a bug to by default cast to USER_DEFAULT, which has scale = 0.;non_debt
ok.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6668/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
"Parse expiration time and reload config at time out, expiration is assumed to be a timestamp in seconds since epoch.
The reload is scheduled in the future after calculating the time difference between the expiration time and current time, the scheduled time can be up to 1 hour prior to the expiration time. 
- Expiration time more than 1 hour, schedule the reload 1 hour prior to the expiration time.
- Expiration time less than 1 hour but more than 15 minutes, schedule the reload 15 minutes prior to the expiration time.
- Expiration time less than 15 minutes, schedule the reload at the expiration time.
Setting the expiration to 0, or comment out the expiration time effectively disables the auto reload feature.";non_debt
Left outer join's cardinality is always greater equal with its left input's cardinality. So this plan is always better than sorting on join's output. ðŸ‘;non_debt
"in #2200 [first commit](https://github.com/apache/incubator-pulsar/pull/2200/commits/6757851bf86b1b8daa0137463ecbf44875075af8) had change to add `--subscription-type` but based on feedback we have added flag for `retain ordering`.
Isn't this PR address the same thing. if `guarantee is set to effectively once` then sub type will be set as FAILOVER else sub-type will be configured as per ordering-flag.  is there anything we are missing here?";non_debt
@mxnet-label-bot update [pr-awaiting-merge];non_debt
There is still an issue with mojarra 2.0 PostAddToViewEvent, Head and StateHandling in mojarra 2.0. No more energy to find a workaround for the crap mojarra impl.;non_debt
@alphalfalfa can you rebase?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4733/
Test FAILed (JDK 8 and Scala 2.12).";non_debt
Improving bootstrap;non_debt
please change these to be `ENV.api['latest']`.  once #4633 gets merged that will be API v3 and root will still be v2.  You can also hardcode in /api/3.0 if you want with a TODO to fix it later if #4633 isnt merged as soon as we would like this to be;non_debt
Will make it a counter. :);non_debt
Should we add `@Expremental`? How to decide what piece of SDK is user facing?;non_debt
"Grammar. Should be ""... supports ...""";non_debt
shouldn't this one and the ones below all have `?PageSpeed=off` ?;non_debt
Any reason this is 3.8.0 and not 3.8.1?;non_debt
"CAn you mention briefly how backwards compatibility will work after these changes?  I assume this is somehow handled within parquet itself -- just a pointer to the relevant info would help.
Also I assume I should not bother triggering tests yet, as automated builds will fail without a published version of parquet?";non_debt
Merged to master.;non_debt
jenkins, retest this, please;non_debt
Can we push the sorting to the spark shuffle machinery? repartitionAndSortWithinPartitions(). It cheap and practically free;non_debt
"Hi Jan, 
I could do the drill now with make release, create TAR, unpack tar, ./configure, make && make install
Anyhow: running Couch afterwards did fine. 
BTW: so did copying in 1.6.1 files, calling them from local port, replicating them up to the cluster port.
That's great so far!
     Sebastian";non_debt
"@nvazquez thanks for the update, yes it's known issue that when running tests all at once it might cause issues. If you have a look at our Travis runner, we're running each test one at a time:
https://github.com/apache/cloudstack/blob/master/tools/travis/script.sh#L43
Trillian runs them one at a time as well, you may take some hints from Trillian's smoketest runner:
https://github.com/shapeblue/Trillian/blob/master/Ansible/roles/marvin/templates/smoketests.sh.j2#L70";non_debt
"This PR fixes the query like:
which raises exceptions:
The reason is that `HOUR` is a pinot UDF which CalciteSQL parser doesn't recognize and marked the SQL_KIND as `OTHER` with the real function name inside the function info.";non_debt
we can remove it now if you want, but it will be needed for the UI work but we can add it back in there if you want;non_debt
`BundleProcessor.ops` is `OrderedDict[str, operations.Operation]`.  Should it be `OrderedDict[str, operations.DoOperation]`?  Or do the transform ids in `BundleProcessor.timers_info` all point to `DoOperations`?;non_debt
:heart:;non_debt
"closes https://github.com/apache/airflow/issues/12832
This format is popular and supported by tools like Ansible.
vs
are covered with tests. And in case of new features or big changes
remember to adjust the documentation.
Feel free to ping committers for the review!
closes: #ISSUE
related: #ISSUE
How to write a good git commit message:
http://chris.beams.io/posts/git-commit/
-->
---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.";non_debt
retest this please;non_debt
AWS: Fail writes fast when S3OutputStream encountered error in async â€¦;non_debt
Done and manually tested it. Works fine.;non_debt
+1;non_debt
cc @gatorsmile;non_debt
R: @dhalperi;non_debt
"* This PR creates a new metadataUpload path for segmentUpload on the controller side, server side yet to come
* Segment versioning will be supported to avoid handling the race condition in upload.";non_debt
"I thought yaml parsing ignored padding, so
was the same as 
**Will add trimming for the string.**";non_debt
Thanks for skimming the whole doc. cc @srowen.;non_debt
@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.;non_debt
better to say `-define(GB, (1024*1024*1024)).` I'm sure the compiler will do the right hing.;non_debt
"Sure, I will work on master branch.
Thanks,
Raghavendra Nandagopal";non_debt
"After debugging i realized that the reason my changes weren't working was because spark.app.name was getting overridden by the default value set in interpreter-setting. 
i did checkout your branch and got the following result.";non_debt
"@srowen 
Pardon for the ping.";non_debt
retest this please;non_debt
@yupeng9 @mayankshriv Cleaned up the PR, added the tests and a JsonIndexQuickStart on the github events data. Please take another look;non_debt
"well, I think it depends what you mean by ""handled correctly"".  We use the time the taskset completes, so its OK if the failures happened long ago when the taskset started, we still count those failures in the app blacklist, so later failures can trickle in and push us over the limit.
OTOH, this also means that if we were already close to the limit on failures for the application when this taskset started, then a really long running taskset will fail to push us over the limit -- by the time the latest task set finishes, we've expired the old failures, so we only get failures from the new taskset.  So if your taskset time is longer than the blacklist timeout, you're unlikely to ever get application level blacklisting.
Clearly this is not great, but its not _that_ bad.  After all, even if it were app-level blacklisted, we'd hit still the timeout and remove the bad resources from the blacklist, so that we'd need to rediscover it in future blacklists.  One of the main reasons for the app-level blacklist is to avoid lots of failures when the tasksets are _short_.  If you really want an application level blacklist which is useful across really long tasksets, then you've got to crank up your timeout.
We could change this slightly by _first_ updating the application level blacklist, and _then_ expiring failures past the timeout.  But to me that behavior seems much less intuitive, for a pretty questionable gain.
Does that make sense?  What do you think?";non_debt
"cache host_name to avoid repeat address resolve.
XXXXX
XXXXX";non_debt
can we pass the desp col to these unique_string calls ?;non_debt
"[incubator-brooklyn-pull-requests #885](https://builds.apache.org/job/incubator-brooklyn-pull-requests/885/) SUCCESS
This pull request looks good";non_debt
@blueorangutan test;non_debt
value to be returned;non_debt
Why move this?;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4903?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4903?src=pr&el=continue).";non_debt
retest this please;non_debt
"- Refactoring and cleanup
- Upgraded the client library from 5.0.0 to 5.2.0";non_debt
I am fine either way. It did seem like the logic was separate but we can refactor this part out once we have UPDATEs.;non_debt
Run Seed Job;non_debt
Sorry, I'm not familiar with hostdb.;non_debt
"We currently don't support specifying broadcast inputs using input descriptors. 
SAMZA-1841 will address that later. Currently these stream IDs are those that were generated using the High Level API broadcast() operator, hence the renaming to clarify that this is only for intermediate streams.
Does that answer your question?";non_debt
We need it because it makes the syntax the same between the SSH sensor and effector initialisers, and it has the normal and well understood semantics of a config key?;non_debt
What is missing here?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR/1073/
Test PASSed.";non_debt
retest this please;non_debt
[SPARK-19026]SPARK_LOCAL_DIRS(multiple directories on different disks) cannot be deleted;non_debt
change it to `spark.sql.hive.metastore.version`?;non_debt
retest this please;non_debt
[STORM-500] Add Spinner when UI is loading stats from nimbus;non_debt
LGTM;non_debt
Is it useful to have display data for this DoFn?;non_debt
ping;non_debt
When and why this exception happens?;non_debt
"Environment: xenserver-72 (x2), Advanced Networking with Mgmt server 7
Total time taken: 27499 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr2211-t1860-xenserver-72.zip
Smoke tests completed. 65 look OK, 1 have error(s)
Only failed tests results shown below:
Test | Result | Time (s) | Test File
--- | --- | --- | ---
test_01_scale_vm | `Error` | 20.30 | test_scale_vm.py
This error may be ignored as the XenServer 7.2 version based test-hosts did not have appropriate license for scaling VMs and failed with following error: (/cc @PaulAngus  - can this be fixed?)";non_debt
`LABELED_PATH`;non_debt
bugfix init reference dead lock;non_debt
Yes. I have added hasNonNullValue() in createIsNullPredicate(). It solves the issue. All other tests OK.;non_debt
thanks for catching that.;non_debt
Perhaps we should rename the old `HivePartitionComputer` to `HiveRowPartitionComputer`?;non_debt
- remove ZK status path nodes for workers after they are removed.;non_debt
After some offline conversation, the ultimate goal would be to move everything to `SingleGfshCommand`.;non_debt
Without going too deep into code, but are we sure this is will always result in localhost? What would this value be in a multi-homed environment? Does this not matter for this test?;non_debt
@kishoreg Gotcha! We can bring in a factor. The max_qps can be calculated from min_qps multipled by this factor. The ideal situation is that all replicas are up and each broker'd use the min_qps quota. If some replicas go down, the qps rate should not exceed max_qps.;non_debt
@ewencp for review;non_debt
+1 will try to review in time for 090;non_debt
"and this also have no effect on timestamp values.
tested.";non_debt
"[ActiveMQ-Artemis-PR-Build #436](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/436/) SUCCESS
This pull request looks good";non_debt
Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/295/;non_debt
[Issue 7787][pulsar-client-cpp] Throw std::exception types;non_debt
VOTE +1;non_debt
So this method should be a pure interface, you don't need to implement in `ColumnVectorBatch`.;non_debt
@uce @rmetzger I use this script build local distribution successfully. But the apache release have not try.;non_debt
"Or maybe
I think it should eval the same way, but moving the common condition `baseUri == null` out?";non_debt
"https://issues.apache.org/jira/browse/TINKERPOP-2217
Fix potentially harmful timing issue: _writeInProgress could be observed by BeginSendingMessages to indicate that the loop in SendMessagesFromQueueAsync is still ""in flight"" while in reality, it has already exited.";non_debt
"[cloudstack-pull-rats #449](https://builds.apache.org/job/cloudstack-pull-rats/449/) SUCCESS
This pull request looks good";non_debt
"@cradal There are several comments from older iterations of this, that are possibly no longer relevant. For those conversations, if you have already incorporated the suggestions, you can mark them as ""Resolved"", so that way the UI doesn't show them as prominently. That will make it easier to track current, ongoing discussions related to the latest iteration of this PR.";non_debt
LUCENE-8972: Add ICUTransformCharFilter, to support pre-tokenizer ICU text transformation;non_debt
no need of this match , directly use case inside map;non_debt
the title is not fixed yet;non_debt
I resolved the conflicts, I think: http://paste.fedoraproject.org/389174/80381301/;non_debt
I have opened this constants up when explicitly supporting JUnit 4.;non_debt
HDDS-2564. Handle InterruptedException in ContainerStateMachine;non_debt
"The output was changed for debugging in #1613 
Revert the output back to original output.
Pulsar admin CLI output should work as before.";non_debt
"Hi @JingsongLi do you have future comments on this? 
Travis is passed in my repo: https://travis-ci.org/wuchong/flink/builds/629333389";non_debt
FINERACT-972: Ensure Tomcat is started after compiling integration tests;non_debt
RETURN_NOT_OK?;non_debt
Updated the comment accordingly.;non_debt
"Yes, the condition tried to identify it is the same kind of 'is' or not...unfortunately I  wasn't taking into account 'is false'
On 3 August 2018 16:55:18 CEST, Vladimir Sitnikov <notifications@github.com> wrote:";non_debt
TS-4838: CONNECT requests get forgotten across threads.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/4868/;non_debt
Update release doc;non_debt
33884891-14492 review-584304954;non_debt
Can you use the same license header as the other XML files in Apache Camel. They need to be similar as we have a RAT check that check the code for license alignments.;non_debt
[BEAM-8343] Added nessesary methods to BeamSqlTable to enable support for predicate/project push-down;non_debt
"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4390?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4390?src=pr&el=continue).";non_debt
"Add unit test for monitor module
Close #1688 
Add test for dubbo-monitor-api
Add test for dubbo-monitor-default
CI PASS https://travis-ci.org/diamondblack/incubator-dubbo/builds/375168584";non_debt
Thanks. I will update the description.;non_debt
If updating chill means updating to kryo 5 and that means changing some imports in Spark, that's probably OK. Yes, kryo ends up in the user app namespace and that's a change that could be visible, but I don't think we expect users to rely on kryo directly. Still that might be a point up for discussion.;non_debt
yes, in the docker compose yml file, we specify the port in the container to open, which is then mapped to a random, open port externally. Docker (and the library we are using) handle the mapping between the ports from there.;non_debt
fix: Download as image not working on Dashboard view;non_debt
Note for reviewers: I've added a few test cases which are probably risks for future transient failures. Unfortunately it's difficult to test this without using system time since the coordinator depends on the purgatory implementation which doesn't work with MockTime. Any ideas to fix this would be appreciated.;non_debt
"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13004/";non_debt
Change the PR title to `[SPARK-23264][SQL] Make INTERVAL keyword optional in INTERVAL clauses`?;non_debt
"`""An exception occurred while processing a metric message.""`?";non_debt
"@joefk @saandrews @rdhabalia Please take a look ) 
Once we get this one in, the build will generate the first version in the `asf-site` branch in the same repo. At that point we can ask INFRA to enable the gitpubsub bridge to publish the site. We can still do all sorts of changes fixes before (and after) that.";non_debt
detailed documentation of hdfs uploader;non_debt
Merging in master / 2.2.;non_debt
ARROW-5396: [JS] Support files and streams with no record batches;non_debt
line len > 100 chars;non_debt
"What is this really trying to do? Maybe fix it?
C++11 style casting changes at least.";non_debt
It's both. Clarified.;non_debt
99919302-5033 review-538948912;non_debt
Maybe I explained badly. What I meant is to change the `onEnabled` method to look something like this (semi-pseudo code):;non_debt
`const char *`.;non_debt
"Authored-by: Mark Hanson <mhanson@pivotal.io>
Draft pull request to see test results.";non_debt
"**NOT** ready for merge.
Opening for discussion **only**.
- Somewhat related to https://github.com/apache/couchdb/issues/1515
Every developer might have a slightly different setup. Currently in order to do this customization developers are forced to modify `dev/run` and some other files. This PR is proposing to introduce a `hooks_dir` for `dev/run` customizations. The hooks are simple python modules which export following set of functions:
- `before_setup(ctx)`
- `after_setup(ctx)`
- `before_boot_nodes(ctx)`
- `after_boot_nodes(ctx)`
- `before_startup(ctx)`
- `after_startup(ctx)`";non_debt
Avro: Fix pruning columns when a logical-map array's value type is nested;non_debt
Merged to master branch. Thanks @nkalmar !;non_debt
ping @michal-databricks;non_debt
fixed;non_debt
Can we also check that it returns null on non-matching documents?;non_debt
Ok, I know. To be transparent to users, how about add a new thread local property `SparkContext.SPARK_RESERVED_JOB_GROUP_ID` or `SPARK_THRIFTSERVER_JOB_GROUP_ID` to separate it.;non_debt
It works for me.;non_debt
HIVE-23095 ndv 70;non_debt
[BEAM-11530] Annotated setter parameters handled wrong in schema creation;non_debt
"This uses a webcam or video capture device to run SSD in real time.
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
[AIRFLOW-865] - Configure FTP connection mode;non_debt
It's a huge topic, let's talk about this offline.;non_debt
clang-analyzer build *successful*! https://ci.trafficserver.apache.org/job/clang-analyzer-github/750/;non_debt
"This PR groups `spark.naiveBayes`, `summary(NB)`, `predict(NB)`, and `write.ml(NB)` into a single Rd.
Manually checked generated HTML doc. See attached screenshots.";non_debt
cc @cloud-fan , please take a look, thanks!;non_debt
ok to test;non_debt
Fix maxBytesInMemory for heap overhead of all sinks and hydrants check;non_debt
+1 - Since the differences come from thee RECORD and ARRAY/MAP are containers, testing against the RECORD is sufficient is my thinking as well.;non_debt
45721011-6183 review-556204702;non_debt
Could we add some simple test for this bug?;non_debt
Not necessary as such, but will be useful in debugging if some change to `getData()` method might cause this test to fail.;non_debt
Can you please rebase the PR on the master, there were some CI issues we fixed;non_debt
Hi, this looks great! Is there a reason why sort based join is not in spark core, only in spark SQL?;non_debt
LGTM from my side, but I will wait for @JoshRosen to LGTM it since he is most knowledgeable about this.;non_debt
"Choose one
https://github.com/apache/incubator-superset/pull/7507 added `cache_value` as a keyword arg, but `cache.set` doesn't take `cache_value`, it takes `value` https://github.com/sh4nks/flask-caching/blob/master/flask_caching/backends/rediscache.py#L102
CI
@graceguo-supercat @michellethomas @serenajiang @conglei";non_debt
I really don't understand these comparisons, but they are guaranteed to fail because ret is null.;non_debt
[BEAM-11377] Fix retry & cleanup issues.;non_debt
Prefer delete over remove.;non_debt
What is the difference if childRel is join?;non_debt
"would be great to add implementations of all the ClientInterface with ""override""";non_debt
as above;non_debt
Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5823/;non_debt
Nice job!;non_debt
"""be cases, in which"" --> (no comma)";non_debt
"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
hi @sruthiskumar - just pinging to make sure you haven't forgotten : );non_debt
Somehow I still see python2.7 usage in the CI.;non_debt
cc @hvanhovell;non_debt
added one more select query.;non_debt
[AMBARI-23035] HDFS Balancer via Ambari fails when FIPS mode is activated on the OS;non_debt
"this service registry is used by WildFly to pass instance of user-defined classes (such as transformers, interceptors) to Artemis.
WildFly uses a modular class loader so it's its responsibility to instantiate these classes from the correct module. The ServiceRegistry is really just an object repository since Artemis configuration does not allow to pass instances but only class names";non_debt
@mxnet-label-bot add[Test, pr-awaiting-merge];non_debt
2211243-4461 description-0;non_debt
Collection;non_debt
This PR's description?;non_debt
As the title. When using zk client, the query topicDescribe are output in order of topic names. Similarly, when using adminClient, they are also output in order of topic names.;non_debt
"We should normalize the attribute name. Let's say a table has a column `id`, and users write `UPDATE ... ID=1`, then the `AttributeReference` will be named `ID` instead of `id`.
The same thing should also apply to filters. You can take a look at `DataSourceStrategy.normalizeFilters` and see where we call it.";non_debt
@blueorangutan package;non_debt
we should probably mention here that it `CPU_CORES` has to be an integer in case of yarn and check it as well in `YarnResourceManager`.;non_debt
@pjain1 you must be running using an old version of the mysql connector code. The current code does not have any `SHOW VARIABLES` statements.;non_debt
Run Load Tests Java ParDo Dataflow Batch;non_debt
"When third party env vars *_HOME are not present, use cmake's
ExternalProject to fetch and build them.  When those vars are present,
we just use them.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4538/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
[FLINK-16160][table] Fix proctime()/rowtime() doesn't work for TableEâ€¦;non_debt
Shouldn't the ssh connection type also be added to www/views.py -> ConnectionModelView.form_choices? Would be awesome if these would somehow be configurable through plugins as well.;non_debt
ditto;non_debt
"I made some updates to the docs. It's getting late here so I'll merge this sometime tomorrow.
VOTE +1";non_debt
Have you run beamimport to test this PR internally?;non_debt
This allows you to drop the query string when doing the consistent hash in parent selection. Default behavior is the same. Add `qstring=ignore` to the parent.config line to enable.;non_debt
"I did not realize your -1, and I confused this wait with a similar change on FDs.
I'm reverting it.
@brusdev  can you address the issue with a new PR? a restart of the broker should remove any previous messages that were not deleted.";non_debt
We could skip the push at the end of the build if we build on Dockerhub directly but attaching our repo via webhook. (Doesn't work ootb with our build processes though);non_debt
@kunal642 In AlterTableCommands.scala move all the lock acquiring logic inside try block;non_debt
@potiuk @mik-laj;non_debt
I clarify the interface of AlphaRowsetReader and provide two private function _union_block and _merge_block in response to next_block.;non_debt
Otherwise LGTM +1;non_debt
fix proxy hang up while throwing a customize SQLException;non_debt
This has been fixed elsewhere.;non_debt
I'll modify unit tests based on your comments, thanks!;non_debt
[TE] enchance anomaly api to propagate feedback;non_debt
Bump jetty.version from 9.3.19.v20170502 to 9.4.24.v20191120 in /contrib/views;non_debt
Is the agent compiling test failure from one to another because of network issue? The CI passed, so I assume the compiling should be fine.;non_debt
"Not nothing. Human diligence prevents it.
Even if there is a regression test someone could remove it. But we assume good faith, so we assume that doesn't happen.";non_debt
I think it again, we do not need check size of aggregates in this method. and we can use `isTableAggFunctionCall` when need. such as `isTableAggFunctionCall(expr)`, see blow comments. :);non_debt
[LLVM] Fix build breaks with StringRef changes;non_debt
;non_debt
thanks, merging to master/2.1!;non_debt
LGTM;non_debt
"""have you ensured""";non_debt
Hi @guozhangwang, thanks for reviewing. I will check again later. And for the unit test, I will do it in a follow-up one PR.;non_debt
Do we need to define another exit status to distinguish with the class not found exception?;non_debt
"This [PR](https://github.com/apache/incubator-superset/pull/3722) added a check for `isXAxisString = ['dist_bar', 'box_plot'].includes(vizType) >= 0` but for a different viz type (like line) includes will evaluate to false and `false >= 0` will evaluate to true (so the formatter was not getting set).
Changing to use indexOf.";non_debt
Benchmark updated to include integers with `wide` and `narrow` value range.;non_debt
"Needs to be ""Point"".  See http://geojson.org/geojson-spec.html#point";non_debt
It seems there are many requests for that, but GA still does not support it: https://github.community/t/support-for-yaml-anchors/16128;non_debt
[FLINK-1725]- New Partitioner for better load balancing for skewed data;non_debt
"Because of the problems mentioned above, I did not change the implementation of `zetaSqlTypeToBeamFieldType`, instead I added another overload of it which does not depend on Beam `fieldType`. It will allow compliance test driver to call it (cl/316152825).
@apilloud Please check if this PR still LGTY.";non_debt
For stddev/variance, it's clear that using ImerativeAggregate will be faster. Then we don't have case that need the MutableProject to be atomic, should we revert this PR? cc @marmbrus @yhuai;non_debt
Revert inadvertent merge commit and re-apply HBASE-24280 ADDENDUM to branch-2;non_debt
[TE] Extend anomalies endpoint to fetch by metric/dataset and true anâ€¦;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7325/
--none--";non_debt
@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests;non_debt
"Thx @rmannibucau  for the follow up!
Yes - testing with `contains` is a simplification and neglects some weirdo cases... and yes: it would require char, position + offsets to properly test and replace the closing and opening brackets. 
Thus, I like the idea of using `PropertiesLookup#lookup` and returning the key there. The parsing ""magic"" (char, position, offsets) is already done in the substitutor, so I guess fallbacking there is a good option. 
I will take a look at it.";non_debt
This still says log4j.;non_debt
LGTM. Merged into master and branch 2.2;non_debt
â€¦ info from the wiki;non_debt
"Can we still perhaps do a warning and `url = new URL(String.format(DOWNLOAD_URL_PATTERN, ver.replace(""-incubating"", """"), LAST_KNOWN_VERSION))` here?";non_debt
"`empty partitions` -> should we say `empty topics` ?
Just to clarify that some partitions from _different_ topics are empty?";non_debt
ORC-490 Refactor SARG applier;non_debt
@DariuszAniszewski Sorry for the unclear explanation of this fix. This line basically fix #6074. The tar file will be broken if we build it outside of `../sdks/python`, which causes worker crash looping during startup and timeout in Jenkins job. You can get more details if you check the tar file from staging location.;non_debt
Indeed looks incorrect;non_debt
@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests;non_debt
There are a number of CVEs released fixed in the recent Tika 1.18 release - we should upgrade.;non_debt
It is good to go once the CI tests pass;non_debt
maybe check whether the result is ResourceProfile.ZERO?;non_debt
"This is an alternative to #5473.
I added an extra bit of code (which, unfortunately, would break the actual code if executed) to convince clang analyzer to shut up. I fiddled with this far more than I should and was unable to find any sort of assert
that would suppress the false positive. I _think_ the problem is the analyzer knows `x->_next == n` because of the previous code path, but doesn't realize `x->_next` gets updated when `n` is removed (via standard linked list removal). It's also annoying that the analyzer complains about the _assignment_ of a stale pointer as if it were a dereference. While dangerous, the assignment of `n = next(x)` is not actually a use after free. If I replace that with the content of the `next` function, however, the use after free moves to the next loop iteration, which would be correct if there was an actual use after free.
I prefer this because if the code is changed solely to make clang analyzer shut up, that should be obvious to a reader of the code, rather than puzzling over obtusely different implementation styles. It also means if we ever discuss this with the LLVM group, we can easily find the places in the code where the clang analyzer messes up.";non_debt
Simple fixup.;non_debt
yes, that would also be fine.;non_debt
"@mistercrunch 
This should fix https://github.com/airbnb/airflow/issues/142
This operator is still not as robust in its parsing as the key sensor, but prefixes are more tricky.";non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/2430/;non_debt
@billonahill Thanks Bill. The code LGTM.;non_debt
The code changes looks good to me, but my experience in code working with SSL is still small so someone with more experience should also double-check.;non_debt
This PR is covered in https://github.com/apache/incubator-beam/pull/1400.;non_debt
@eqy @merrymercy;non_debt
The auto-merge in 70917a5 somehow reverted part of this PR, causing the newly added test to fail.;non_debt
The parameter `identifier` cannot correctly describe the meaning of orchestration instance unique identification,  my suggestion is `instanceId`, and i think it's better to define it as `String` type.;non_debt
"I feel this line misses behavior ""executing command"".
I'd like to have this line changed like 
`Archives resources to jar and uploads jar to Nimbus, and executes following arguments on ""local"". Useful for non JVM languages.`";non_debt
[BYOC][Verilator] Refactor Verilator runtime;non_debt
Thank you, Pablo and apologies.  I was trying to figure out how this could be prevented in the future.  Is there anything this PR specifically that led to breaking whitespace precommit?  I was looking into the output of the pre-commit tests to see if there was anything related to the content of the PR specifically.;non_debt
"Incorrect ordering of replication packets may happen because of
useExecutor parameter in the sendReplicatePacket method.
ReplicationStartSyncMessage packets are sent as first, but they are sent
with useExecutor=true. Although ReplicationSyncFileMessage packets are
sent after ReplicationStartSyncMessage packets, they are sent with
useExecutor=false. So sending of ReplicationStartSyncMessage packets is
scheduled to executor and there is no guarantee when the task will be
executed, whereas ReplicationStartSyncMessage packets are sent
immediately.
The solution is to wait for an ack for ReplicationStartSyncMessages.";non_debt
I think the tool class should be stateless, or we make a common data structure there with pre-registered tables there only for testing.;non_debt
[bookie-ledger-recovery] Fix bookie recovery stuck even with enough ack-quorum response;non_debt
What's difference between `spark.yarn.user.classpath.first` and `spark.files.userClassPathFirst`? For me, it seems to be the same thing with two different configuration.;non_debt
@matthiasblaesing are you satisfied with the changes now?;non_debt
MINOR: fix integration tests;non_debt
thanks, I'll resolve the conversations as I resolve these in my branch and don't have any questions;non_debt
"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13795/";non_debt
Looks good @jbonofre!;non_debt
"@ptgoetz No worries. I didn't take your comments in that way. It was just my criticism of **current metrics API**.
Storm community discussed about metrics several times, and the only one that I can get is the needs of new metrics feature. 
This is a latest discussion which also contains requirements of metrics feature. I don't think we should address all of them, but addressing most of them would be great.
http://mail-archives.us.apache.org/mod_mbox/storm-dev/201605.mbox/%3CCAF5108jB=4aL0Z1qMMnEe7U4Yx_9TZjD11BJYhKtkTtVJoTdhQ@mail.gmail.com%3E
Adding fixes to current metrics seems not competitive to recent metrics from others stream frameworks, and even JStorm, which is the thing we would eventually evaluate (I already did a first pass and looks promising) and port if it's better. This patch only has a value with current metrics, and I don't want to break backward compatibility because of will-be-deprecated-and-dropped feature.";non_debt
Fixes #1420 - Made fields in AESCryptoService inner class constructorâ€¦;non_debt
"This may sound like an over-optimization but adding type specific GroupKeyGenerator has the potential to add overhead for runtime dispatch which Java doesn't handle well.
What is the advantage of having a type specific next method like the following for Double
v/s the existing generic method
Both are doing toString()";non_debt
Thanks all for the review.;non_debt
"I'm slightly leaning towards Stephan's suggestion, which I also agree is the better solution for this case.
It might be ok to have this as a ""hidden"" API for now anyways, since 1) it is marked `@PublicEvolving`, and 2) the API was added in quite a short timeframe.
If we want this fix in 1.5, I wouldn't suggest ""fully"" exposing it.";non_debt
Any thoughts on moving this one forward?;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/1148/;non_debt
@tdas please have a look.;non_debt
add the parent allocator in the message to help with debugging?;non_debt
Done;non_debt
FreeBSD11 build *failed*! https://ci.trafficserver.apache.org/job/freebsd-github/2207/;non_debt
DRILL-7680: Place UDFs before plugins in contrib;non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1602/;non_debt
will enable source maps. good point!!;non_debt
Add test for out-of-order data to make sure we set the correct timestamp;non_debt
"Hi @StephanEwen 
Thank you for the review. I have rebased and done those improvements in your suggestion.";non_debt
As it is ... it's not in the (new) first commit and I'll leave it out of this PR until jemalloc shows it cannot do it.;non_debt
It's better to be done in another pr.;non_debt
49876476-1162 description-0;non_debt
@aromanenko-dev I didn't mean to ignore your comment about jira prefixes on commit messages. I just saw it late. Apologies.  Hope I can get a pass this time!;non_debt
"We need to call out the specific items and not just include the boilerplate message by the source LICENSE.
In the case of those exclusions we must note that ""the project includes <project/source> which is available under <license>"" and the associated copyright.  As a reference, scope out the other items in this LICENSE to see how they are handled. 
We should not have the caveats listed by the source but interpret them and include the appropriate clauses for those items which we do include.";non_debt
SecurityGroupRulesCmd code cleanup;non_debt
An alternative way, that I think it is equivalent, with less branches and return statements in the same function. Feel free to keep your version if you want :);non_debt
retest this please;non_debt
Nice catch, I'm not sure what @kishorvpatil 's original reason for blocking it from search like that was but this is good;non_debt
/pulsarbot run-failure-checks;non_debt
78186814-1446 description-0;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1226/
Test PASSed (JDK 8 and Scala 2.11).";non_debt
cc @felixcheung;non_debt
"Replaces some incorrect usage of `new Configuration()` as it will load default configs defined in Hadoop
Unexpected config could be accessed instead of the expected config, see SPARK-28203 for example
No.
Existed tests.";non_debt
SDV Build Success with Spark 2.1, Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/106/;non_debt
"Looking at the discussion and the different issues I see what you mean @1ambda. We are actually having two issues:
1. Local changes are overwritten when the notebook gets updates from the server (e.g. someone renames a note)
2. Local changes are not propagated on a regular basis (and you risk bigger conflicts when editing simultaneously)
The first point is addressed by #2176 #2177 more than this PR, because even when saving on losing focus you might risk losing local changes if you didn't lose the focus and someone renames, correct?
The second point is addressed in a way here but as mentioned in one of the other discussions we should really move towards real time updates where every keystroke gets transmitted (or at least while typing maybe after a few seconds) and you can see the cursor of the other users in your paragraph (similar to Google Docs).
Does it make sense?";non_debt
yes, this seems fine;non_debt
LGTM;non_debt
retest this please;non_debt
This was merged, closing;non_debt
50904245-11610 review-425548953;non_debt
Done.;non_debt
Hi @lewismc Thanks for your effort to test my patch and I am very glad that everything worked perfectly and the PR is more likely to be merged :D I've added parameterized logging where possible and pushed the changes. However, [here](https://github.com/apache/nutch/pull/184/files#diff-ed9e59402a5d1e6a0f39361b2429e580R97) and [here](https://github.com/apache/nutch/pull/184/files#diff-ed9e59402a5d1e6a0f39361b2429e580R91), the log-messages need to be created to throw `RuntimeException` no matter log is enabled or not. So I used `String.format` here.;non_debt
"I like the proposed name by @chamikaramj but I'd prefer @iemejia idea to keep a consistency with naming. 
So, my **+1** to `withOutputParallelization()` and `withOutputParallelization(boolean)` where default value is `true`.";non_debt
"Some documentation questions:
Is this meant to completely replace the artifacts supplied in replacements?
What if a user doesn't pass in all the original artifacts?
If something can't be ""resolved"" to something simpler, does it still appear in the output?";non_debt
"Update Arrow version to 0.6.0
Here is a [release note](http://arrow.apache.org/release/0.6.0.html).
Existing tests";non_debt
" 21526 passing (30s)
  48 pending";non_debt
I'm merging this into `master` and `branch-1.2`.  Thanks!;non_debt
"Thank you for submitting a contribution to Apache NiFi.
Adds a 'replace first' and 'replace all' strategy to the ReplaceText processor.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
@criccomini;non_debt
Made correction to fix TINKERPOP3-855.;non_debt
@mxnet-label-bot add [MKLDNN, pr-awaiting-review];non_debt
"suggest changing ""a L2 norm"" to ""the L2 norm""";non_debt
"The default dataset delay logic will apply when we automatically onboard all the alerts for SLA detection. The yaml translation logic needs to take care of it. 
Here, we expect that the sla is defined by the user and we will strictly adhere to it.";non_debt
@masahi It looks like certain recent change causes this error. I'm investigating.;non_debt
+1;non_debt
Before this change, `Expected Array(Array(1)), but got Array() Wrong query result`;non_debt
"Motivation: to have possibility to run this code on ververica platform when version 1.12 will be available.
I can add dependency in my job. 
But this will also probably need some shading as I will have two different version of confluent libraries.";non_debt
"@membphis 
32 seconds";non_debt
there is an isNotBlank before calling that method;non_debt
"I try to run the same unit test command on local
without any failed test case.
But online test failed after retry 3 times and each time failed with different test cases:";non_debt
I think we can just use topi.identity op here. No need to write the shape func?;non_debt
TVMGetLastError may already contain a backtrace, so this would double it. (This was a preexisting issue).;non_debt
CLOUDSTACK-7758: Fixed although api calls are failing, event tab shows them as successful;non_debt
[NIFI-3943] align combo option item toolips to hover closer to the icon;non_debt
Fix pushed.;non_debt
Each of the commits is independent for each operator, so I prefer keeping them atomic;non_debt
"it seems that there is an issue for python, I'll fix the signature soon!
I'd appreciate your feedback on the signature and the logic @davies @shivaram @rxin @sun-rui 
Thanks,
Narine";non_debt
@ralphge Thanks for the contribution. I've tested and worked well. And CI failure is a temporary issue. Could you please re-trigger CI? It's OK to close and re-open this PR.;non_debt
LGTM please self-merge to the release branch.;non_debt
+1 again;non_debt
Reading the code, I wondered if it might be pervasive in most/all runners. Is it a release blocker though? This bug has existed presumably for many releases.;non_debt
Run Python2_PVR_Flink PreCommit;non_debt
The build failure is puzzling me. The line that it's complaining about seems correct and it builds locally fine. Am I missing something?;non_debt
The `r` and `nightly` groups are defined with wildcards that will match the name I used for this test.;non_debt
`__all__ = ['RowCoder']`;non_debt
"Ah, I think I know the issue. Python ORT is setting the version to 2.0, which is likely causing an exception since 2.0 is ""not supported"".";non_debt
GeodeAwaitalities' default wait time is 5 min.;non_debt
Ah yes, good catch.;non_debt
Retest this please;non_debt
Done;non_debt
"it is backcompatible, the code reads """"helix.callbackhandler.isAsyncBatchModeEnabled"""" first, if it is not set, it reads ""isAsyncBatchModeEnabled"" instead (see the lines 199-202 following)";non_debt
"We would need this ""totalCount"" as part of DatanodeMetadata response to support server side pagination in the future.";non_debt
"Fix using `num_workers` in omp
@hlu1 please kindly review";non_debt
needs a file-level license;non_debt
"[Reef-pull-request-ubuntu #143](https://builds.apache.org/job/Reef-pull-request-ubuntu/143/) SUCCESS
This pull request looks good";non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2258/;non_debt
Updated the title;non_debt
@dalaro when i build this branch i get some errors (they pop up intermittently in integration tests and not in all environments) that i long ago thought i took care of. is it possible that the branch in your fork needs to be rebased on the latest stuff from tp32?;non_debt
core parts LGTM;non_debt
[AIRFLOW-5840] Add operator extra link to external task sensor;non_debt
62117812-8761 comment-737106073;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6405/
--none--";non_debt
"dtype.h -> runtime/data_type.h
Changes:
- Rename all old reference of tvm::Type to DataType
- ExprNode.type -> ExprNode.dtype
- Expr.type() -> Expr.dtype()
- Change Expr related functions to expr_operator.
  - DataType::min() -> min_value(DataType)
  - DataType::max() -> max_value(DataType)
- Move type constructor Int, UInt, Float, Handle, Bool into DataType.
  - Int(bits) -> DataType::Int(bits)
  - UInt(bits) -> DataType::UInt(bits)";non_debt
"@milleruntime I pushed my commit to remove SimulationMode from the tests. Does everything else look good? 
I also have the clientPropsPath test ready to go. Should I create a new pull request for that one or just push it onto this branch?";non_debt
"Hi @piiswrong  I have modify the code and delete the input_data_shape according the @zhreshold 's advice.
Pls review it again!";non_debt
[SCB 456]Remove warning;non_debt
Run Java PreCommit;non_debt
controllers, brokers, and servers now support multi-ingress for different protocol. this opens up a three-phase migration path by first enabling TLS-secured servers, then upgrading client connections to TLS, and finally shutting off unsecured server ingress.;non_debt
yep, `ClassLoaderUtilsTest` depends on this and it passes.  pretty cool.;non_debt
Let me try and take a look tomorrow - I'm in transit today.;non_debt
fix #2767;non_debt
"This removes default implementation of getRowCount for BeamTables. It will also change its name to getTableStatistics because it can represent rate as well.
**Please** add a meaningful description for your change here
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/)
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/)
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.";non_debt
"[Reef-pull-request-windows #156](https://builds.apache.org/job/Reef-pull-request-windows/156/) FAILURE
Looks like there's a problem with this pull request";non_debt
ARROW-6927: [C++] Add gRPC version check;non_debt
WIP: SOLR-12361;non_debt
10805187-514 description-0;non_debt
staging was broken because of this, merging;non_debt
Remove this block?;non_debt
Merged build started.;non_debt
[CARBONDATA-2369] updated the document about AVRO to carbon schema converter;non_debt
Merged to master, branch-3.1, branch-3.0 and branch-2.4.;non_debt
"Jenkins keeps failing, but failures don't seem to be related in any way with the changes contained here.
Given the discussion at swagger-api/swagger-core#1558 I am going to merge this PR right now.
We can always remove it should they decide to do something about it.";non_debt
99919302-2431 review-357906474;non_debt
"@uddhavarote That's not how it works. The stream you emit to doesn't matter. As long as the tuple you emit from the KafkaBolt is anchored to the input tuple, the tuple will be replayed from the spout and go to the entire topology again if processing fails after the KafkaBolt.
Basically the choice you will have is to use`OutputCollector.emit(input, new Values(...))`, in which case the tuple gets replayed from the spout if processing fails in your bolt, or `(OutputCollector.emit(new Values(...))`, in which case the tuple does not get replayed and is lost if processing fails in your bolt.
That said, your application probably needs to deal with potential duplicate writes to Kafka anyway, so it might not be a big deal. I'm fine with adding the output collector and tuple to the callback. Please raise an issue at https://issues.apache.org/jira and feel free to submit a PR here that makes the change.";non_debt
I don't think it's the right place to do it, but right now it fixes the build for the release, we should do it in the right way aftwards.;non_debt
How about throwing `NoSuchDatabaseException`? We also have the other NoSuchXYZ exceptions. Then, when writing the test cases, we can check these specific exception types instead of the general `AnalysisException`;non_debt
"**Please** add a meaningful description for your change here
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.";non_debt
can you include the name of the table / relation ? the computation can refer to multiple tables which might have same `partitionColumn`;non_debt
"During the porting of rewrite_js.js tests into the elixir test suite I've found an erroneous behavior in the JS-rewrite functionality. 
When a rewritten request contains a BODY (POST/PUT) and the rewrite rule rewrites the query string, this query string is ignored in the final request. The following case illustrates this issue.
1 - Create a design document with a rule that rewrites the query string, like this one:
any request should be rewritten to `_changes?filter=_docs_ids`
2 - Create a couple of docs
3 - Perform a POST request to be rewritten
The expected result for this case is to have only the change for the doc with id 2 in the response, but the full change list is returned. The `filter=_doc_ids` is ignored after the rewritten.
This PR changes this behavior and the query string is preserved, so the result for the previous case is the excepted one returning only the change for the doc with id 2.
     Does it provides any behaviour that the end users
     could notice? -->
The previous test case has been added to rewrite_js.js tests
This issue #1612 and the companion PR #1620 are related to this issue
     repositories please put links to those issues or pull requests here.  -->";non_debt
it looks like as if this `appveyor.yml` is not enough to trigger appveyor test? it doesn't seem to be kicking off a run;non_debt
[approve ci autest];non_debt
Done.;non_debt
So if this is related to virtual topics why complicate things for the Core JMS client and introduce such problems it has JMS 2.0 spec as such shared durable subscribers, which is the spec equiv of such feature.;non_debt
Run CommunityMetrics PreCommit;non_debt
Closing this as incremental fetch capability is needed. Will produce a new PR against NIFI-2126 with both processors inside;non_debt
I think we should first discuss whether Quarkus JAXP extension would not be a better place for this. Apparently, Quarkus JAXP extension does not exist yet. It looks like they put the JAXP stuff into their JAXB extension. I think we should propose splitting their JAXB into JAXB and JAXP and then put the `SAXMessages` registration in the new Quarkus JAXP extension. WDYT, @jamesnetherton ?;non_debt
[CARBONDATA-655][CARBONDATA-604]Make no kettle dataload flow as default;non_debt
"In order to support nested predicate subquery, this PR introduce an internal join type ExistenceJoin, which will emit all the rows from left, plus an additional column, which presents there are any rows matched from right or not (it's not null-aware right now). This additional column could be used to replace the subquery in Filter.
In theory, all the predicate subquery could use this join type, but it's slower than LeftSemi and LeftAnti, so it's only used for nested subquery (subquery inside OR).
For example, the following SQL:
This PR also fix a bug in predicate subquery push down through join (they should not).
Nested null-aware subquery is still not supported. For example,   `a > 3 OR b NOT IN (select bb from t)`
After this, we could run TPCDS query Q10, Q35, Q45
Added unit tests.";non_debt
Merged build finished. All automated tests passed.;non_debt
CSS class/IDs are generally case insensitive. Can we use `dash-case` instead?;non_debt
Build Success with Spark 2.3.4, Please check CI http://121.244.95.60:12444/job/ApacheCarbonPRBuilder2.3/5261/;non_debt
New Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/2896/;non_debt
Thanks!  I've merged this into master and 1.1;non_debt
Added;non_debt
+1;non_debt
Build Success with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/2538/;non_debt
"- Since, menu.js file was deleted, the preview was not working. 
  I made the required changes and checked it. Now my preview is working.";non_debt
@sanjaypujare please change these properties to use apex. prefix. For those that existed prior to 3.6 release, they should be deprecated like it was done for attributes, for others they should be just changed.;non_debt
Retest this please;non_debt
NIFI-7248: Atlas reporting task handles PutHive3Streaming;non_debt
Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12602/job/ApacheCarbon_PR_Builder_2.4.5/3347/;non_debt
"We can get rid of this. The reason I kept readme was to keep a short description in-tree as I think this may provide some value to the future reader.
Do you think saving ~72K is worth removing this potential value? (honest question)";non_debt
After this patch, the query job description is shown as below:;non_debt
ditto;non_debt
@achristianson I see conflicts. they may be easy to resolve, so i can take a look if need be.;non_debt
"I think it only escapes illegal characters, not reserved ones, so `q1=my&value&q2=my-value` won't be changed - it assumes you know what you are doing. Let's see...
This passes:";non_debt
Thanks, @C0urante. I've added your suggested tests plus a few more.;non_debt
`null` is a `value` and Hive exposes `void` as a type.;non_debt
thanks all, I would update the PR soon.;non_debt
It's better to add lock here to avoid W/R conflict;non_debt
"Re-executing against the patched versions to perform further tests. 
The console is at https://builds.apache.org/job/hadoop-multibranch/job/PR-2051/7/console in case of problems.";non_debt
`ProducerBatcn.finalState` can also be updated to `FinalState.ABORTED` through  `Sender.run() --> RecordAccumulator.abortIncompleteBatches() or abortUndrainedBatches() -->  ... -> ProducerBatch.abort()` .;non_debt
We should add lock here to avoid mulitple thread entering when `isReleased` is false.;non_debt
One thing from the ClientRequest that we don't get from the builder is the correlationId. This is occasionally useful when debugging. If you think it's useful, we might consider adding it to the log lines in `doSend` as well.;non_debt
@gatorsmile , I believe I addressed all your comments and I'm seeing that tests are passing now. Thanks for pointers to -1 and <=0 changes. Tests were failing before because I missed these changes. Let me know if anything else needs more work or if this is now good to merge.;non_debt
@dpgaspar can you give this a look?;non_debt
ok to test;non_debt
If the aggregate function is computed for the original column, it should be not key whether  the original column is a key column or a value column;non_debt
Sure, I can do that.;non_debt
@HeartSaVioR @xuanyuanking Thanks for reviewing.;non_debt
Ah seems that way from the cmake.;non_debt
@drcrallen I don't think actual object support is there for java.util granularity;non_debt
Actually one more thing to do is to change the vignettes to use 1 core as well since they get rebuilt / checked during the CRAN check.;non_debt
It is disappointing that the Calcite's MV is trying to match physical plans. Further more, Choosing MV or not based on cost is totally meaningless.;non_debt
There seem to be some bugs in test_operator_gpu.test_poisson_generator which lead to failure of CI.;non_debt
@chiwanpark: I think you can register multiple email addresses with GitHub, so that they can associate all your commits (from different addresses) to you.;non_debt
Also @pwendell;non_debt
double check all import sequence. I believe this will break CI. We have import rule configured, 'org.apache.dubbo' should go first.;non_debt
Update before_doc_update/2 to before_doc_update/3;non_debt
"Following the [announcement](0) to EOL branch-1.3, update the precommit
script to not consider this branch any longer, and refresh mentions of
this branch in the doc.
[0]: https://lists.apache.org/thread.html/r9552e9085aaac2a43f8b26b866d34825a84a9be7f19118ac560d14de%40%3Cuser.hbase.apache.org%3E";non_debt
"From above PR test description, the comma separator is inside the quote, this csv case is ok.
When the string value is **fo""o \t b""ar**, then it goes to tsv case. 
The ""\t"" in **fo""o \t b""ar** can be correctly read by Excel. But if the string value is **fo""o	b""ar**, in which contains ""\t"" typed through **keyboard**, this string value will be misunderstanded by Excel. So the tsv case still need to be deal with.
I have pushed a new commit, which has delete the redundant excape.";non_debt
+1;non_debt
[SPARK-11933][SQL] Rename mapGroup -> mapGroups and flatMapGroup -> flatMapGroups.;non_debt
Good catch! Have added test cases.;non_debt
Doc: centering images is not in view;non_debt
Avoid printing object hashes in log messages related to containers;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6241/
Test PASSed (JDK 7 and Scala 2.11).";non_debt
"Closes #12757 
When a task does not have a dag at the time it's being set as a relative of another task, the Graph View becomes empty.
For example:
Note op2 does not have a dag when `op1 >> op2` is called. It's added to op1's DAG after this line, but not added to any `TaskGroup`. This causes Graph View to break.
This PR fixes the issue by adding `op2` to the root `TaskGroup` if op1's DAG when `op1 >> op2` is called.";non_debt
jenkins, add user;non_debt
NIFI-1440 Allow Remote Process Group Target URI to change after creation;non_debt
"The RpcGateway.getAddress method allows to retrieve the fully qualified address of the
associated RpcEndpoint.";non_debt
Why are we using `assert` instead of `JUnit` assertions?;non_debt
@rmetzger the test failures were because we had `auto.commit.enable=false` in the standard `KafkaTestEnvironmentImpl` standard props and it wasn't respecting that previously, updated those defaults;non_debt
"@arina-ielchiieva 
Some time spent debugging the test showed that the last schema contains all fields. The field is added in ProjectRecordBatch#setupNewSchemaFromInput. 
In the original version of the test, field A was not added due to plan optimization - condition `1=0` was replaced by` limit 0`
I can still provide a solution with combining schema if required.";non_debt
How many times this would be called? I am just wondering why we have this null check.;non_debt
done. using 5 in both places now.;non_debt
"should we have a test against DataFrame with binary column?
or, this `test_that(""dapplyCollect() on dataframe with list columns""` should say `bytes column` or `binary column`?";non_debt
"Before this PR, 
After this PR,";non_debt
Can I merge https://github.com/apache/incubator-superset/pull/6808 first?;non_debt
"# Description
SOLR-13608 introduced support into Solr for an ""incremental"" backup file structure, which allows storing multiple backup points for the same collection at a given location.  With the ability to store multiple backups at the same place, users will need to be able to list and cleanup these backups.
# Solution
This PR introduces two new APIs: one for listing the backups at a given location (along with associated metadata), and one to delete or cleanup these backups.  The APIs are offered in both v1 and v2 flavors.
# Tests
Manual testing, along with new automated tests in `PurgeGraphTest` (reference checking for detecting index files to delete), `V2CollectionBackupsAPIMappingTest` (v1<->v2 mapping), and `AbstractIncrementalBackupTest` (integration test for list, delete functionality).
# Checklist";non_debt
"Err..I don't think you get the discussed points above. Let me clarify it...
@HyukjinKwon suggested to use ""TRANSFORM"" for the purpose of piping through external process, instead of adding ""pipe"" to Dataset API. The idea is basically to add DSL. But the problem is, ""TRANSFORM"" is not an expression and cannot be used in a DSL approach. So in the end in order to use ""TRANSFORM"" for piping through external process for streaming Dataset, you will need a top-level API too...But the point of DSL is to avoid a top-level API. So...";non_debt
42763345-303 description-0;non_debt
"@Savalek can you update the title of this PR?
something like ""change Description ""auto-restart interpreter on cron execution""""";non_debt
"Does this revert the behavior that ACCUMULO-4574 was trying to address - if the table is online, return and do not wait for additional actions? The original behavior would block if there was a fate operation on the table and online was called - the original fix was to not block and return because the table was online and no additional actions were required to be in online.
If the table is online - why do we need to wait?";non_debt
@DaanHoogland The problem is that for Redfish testing (and IPMI) you need a physical machine do test this on.;non_debt
retest this please;non_debt
Currently, there are only two reporters MetricsGraphiteReporter and InMemoryMetricsReporter. InMemoryMetricsReporter is used for testing. So actually we only have one metrics reporter.  propose to provide a CSV metrics reporter.;non_debt
"Ur, @srowen  It's replaced with a `private` version as I mentioned before.
For example, if you see HIVE-14259, it was like the following.
The new version on the lastest master branch looks like the following.";non_debt
I think capability details map should ok in the response.;non_debt
to be safe, can we compare the URI? we can convert path string to URI with `CatalogUtils.stringToURI`.;non_debt
Changed;non_debt
Other than that if it doesn't change existing behavior and lets symlinks stand a chance of working, seems good.;non_debt
[explore] include ControlHeader as part of Control interface;non_debt
I have fixed 3 out of 4. I will open a ticket for the last one.;non_debt
"I like the idea of supporting that as a first-class feature, but I think it wouldn't need to be in the MVP. I think we could get by with just naming a Topology ""http-live-default"", etc, and simply choosing that based on the requested DS type if it wasn't clear which Topology to choose. Then if that ends up being a royal pain, maybe we add first-class support for it?";non_debt
@tillrohrmann , I rebase the PR. Thanks.;non_debt
It's the opposite. If try_claim() returns true the current reader owns that set of records and must return that. If try_claim() returns fails the current reader should return right away without returning any records that it failed to claim. The reader should not return any records that it didn't claim. See https://beam.apache.org/documentation/io/developing-io-python/;non_debt
â€¦on Windows Correct linking with IMPORTED_IMPLIB of 3rd party shared libs on WIndows.;non_debt
20587599-10999 review-376764141;non_debt
"This change is to support user provided nullable Avro schema for data with non-nullable catalyst schema in Avro writing. 
Without this change, when users try to use a nullable Avro schema to write data with a non-nullable catalyst schema, it will throw an `IncompatibleSchemaException` with a message like `Cannot convert Catalyst type StringType to Avro type [""null"",""string""]`. With this change it will assume that the data is non-nullable, log a warning message for the nullability difference and serialize the data to Avro format with the nullable Avro schema provided.
This change is needed because sometimes our users do not have full control over the nullability of the Avro schemas they use, and this change provides them with the flexibility.
Yes. Users are allowed to use nullable Avro schemas for data with non-nullable catalyst schemas in Avro writing after the change.
Added unit tests.";non_debt
We'll need to check but current systemvmtemplate only has python 2.7.;non_debt
"Helix allow messages to be sent to a partition with specific state. e.g. Sending message to PARTITION_8 whose state is SLAVE.
However, sometimes user wish the message is sent to only ONE SLAVE rather than ALL SLAVEs, e.g. A backup message should be sent to only ONE SLAVE.
This diff adds a new field in Criteria which allows user to specify maxReceipts, by default maxReceipts is null means no uplimit. In previous case, user just set maxReceipts as 1 and message would be sent to only 1 SLAVE.";non_debt
"see #2956
XXXXX
XXXXX";non_debt
Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/324/;non_debt
This code is dumping location of all the external tables at the time of incremental dump, which may not be the exact set of tables that would be available at the time of lastReplid when specified by the user. Can we avoid the tables which may not be present at lastReplid?;non_debt
Won't this kill JVM on _any_ exception regardless of whether it was handled below or not?;non_debt
Convert tabs to spaces;non_debt
Alright - yeah lets leave `explode` as is for now. LGTM. Merging this to master, branch-2.0;non_debt
This is an out parameter. So the order will be kept when k-v pairs are added to the LinkedHashMap in this function. Forcing the caller to pass in a LinkedHashMap will  guarantee the order in which pairs were inserted into the map when user uses the map later.;non_debt
206424-500 description-0;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/531/
Test FAILed (JDK 7 and Scala 2.10).";non_debt
Fix NuageVsp errors for build-master-slowbuild;non_debt
@blueorangutan test;non_debt
BTW, test added: https://github.com/apache/spark/pull/14482/files#diff-b7094baa12601424a5d19cb930e3402fR144;non_debt
"Yes we still keep the existing behavior for the above scenarios which this PR doesn't touch. Technically we could switch them to all use cascade uncache but that is a broader behavior change and should be discussed separately.
Sure I can add some comments.";non_debt
[HELIX-584] SimpleDateFormat should not be used as singleton due to its race conditions;non_debt
fair.;non_debt
I hadn't planned on changing anything but the mode of the read, but I'll commit your suggestion.;non_debt
All fixed @kaxil;non_debt
Fixed!;non_debt
SGTM. I think it's a better idea to forget about GitBox. Once a committer has done the setup, he can pull/push to the github repo, which is the single south of truth.;non_debt
Changed;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3710/
Test PASSed (JDK 7 and Scala 2.10).";non_debt
"@vrozov 
Made the requested modifications.";non_debt
"LGTM
please rebase";non_debt
80904111-133 description-0;non_debt
s/map/set/;non_debt
fix memory issue;non_debt
Update ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java;non_debt
ok to test;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/carbondata-pr-spark-1.6/99/";non_debt
Fixed right/left margin issue;non_debt
"- Why submit this pull request?
- Related issues
___
- Bug description.
- How to fix?
___
- Describe the details and related test reports.
update ui submodule";non_debt
Seems that way. I opened https://issues.apache.org/jira/browse/ARROW-3892;non_debt
"Ur, I meant only current ORC readers. I didn't check the detail of old Hive ORC reader.
- org.apache.orc.mapred.OrcInputFormat.createRecordReader => MR reader
- org.apache.orc.OrcFile.createReader() => RowBatch reader";non_debt
"If JAVA_HOME is not set, setJavaHome()(cloud-usage.rc:49)  method is trying to find it from default java available using something like `JAVA_HOME=$(dirname $(dirname $(readlink -e $(which java))))` and exits if couldn't find anything.
wouldn't that work in this case?";non_debt
It's weird to have this as part of the cast options.;non_debt
ok;non_debt
Don't log here, just throw IOException that wraps the ExecutionException.;non_debt
"@GabrielBrascher I didn't make changes in line 314 because it has different behavior compared to line 165.
I have mentioned the same in issue #4221 also. list hosts api returns absolute value where as findhostsformigration returns percentage. If we decided what to return then I can make changes in these two places also";non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1738/;non_debt
Need to enable all the tests again;non_debt
retest this please;non_debt
You mean exception handling? For the producer all exception handling is done within `StreamsProducer` (note that `threadProducer` above is a `StreamsProducer`, not a `KafkaProducer`);non_debt
+1;non_debt
test, do not merge;non_debt
@Pearl1594 a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests;non_debt
"Dear Gobblin maintainers,
    - https://issues.apache.org/jira/browse/GOBBLIN-190
- New unit test is added which is Kafka09TopicProvisionTest which can be run using 
./gradlew -PgobblinFlavor=custom :gobblin-modules:gobblin-kafka-09:test
- Live Unit test is also possible which can be run like this 
    ./gradlew -PgobblinFlavor=custom :gobblin-modules:gobblin-kafka-09:test -Dlive.cluster.count=3 -Dlive.zookeeper=<ZKHOST>:2181 -Dlive.broker=<KAFKA_BROKER_HOST>:9092 -Dlive.newtopic=liveTopic-Test -Dlive.newtopic.partitionCount=7 -Dlive.newtopic.replicationCount=2
For the tests to run the kafka-09 flavor needs to be enabled.";non_debt
"Since we've added Kafka Streams optimizations in `2.1` we need to move the optimization for source `KTable` nodes (use source topic as changelog) to the optimization framework.
Updated streams tests.";non_debt
62117818-1127 description-0;non_debt
"let's use EMAIL_REPORTS_WEBDRIVER or just converge those 2
easy solution would be:
or";non_debt
Hm, got it. I'm running `HistoryServerSuite` now.;non_debt
Again, could you please explain why there are only 3 cases regarding the number of axes?;non_debt
@ijuma Yes, just updating the log for now and leaving the JIRA open.;non_debt
It's better to let the NodeManager get down rather than take a port retry when `spark.shuffle.service.port` has been conflicted during starting the Spark Yarn Shuffle Server, because the retry mechanism will make the inconsistency of shuffle port and also make client fail to find the port.;non_debt
"R: @kennknowles 
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/)
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python2/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python2_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python35_VR_Flink/lastCompletedBuild/) | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/)
XLang | --- | --- | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | --- | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website
--- | --- | --- | --- | ---
Non-portable | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) 
Portable | --- | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.";non_debt
The [int from the future.builtins](http://python-future.org/compatible_idioms.html#long-integers) is a subclass of python 2's long.;non_debt
"Upgrade fastjson version to 1.2.60 (https://github.com/alibaba/fastjson/releases/tag/1.2.60)  for security issue 
pom.xml
ci pass";non_debt
Lets also revert this change as AbstractCommandConfig will be in hoodie-utilities;non_debt
Looks good tests pass, I'll merge this in. +1;non_debt
Done.;non_debt
Add mail server configuration description information;non_debt
This PR is try to implement range interval partition and now work on process.;non_debt
@cloud-fan What do you think of the PR?;non_debt
@ema please run `tools/clang-format.sh` on your branch. Also TS API changes should be discussed on the dev mailing list IIRC.;non_debt
Rebased.;non_debt
@travishegner looks like it is best to just do it in the oracle dialect.;non_debt
@milleruntime The only standby behavior we want to retain is that of the log receiver. The active monitor registers itself as the log receiver in ZK, but the standbys shouldn't. Everything else should be the same.;non_debt
When expunge a Running vm, vm will be stopped with forcestop=false which does not make sense. we should honor vm.destroy.forcestop in global setting, or always set forcestop=true.;non_debt
@tweise please merge and also cherry pick to release-3.2 and release-3.3;non_debt
Run JavaPortabilityApi PreCommit;non_debt
space;non_debt
@iemejia The CI is green now. What do you think of the Deque encoder?;non_debt
"@venkatasairamlanka  Please help review/merge this to trunk and branch-2.6.
Ref: https://reviews.apache.org/r/64894/";non_debt
Actually, the default value of `maxCommittedCheckpointId` can be null instead of `INITIAL_CHECKPOINT_ID`.;non_debt
@vmamidi Yeah, please merge master to your branch, and git push this again (with --force) to your PR branch.;non_debt
done;non_debt
Why is this `NOLINT` here?;non_debt
What is the difference between this variable and the above?;non_debt
"PR updated. Several findings: 
1. LocalLAPACK and LocalARPACK shares similar upper bound, ""requested array exceeds vm limit""   when n = 17000. For 15000, it will take more than 5 hours but doable.
2. k is actually ignored in LocalLAPACK mode. It always computes full svd.
3. computeGramianMatrix also has upper limit somewhere < 17000. Actually that's why 1 fails at 17000. I'll look into it. Yet I need more time to locate the root cause for that.
4. Under DistARPACK mode, for 17K \* 17K full svd, I got a lot of future times out and job failed. I'm trying k = 10 with 17K \* 17K (1 hour now), and seems all worker CPUs are always idle.
My intention is to expand the range of matrix computing for Spark... 
I'll probably try to optimize the DistARPACK mode.";non_debt
Run Python PreCommit;non_debt
My editor doesn't seem to agree with git about spaces.  should be fixed now.;non_debt
Can you please file a JIRA, and change the title of this PR per the policy?;non_debt
How can GemFireCacheImpl.getInstance depend on InternalDIstributedSystem.getAnyInstance if when you connect a DistributedSystem it does not call addSystem(newSystem)? If ALLOW_MULTIPLE_SYSTEMS is true then you just call InternalDistributedSystem.newInstance(config). Doesn't this prevent it from getting registered in existingSystems that getAnyInstance uses?;non_debt
"@HyukjinKwon Thanks!
Actually I had filed, but forgot to tag the JIRA ID and the category.";non_debt
I donâ€™t understand why we canâ€™t run the autest in a for loop with time on the CI to figure out what performance issues exist. Without data, any optimizations are purely conjecture.;non_debt
I will refine.;non_debt
[dashboards] New, tittle and slug OR filter;non_debt
`E.col('a')` -> `t.a`;non_debt
retest this please;non_debt
Ref #8966 .;non_debt
Run Python PreCommit;non_debt
Run Python Flink ValidatesRunner;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/3637/
Test PASSed.";non_debt
@cloud-fan I think the [comment](https://github.com/apache/spark/pull/14712#discussion_r75540560) by @gatorsmile can answer your question.;non_debt
Again, this could be expanded to allow direct population of the public key, and many public keys are not provided in `.pub` files -- perhaps the default extension filter could be `*.pub` but allow for a regex like `*.pub|*.pem`, etc.;non_debt
Ignite 2.7.1 p2;non_debt
Create Readme_mohit;non_debt
sounds good to me.;non_debt
not sure its anything to be fixed here, but I had been hoping to keep logical types as obvious pre- or post- processing steps (and where possible, dispatch to handlers via simple map lookups). I would need to think more about how to make that work in this case, but something to consider, even if just for a future refactoring.;non_debt
"I think you can supply your own ordering and partitioner separately? the ordering is defined implicitly, which is sort of awkward to override. But then you should be able to partition differently from that ordering.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala#L50";non_debt
Appropriately concerned. I started writing a test before I realized I couldn't get a non-String value in there :);non_debt
51905353-4895 description-0;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/1454/;non_debt
for reviewers: this change is based on #2243 . commit b1865aa is the change to review.;non_debt
@ijuma, Actually, I now remember why I did it. `MessageFormatter` supports Java arrays while `String.format` does not.;non_debt
Oh check out `assets/javascripts/components/Button.jsx` it's basically a bootstrap button with a tooltip prop.;non_debt
@MikeThomsen - since you've been extensively working on the Mongo processors, I'd appreciate if you could give your feedback on this one. Thanks much!;non_debt
"@comaniac ,
Looks good to me (looked at winograd part).";non_debt
Merged to master/2.1;non_debt
In such a case this would NPE I think;non_debt
R: @lukecwik;non_debt
Ack;non_debt
I think this issue has been already resolved in Openresty;non_debt
"It seems has improved:
https://github.com/apache/spark/blob/v2.4.0/mllib/src/main/scala/org/apache/spark/ml/feature/Bucketizer.scala#L91-L104";non_debt
Are there any checks to avoid a case where both of these attributes are set? Can something screw up if that does happen?;non_debt
Joining the publisher thread before disconnecting and the test timeout configuration were the real fixes.;non_debt
@abbccdda On second thought, we should just target trunk here and pick into 2.7 like usual.;non_debt
"@tmoreau89 I think we are waiting for more commits and responses from @anilmartha. I've just resolved the comments that should not be the blockers. The blockers now can be summarized as follows:
- @anilmartha is working on serialization of xgraph and will add it in this PR itself (https://github.com/apache/incubator-tvm/pull/6343#discussion_r483508132).
- Organize Xilinx artifacts in a better way (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479578125, https://github.com/apache/incubator-tvm/pull/6343#discussion_r484641148).
- Question about 2 subgaph support (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479581117).
- Rename in test (https://github.com/apache/incubator-tvm/pull/6343#discussion_r479239611).";non_debt
27911088-367 description-0;non_debt
"[FLINK-3782] ByteArrayOutputStream and ObjectOutputStream should close
ByteArrayOutputStream close method does nothing and has no use, so is usually never called.However I am using try with resources for both to take care of closing closeable resources automatically.";non_debt
Sorry for the inconvenience.;non_debt
doesn't really make sense to use ignore for single output since you can just drop this instance alltogether;non_debt
Thanks @alekstorm . Appreciate that.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3888/;non_debt
"This PR modifies the display of the name of the note.
Now, instead of the full path, only the name of the note is displayed.
Improvement
[ZEPPELIN-3007](https://issues.apache.org/jira/browse/ZEPPELIN-3007)";non_debt
[HUDI-1089] Refactor hudi-client to support multi-engine;non_debt
@pvillard31;non_debt
CASSANDRA-15630 fix testSerializeError;non_debt
Nice catch. Originally I intended to use the return value;non_debt
I definitely missed that one extended the other. Thanks for catching this.;non_debt
Merged #7132 into trunk;non_debt
"This is a set of management APIs that was used by the Web UI, for creating,
modifying and deleting configuration files and entries. This is another step
towards removing the requirements for writing to configuration files.";non_debt
@bbejeck It seems your newly added test did hang in a build: would you mind taking a look? https://builds.apache.org/job/kafka-trunk-jdk8/1390/console;non_debt
Streams 33;non_debt
356066-3340 description-0;non_debt
Can we fix the problem if we use different `Pickler` instances for sending the input data to python and sending the UDF results to JVM?;non_debt
"Besides source and sink nodes, there are other places in the middle of the topology that would pass in serdes:
1. Materialized.
2. Grouped.
3. Joined.
We need to cover those cases as well.";non_debt
[HUDI-913] Update docs about KeyGenerator;non_debt
hi, there is a `.select('count.sum)` after `groupBy('word).select('word as 'word, 'num.sum as 'count)`;non_debt
"Fix #3680 
Change the load label of audit plugin as:
`audit_yyyyMMdd_HHmmss_feIdentity`.
The `feIdentity` is got from the FE which run this plugin, currently just use FE's IP.";non_debt
Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/632/;non_debt
"         * list lucene index gfsh command will now display on of three states
	* NOT_INITIALIZED if the index is present in only in defined map
	* INITIALIZED if the index is present in the index map
	* INDEXING_IN_PROGRESS if the index creation is in progress.
Thank you for submitting a contribution to Apache Geode.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
submit an update to your PR as soon as possible. If you need help, please send an
email to dev@geode.apache.org.";non_debt
@nlu90 - Should we remove the `local` prefix in pom.xml?;non_debt
I think the current approach is fine. I'm going to merge this.;non_debt
31006158-5410 description-0;non_debt
feat: return back the data just created via POST method in manager API;non_debt
I fixed about repair partition and verified that this patch had run as expected on testing cluster.;non_debt
Yeah it's at the frontier of data/metadata, though to me annotations are closer to metadata.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6353/;non_debt
Maybe change this to loadSchemaFromXml?;non_debt
cc @Ngone51;non_debt
On the jenkins slave pip2.7 is unavailable for same reason. Unfortunately I don't have enough privileges to see how the slave is configured.;non_debt
"Usually its corner cases like people putting quotes and/or commas in their values.
The ""safe"" thing to do (and what we do elsewhere) is to add each value individually with an invalid UTF8 character (like `0xFF`) after each string.";non_debt
"Thinking further on it, STORM-1876 is needed when building from the source. So it should be fine if not included in the release candidate. 
@ptgoetz  - +1 for merging this now and going ahead with release.";non_debt
How about using `ObjectIdentifier` here?;non_debt
correct. Dashboard is running in synchronized mode, there is no query id passed from query engine to dashboard. While in SQL lab, which is running in asynchronized mode, query id is saved into database, and celery Worker will update query status.;non_debt
This PR is the implementation of pre-quantized fully connected op.;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/7939/;non_debt
Added one comment.;non_debt
"Can we parameterize `JAVA_VERSION` and default still to 8?
So out build script can set `JAVA_VERSION` to 11 if needed.";non_debt
Which repositories have the dependency of index-common, indexr-segment?  I tried to build drill after applying the patch, and hit mvn dependency error.;non_debt
"Hi, @markhamstra ! Thank you for commenting. 
I agree with your viewpoint. So, this PR has a meaning to add just a function, `bround` not a HQL language level meaning. 
In terms of semantics, this is the same implementation with Hive. The following is [Hive code from the Hive master branch](https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/RoundUtils.java).
By the way, for the last issue, I think in a different way.
In order to become less and less directly dependent on Hive, we need to provide this as a Spark SQL function.";non_debt
+1 I like these improvements.;non_debt
yes, there issue is reported by tester. He use this csv file, it's difficult to replace it. I need reproduce it and validate with this file.;non_debt
This should be `byte` instead of `Byte`.;non_debt
"Use the scala-way.
`val chromos = Seq.fill(determinPopSize(conf, itemsMap.size) {
  Chromosome(conf, shuffle(itemsMap), conditions, topOutputSet)
}`";non_debt
[AMBARI-24632] APT/DPKG existence check doesn't work for system packages (dgrinenko);non_debt
We can specify that the document `contains online demo and screenshots`;non_debt
"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/22276/
Test FAILed.";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2587/;non_debt
 update worker get task from queue;non_debt
This was a common thread of feedback from other PR's as well-- I'll add some constructs to register values if not default.;non_debt
Add IdentifierValue;non_debt
"#1728
System property dubbo.service.delay invalid, and we fix this bug.
this bug caused by  appendProperties(this) called in doExport(), and we put appendProperties(this) in front of doExport().";non_debt
@bolkedebruin nice! super excited about this;non_debt
So that's weird... all tests pass both in the IDE and with maven verify from command line. I'd be happy to blame Jenkins but Travis fails the same way.. ðŸ˜®;non_debt
I like the `boost` package - I'm already thinking about a `TypeToBoostTokenFilter` that would automatically boost tokens marked with a `SYNONYM` type for example, and there are probably other boosting filters we can come up with, so a package to collect them all makes sense to me.  I prefer to group packages by functionality rather than implementation.;non_debt
add doc FAQ about a/b test;non_debt
So, shall we keep it as it is or restore it back?;non_debt
33884891-2011 comment-274330617;non_debt
"Currently, the WebServer side needs to process the entire set of results and stream it back to the WebClient, which puts immense pressure on the WebServer when rendering the resultset. 
Since the WebUI does paginate results, we can load a larger set for pagination on the browser client and relieve pressure off the WebServer to host all the data (most of which will never be streamed to the browser).
e.g. Fetching all rows from a 1Billion records table is impractical and can be capped at (say) 1K. Currently, the user has to explicitly specify LIMIT in the submitted query. 
An option is provided in the field to allow for this entry, and can be set to selected by default for the Web UI.
The submitted query indicates that an auto-limiting wrapper was applied.
In addition, the resultset is now configurable to allow for the default number or rows displayed per page to be changed from 10 to anything that the user might want.
Configuration additions in `drill-module.conf` (changes should be made in `drill-override.conf` ):
Screenshot with default (unselected) set to limit of 23:
Screenshot with default rows per page changed to 12.
Screenshot of the profile indicates the auto-limit having been applied:";non_debt
"Hmmmâ€¦ Seems @SolidWallOfCode fixed the issue 2 days ago with this commit https://github.com/apache/trafficserver/commit/e459667aad1756bad55db88235e8aa6dd229da61
I guess this PR is now useless.
Closing it";non_debt
Yes it conflicts, thanks for finding this Max. Under the new model (you can see in my PR that max linked https://github.com/apache/incubator-airflow/pull/1525 ) you would create a dependency class for future succeeded.;non_debt
"Addendum patch to fix compilation error
https://issues.apache.org/jira/browse/HDDS-5072
Existing testsuites.";non_debt
â€¦lled;non_debt
HDFS-15202 Boost short circuit cache;non_debt
@garyli1019 : Thanks for your suggesstion, i'll add usage in ITTest and try my best to mak it run in docker. If still failed, an available script is a good choice.;non_debt
`task.markTaskCompletion` should only be called if all retries fail. So calling it in a finally clause is not appropriate. What we can do is to make sure it is called if any `Throwable` is thrown.;non_debt
hi @tillrohrmann can this PR been merged into master branch, so that we can close it?;non_debt
Adding pre and post execute hooks to BaseOperator;non_debt
Sounds good.;non_debt
"Performed a build and verified both the Maven and the Hub (using 1.7.0) variants and worked as anticipated.  I think this should make it a bit easier for folks to migrate between versions.
Thanks for taking care of this and I'll get the merge in.";non_debt
Backport of the fix of issue KARAF-5796 into the karaf-3.0.x branch.;non_debt
"@niketanpansare I agree that this move towards Scala support is really a necessary step in the evolution of the project since we run on Spark, and it is a step that we need to take. I just wanted other Eclipse developers to be aware that this PR represents taking that step.
Also, thank you for the Scala IDE setup information. It is good to hear that the integration is clean.";non_debt
Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5304/;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1940/;non_debt
Review by @guozhangwang.;non_debt
Linux build _failed_! See https://ci.trafficserver.apache.org/job/Github-Linux/732/ for details.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6323/
Test PASSed (JDK 7 and Scala 2.11).";non_debt
"1 . variance same as var_pop
2. stddev same as stddev_pop";non_debt
Failed test seems irrelevant: org.apache.spark.sql.kafka010.KafkaDelegationTokenSuite.(It is not a test it is a sbt.testing.SuiteSelector);non_debt
Shall we call it `defaultCatalog`?;non_debt
41348333-33 description-0;non_debt
code good, verified L&N, verified builds on a few different systems without issue.  will merge;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/2874/
Test PASSed.";non_debt
This is the actual fix: as there are two state transitions, we cannot simply check for six transitions, because if we check too early, and only five transitions finished (or maybe none), `stateListener.numChanges` might still be at 4 or 5.;non_debt
"Thanks a lot for working on this @shixiaogang. ! ðŸ˜ƒ 
I just merged your PR, could you please close is.";non_debt
"add a simple classdoc here - something like ""A TopologyMapper that assumes all nodes are in the same rack""";non_debt
ðŸ‘Œ;non_debt
Fix #1468;non_debt
can we add an end-to-end test to show the correctness bug? I'm still not very sure why we compare rows with different schema. Sounds like something we should forbid at the analysis phase.;non_debt
"Yeah, I've also had some discussions in our weekly sync while didn't figure out any better solutions.
There're several reasons:
1. Different ops have different requirements over specific inputs
2. While the problems is in a subgraph generated in Relay Integration, the placeholder are all the same, we can not differentiate them from tag, name or any other way, even the order of inputs are not guaranteed.
Current approach is to merge all specific inputs checking to this function, at least they have a same entry here. For the other ops, you have to add their own check functions below.";non_debt
You might want to change the logger in `BaseCombineOperator` to private?;non_debt
[SPARK-24909][core] Always unregister pending partition on task completion.;non_debt
Wait. As a sender, I don't expect a reply to be sent to my `receive` method. I expect the reply to be sent back to the point in the code that made the call - which is why I've mentioned before that `sendWithReply` should return a Future.;non_debt
"hi sujith
i am thinking if user already trim the data with the option setting,then when user query with some space filter,it would no getting result.";non_debt
`which is different with Announcer` means `NodeAnnouncer announces single node on Zookeeper and only watch this node`, while Announcer watches all child path, not only this node;non_debt
â€¦gionServer can fail with retries exhausted on an admin call.;non_debt
"# [Codecov](https://codecov.io/gh/apache/airflow/pull/4963?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/4963?src=pr&el=continue).";non_debt
This problem was introduced by private repository, currently no need to apply this PR, so close this.;non_debt
jenkins, retest this, please;non_debt
sorry I literally meant the SQLQueryTestSuite, which is file based :);non_debt
"I finalized the changes, however there are a couple of errors in my builds which I think are irrelevant to my changes.
Here are the links to my build results (please let me know if you cannot access them):
Travis-CI: https://travis-ci.org/ozymaxx/thrift
AppVeyor: https://ci.appveyor.com/project/ozymaxx/thrift/build/1.0.0-dev.42
Here are the error descriptions:
On Travis CI `#19.7`:
On Travis CI `19.11`:
On AppVeyor `Environment: PROFILE=CYGWIN, PLATFORM=x64, CONFIGURATION=RelWithDebInfo, DISABLED_TESTS=(ZlibTest|OpenSSLManualInitTest|TNonblockingServerTest|StressTestNonBlocking)`:";non_debt
Actually you should not need to disable it here, because you documented the class (starting from line 29). It should just work if you remove the complete docstring here.;non_debt
HadoopMapReduceCommitProtocol is somehow a custom committer in Spark. Actually we use it with dynamicPartitionOverwrite in this way, at least for data source.;non_debt
"Mutable nodes violate a basic assumption of catalyst, that trees are immutable. Here's a good quote from the SIGMOD paper (by @rxin, @yhuai, and @marmbrus et al.):
Mixing mutable nodes into supposedly immutable trees is a bad idea. Other nodes in the tree assume that children do not change.";non_debt
will do;non_debt
Should we do the operations above this point in the transaction as well? That seems reasonable to me. I'm not sure why we don't in other places.;non_debt
Just include it into the next `if` clause.;non_debt
:+1: after my comment around the README fix is addressed;non_debt
"No worries! I think it is easiest to do development from a branch other than your master branch. Here is a sequence of commands that might work:
git checkout -b add_escape_utils
git fetch upstream master
git reset --hard upstream/master
git cherry-pick 2a4f1bd68a17d3fc765a9d6c8a4e48ef21c77bfe
git cherry-pick 37651b83bca350dd3c50da16e6b4eee0f2768a7f
git cherry-pick b3fbe7540ffa47905bbbfa172ca08da40387cc58
git cherry-pick 4e6a0d57958a3575b1bfd7647538aaf61b28c712
git cherry-pick fd63fe95eaadb31209bf245e393e60d0030dbb88
git checkout master
git reset --hard add_escape_utils
git push -f origin master
That would update this PR. For future contributions you may want to create a new branch off upstream/master and do your development there, it makes rebasing a lot easier. For example, in your local master, every time you do a ""git pull upstream master"" you will get a merge commit (like you see in this branch/PR).
If something gets messed up and (god forbid) you don't have your commits after any of this, I took the liberty of doing the above and pushing to my fork: https://github.com/mattyb149/nifi/tree/add_escape_utils
Looking forward to having this contribution in NiFi, thanks again!";non_debt
Filed https://github.com/apache/incubator-druid/issues/6009;non_debt
Hmm, I just want a test case to show it actually order the bins by soft prediction. Although @jkbradley suggested we should use directly `binsToBestSplit`, but in order to do that, we also need to expose many details of `findBestSplits` too, e.g., `binSeqOp`, `getNodeToFeatures` and `partitionAggregates`...etc.;non_debt
Triggers: handle missing case;non_debt
Thanks for your review, @kl0u , @gaoyunhaii !;non_debt
Welcome, @cguan7!;non_debt
"Modify the if-condition in ```hadoop-ozone-manager.sh```
https://issues.apache.org/jira/browse/HDDS-2361
Ran the command in ```hadoop-ozone/```:";non_debt
Retest this please.;non_debt
[ISSUE #1564]Fix the ip filter logic in IPv6/IPv4 coexist environment;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3223/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
Yes it is.  The enclosing namespace starts on line 665, just before the definition of `struct ps_main_conf_t` above, and ends on line 1287, just after the definition of `ps_determine_port()`.;non_debt
Mentioning this on the KIP slipped.;non_debt
"currently, we never update Routing on instanceConfigChange callback because of the following code in HelixExternalViewBasedRouting.isRoutingTableRebuildRequired.
Pinot expected Helix to populate the version field but that does not seem to be the case. Will file a separate Helix ticket to fix this in Helix. 
For now, since we already read the versions in processInstanceConfig method, the fix was to set the version in the instanceConfig";non_debt
Note that we don't lock the `_schedule` here - is that Ok?;non_debt
"Some updates here, I found the place where we fail, it is inside startMiniCluster, where we will scan meta to see if cluster is up. Still need to dig why we can not get meta location from MasterRegistry.
And #2420 is for the missing stack trace. We just throw out the exception in scanner.next so it lost the actual stack trace which makes really hard to find out the place we fail.";non_debt
@bufferoverflow I'm sorry for not explaining, the intention was merely to initiate a discussion. I would love the option to generate only ARC-compatible code, with a CLI flag/option which would remove lots of boilerplate setters/getters. I will check JIRA and link a ticket if that's not already there in some form. Thanks;non_debt
[wip][auto_scheduler] buffer support, correctness check;non_debt
Actually, when will a user want to specify non-nullable for any json field? I am not sure if we are actually addressing the right problem. I am wondering if we should just not allow non-nullable fields for json.;non_debt
"I think this error should be handled in onSchedule. 
This function (processbin) is called runtime, config issues shouldn't be handled here.";non_debt
Done in #8255, will rebase;non_debt
"Thanks for your comments.
I have added a note for this.";non_debt
What will happen if there is incompatible message in Kafka? Will pipeline stall? What will be the way to fix it without purging whole kafka topic?;non_debt
"This solution makes sense to me -- no new `SelfEdge` class and we now have a test case. Thanks.
VOTE +1.";non_debt
Will add orc once ORC patches are committed which read/writer Iceberg generics;non_debt
If there were jiras around what needed to be done or worked out I'm sure some people would want to work on them;non_debt
@rhtyd it requires haproxy 1.8+ for http2 support. As I remember haproxy 1.8 is installed in debian10 systemvm template. No other changes is needed in systemvm template;non_debt
â€¦xchange received (#2669);non_debt
this is not important, but you can just pass nameIdentifier and .toString() will be called implicitly;non_debt
Changed it to `GreedyRequestAllocator`;non_debt
Failing Java11 tests unrelated:;non_debt
Done;non_debt
"The code that introduces a cleanup probably comes from:
The test creates a bunch of pipeline objects with references to them in the interactive environment. The test tries to find out whether a `cleanup(pipeline)` is invoked when explicitly tracking all user pipelines if some of the pipelines held in the environment is no longer in scope. Of all the pipeline objects created, only one would be cleaned up (because we intentionally put a string typed ""pipeline"" in the interactive environment so it would just ""disappear"" when tracking pipeline-typed objects), so cleanup should just be invoked once during the call. The changed test still tests this logic.
R: @TheNeuralBit 
PTAL, thx!";non_debt
Perhaps also mention the option to specify the target path?;non_debt
Ya this is in RsRpcServices under scan() so should be ok. One thing I noticed is that - once  a scan is created and a next() call happens does the rpcScanRequestCount becomes 2 or is it 1?;non_debt
will add UserWarning and I checked the target file existence before starting downloading at line 280;non_debt
"I'm not convinced this is the right fix.
1) Calling `basic_config` is something we want to avoid (we used to have it all over the place and it caused problems, so we now only want to configure logging from the one place)
2) Normally we add a default config entries to airflow/config_templates/airflow.cfg. I think the fix here would be to add values for the things being complained about.";non_debt
We already have SparkBuildInfo for this purpose.;non_debt
@fjy Ah, sorry. I've missed your comment.;non_debt
Dummy PR;non_debt
"I think that it has a problem when we use shared utf8proc for the arrow static library.
How about using `INTERFACE_COMPILER_DEFINITIONS` property like https://github.com/apache/arrow/blob/master/cpp/cmake_modules/ThirdpartyToolchain.cmake#L2125-L2126 ?";non_debt
@sv2000 can you merge? thanks;non_debt
 Fixed mask for XML elements containing attributes;non_debt
done.;non_debt
@hachikuji Always routing to controller could bring a consistent view.  I believe KAFKA-9096 was triggered by such an inconsistency caused by asynchronously updates. What do you think?;non_debt
45896813-172 description-0;non_debt
[metrics] Make DorisMetrics to be a real singleton;non_debt
"org.apache imports need to go into their own block just above any static imports, ordering imports
This is to try and keep cherry-picking under control.";non_debt
We should also update the SclaDoc for `ContainerProxy`;non_debt
CI is not passed yet~ Please take a look. Thank you. @rongzha1;non_debt
Done;non_debt
[Issue #5176][pulsar-broker] Fix bug that fails to search namespace bundle due to NPE;non_debt
I believe the purpose of retryCounter.sleepUntilNextRetry() should be uninterrupted sleep because RetryCounter is mainly being used by retries with sleeps and retries with different backoff policies. In such scenario, RetryCounter being a library should not ideally throw InterruptedException even if sleep is interrupted because it is being retried by clients to achieve certain tasks.;non_debt
@haiminh87 I was thinking if we could make this configurable in sense that have a boolean like readUsingLatestSchema with a default value of true and can be overridden via TypedProperties instance.;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3673/;non_debt
Sorry for the delay. I think the API with location parameter should be retained as one may want to maintain the table metadata by itself. While in the case of `HadoopCatalog` we should not call that API since it is assumed the metadata is delegated by the catalog. Make sense?;non_debt
"@viirya Yeah, a normal temporary table would be resolved as a LogicalPlan. Analyze Table does not give us any benefit there. 
However, you are also allowed to do this:
For these I would like to be able to collect statistics.";non_debt
LGTM;non_debt
I think from below comment we don't need this new method.;non_debt
Packaging result: âœ–centos6 âœ–centos7 âœ–debian. JID-31;non_debt
"#8292: Added Portuguese translations for ""Edit dashboard"" dropdown";non_debt
Why do we think that an `IOException` might occur? Would a file read/write suppose to happen in an implementation?;non_debt
Build Failed with Spark 2.2.0, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/631/;non_debt
Run Spark RunnableOnService;non_debt
"What I mean is this:
In #4027 I added a bunch of exclusions to the `pom.xml` but they turned out to be redundant and the problem was something else IIRC.
There is a subtlety that I only just sorted out fully. The patterns in a `.gitignore` are resolved relative to the `.gitignore` but _also_ directories inherit the `.gitignore` from parent directories.
So when `.gitignore` contains `vendor/` this means that when `git` is working in `sdks/go` it will inherit the `vendor/` ignore pattern, and it will work. But when the working directory is the root, `sdks/go/vendor` is not ignored.
I think it is probably best to just have full patterns in `.gitignore`, too. It also supports the same `**/vendor/` syntax.";non_debt
So, only whitelist, right?;non_debt
[SPARK-12573][SPARK-12574][SQL] Move SQL Parser from Hive to Catalyst;non_debt
"The AkkaRcpActor should stop itself immediately after the onStop future has been completed.
Before we sent a Kill message which enqueues into the mailbox and does not overtake messages.
Now we call Context.stop(ActorRef) which directly stops the AkkaRpcActor.
- Added `AkkaRpcActorTest#testOnStopFutureCompletionDirectlyTerminatesAkkaRpcActor`.";non_debt
We need to follow this restriction of Hive? IMO its okay to support any char as a line separator normally.;non_debt
Update `estimateDim` to use `blockInfo`.;non_debt
Read/write separated deployment doc;non_debt
perhaps wrap in Collections.unmodifiableList?;non_debt
Display error message for NetworkErrorCode;non_debt
@guozhangwang A minor change that has ConsoleConsumer commit offsets manually just like what MirrorMaker does. In the original version, automatic offset committing does not work well with `--max-messages` since it sees all polled messages as consumed which does not honor `max-messages` config.;non_debt
Can you not use the static_pointer_cast since this is already a shared_ptr?;non_debt
ditto;non_debt
We don't use `@Author` tag in ASF projects.;non_debt
"@StefanRRichter Thanks for you work! ðŸ‘ 
I merged this, could you please close the Jira issue and this PR?";non_debt
Given the desire to also compile for netstandard, this seems reasonable.;non_debt
+1, merged to 3.3 and trunk;non_debt
"mode determined by SC?
i remember changed design that determined by microservice?";non_debt
@gianm, @clintropolis: Ready for re-review;non_debt
You are welcome. Right, we can keep improving base on it. ðŸ˜„;non_debt
deleted.;non_debt
final?;non_debt
Merged with patch, close the PR.;non_debt
sync -vm3 and -vm4 yaml. promote INFRA-14849 to production.;non_debt
KAFKA-7164: Follower should truncate after every leader epoch change;non_debt
 Merged build triggered.;non_debt
Pushed a new version, sql passes 100%. Lots of other things fail. Does this look reasonable? How do we get this in? Do we need to hide this behind a flag like we did with warnings as errors?;non_debt
"Done. Fixed things after receiving +1 are here:
1. nimbus.clj: just log exception instead of crashing nimbus when blob-rm-dependency-jars-in-topology
2. DependencyPropertiesParser.java: check --jars parameter string is empty, and treat it as empty list (also add new unit test testing this change)";non_debt
From DB2;non_debt
"well, I am leaning towards considering a heartbeat with `""""` agent class an error (although we do not check for such cases), it all depends on what the constraints of the c2 protocol are, do you think an `""""` agent class should have the same semantics as a ""missing"" agent class, or we could even abandon the ""missing"" agent class, and use the empty string to denote an agent with non-specified class?";non_debt
FINERACT-155 issue resolved;non_debt
This can be final.;non_debt
Can all the println statements be removed?;non_debt
added case to verify temporary view masks permanent view with same name;non_debt
This option is not available in Python 2.x:;non_debt
Could we merge this please?;non_debt
Move this method besides `snapshotState`.;non_debt
Jenkins, retest this please.;non_debt
WorkerServer refactor;non_debt
"Although it's important to track upstream, Java 14 is already EOL.
Only Java 8 / Java 11 / Java 17 (September 2021) are LTS.";non_debt
Allow Writers, Converters, and QualityCheckers to read the same config key from different branches;non_debt
LGTM;non_debt
"This line is in the if-check block of `if slas:` https://github.com/apache/airflow/blob/fc3b45a61ac51441c3b9e4d99a20473cd3664056/airflow/jobs.py#L673
It's impossible for `len(slas)` to be zero here (correct me if I missed anything here)";non_debt
@jon-wei we'll also need a TOC entry;non_debt
"-->
Fixes #7815 
add functions-worker process jvm metrics, use cluster-name, type lables to distinguish function-worker and function.";non_debt
I would replace this one with a junit rule: https://garygregory.wordpress.com/2010/01/20/junit-tip-use-rules-to-manage-temporary-files-and-folders/;non_debt
"I had a look at this (old) ticket/pull request today.
The main goal at the time was to enable having control over defining a key so that the records with the same are all going into the same Kafka partition.
What I found is that as part of https://issues.apache.org/jira/browse/FLINK-11693 that you (@aljoscha) created the KafkaSerializationSchema which produces a ProducerRecord which has a key and a value (both byte[]).
It looks to me like this feature is the better implementation of the goal of this ticket.
@aljoscha Can you confirm that this change already does what this ticket intends to do also?";non_debt
fix: use dashboard id for stable cache key;non_debt
"@xubo245 What does 'CSV table' mean in the title?
Can you explain why the version upgrading for common-langs is needed? Does it improve something? (Just curious)";non_debt
Build Failed  with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/852/;non_debt
As we mainly use those tools in CloudStack for validating;non_debt
if we failed to create transaction, we have to error out the `toFlushList`.;non_debt
[WIP]Fix constants and method names;non_debt
resolved via https://github.com/twitter/heron/pull/1629/commits/3be17b3309d59eb60b5326d4e37c0f9c8529d3b1;non_debt
[approve ci];non_debt
"First of all, I'm sorry for mistake.
It's for reverting broken #279 and reapply correct patch (using System Environment, not JVM Property).
I reverted #279, and rearrange #279's changesets into one commit.
You can find further information from #279.
If Storm project has a rule about reverting and this PR doesn't fit, I'm sure to wait @clockfly to revert, and re-create PR.
@ptgoetz @clockfly @harshach Please take a look and comment. Thanks in advance.";non_debt
"dreiss's copyright statement says, ""provided the copyright notice and this notice are preserved."".  I don't think we should be removing it.  You can move it down to be below the Apache Thrift license, or @jfarrell needs to approve this change.";non_debt
@felixcheung correct - i missed it - updated doc/examples.;non_debt
"username is mandatory and should normally always be available so this is just a ""fake"" key must not collide with any username. I don't see any advantage in having it null compared to a synthetic one.";non_debt
Travis seems to have one test failing, but when I run it locally all tests pass. I'm gonna look into this tonight.;non_debt
fix to bom;non_debt
Thanks all for reviewing! The latest change looks good to me too. Merged into master.;non_debt
LGTM. Thanks.;non_debt
HBASE-24446 Use EnvironmentEdgeManager to compute clock skew in Master;non_debt
"Yes, I saw that earlier.
One note is all of the `StreamsUpgradeTest `s are failing, but don't use the `VerifiableProducer` at all.  
The latest PR from @mjsax upgrading streams has all streams system tests passing.  I'm thinking maybe once that PR is merged I'll rebase and try again.  
WDYT?";non_debt
"I tried to remove the `try...catch...` here and let `ApplyNameMapping` to return the schema with partial IDs assigned. That causes problems when we visit the parquet type.
Firstly, the `PruneColumns` visitor assumes the `typeWithIds` is fully assigned with IDs. Its `getId` has a precondition check. A file schema with partially assigned IDs cannot pass the condition check.
Secondly, the `HasId` logic also has the same assumption, which makes `SparkParquetReader` throws NPE when it calls `filedType.getId().intValue()`.";non_debt
"@nsuke: any comments from your side? 
this is from f43d0ca6e57c4c30ea742e5f80e086288e999ecb";non_debt
Can you delete this function since it's commented out?;non_debt
I went ahead and pushed my last attempt for the record, but am not going to merge this. I'm going to instead propose just the changes that seem to have no downside.;non_debt
Remove stdout prints?;non_debt
I like this modification here!;non_debt
Looks like comma should be before `COLUMNS`.;non_debt
@yihua is this ready for review?;non_debt
Don't package the image every time. The better way to mount a volume including the target tar file, you could mapping this volume to any place you like.;non_debt
"If this issue is fixed, It may be possible to use ```spark-submit --packages com.uber.hoodie:hoodie-spark-bundle:0.4.5,com.uber.hoodie:hoodie-utilities:0.4.5 ...``` as hoodie utilities brings the shaded versions of hive bundled.
I can check it later to see if it works.";non_debt
LGTM;non_debt
ah, now I got it! What a hidden bug ðŸ˜‚;non_debt
`hasNext()` is correct since interval upper bound is open-ended.;non_debt
LGTM;non_debt
Build Success with Spark 2.3.2, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/11891/;non_debt
And here also.;non_debt
STREAMS-62 | Serializability in processors;non_debt
HOP-303;non_debt
62117818-310 review-94093632;non_debt
@rxin, that's true, but it affects the API because it may change the structure of the builder or may be a reason to use a different pattern.;non_debt
"# [Codecov](https://codecov.io/gh/apache/arrow/pull/9017?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/9017?src=pr&el=continue).";non_debt
If an exception doesn't have any error message, it can't appear in the summary.;non_debt
can we add an analyzer rule to catch all the CREATE TABLE like commands and add the ownership properties?;non_debt
Still getting errors due to container create calls timing out. This PR changes the logic to retry starting the container up to 3 times. If it fails after 3 times, it will raise the exception.;non_debt
LGTM pending tests.;non_debt
@feng-tao @Fokko Thanks! :);non_debt
"Choose one
Show a generic error message and hide stacktrace if SHOW_STACKTRACE feature flag is disabled
Tested by enabling and disabling SHOW_STACKTRACE feature flag.";non_debt
"Jira is used for more than you might think.
It is used to build release notes as well as a metric in board reports for community activity.
There is no reason not to file a ticket, just mark it as improvement.";non_debt
31006158-5506 description-0;non_debt
"Hi @summerleafs!
I think we cannot use this approach to fix this. The most important thing is that this introduces a blocking operation (`.get()` on the future) in the call, which will make the while `scheduleEager()` call block. Since the ExecutionGraph runs in an actor-style context, methods must never block. Everything must be implemented with future completion functions.";non_debt
you can see that if the `nnz` grows, the speed up decrese. That is because with a big `nnz`, the  searching complexity `log(nnz)` dominate the whole process. However, when `nnz` is a small number (most frequently), the conversion is relatively the main part.;non_debt
"I am not sure if it is ok to wait ... This is something I never considered
from the beginning when I added process_local ... Maybe it is ok !
If it is not, then we might need to come up with something.
Unlike earlier, the noPrefs list now truely contains tasks which have no
preference (earlier task failure also ended up here) ... So maybe not
common anymore ? And so ok to wait ?
On 17-Jul-2014 12:26 am, ""Matei Zaharia"" notifications@github.com wrote:";non_debt
Build Success with Spark 2.2.0, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/648/;non_debt
ARROW-4076: [Python] Validate ParquetDataset schema after filtering;non_debt
[hotfix][docs] Align the documentation of checkpoint directory to the actual implementation;non_debt
Hi @iemejia, It seems that the last failure of jenkins ( https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/13334/console ) does not relate to this PR;non_debt
Maybe adding `BOOST_SOURCE: BUNDLED` around [here](https://github.com/apache/arrow/blob/master/.github/workflows/cpp.yml#L179) would be sufficient? The job is already building Thrift.;non_debt
Unclear, but this successfully removes lintr (and associated failures), it seems? seems OK to merge if so.;non_debt
Can we add a comment explaining that the expected result is 1 or 2 depending on whether both ipv4 and ipv6 are enabled or not?;non_debt
" Green Travis build https://travis-ci.org/DmytroShkvyra/flink/builds/209713170
This PR is renewed https://github.com/apache/flink/pull/2870";non_debt
"@lizhanhui , # 4 is not the nessary conditions, since even it is not enble, the tpInfo's method is still used.
        if (this.sendLatencyFaultEnable) {
            try {
                int index = tpInfo.getSendWhichQueue().getAndIncrement()
                for (int i = 0 i < tpInfo.getMessageQueueList().size() i++) {
                    int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size()
                    if (pos < 0)
                        pos = 0
                    MessageQueue mq = tpInfo.getMessageQueueList().get(pos)
                    if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) {
                        if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName))
                            return mq
                    }
                }
                final String notBestBroker = latencyFaultTolerance.pickOneAtLeast()
                int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker)
                if (writeQueueNums > 0) {
                    final MessageQueue mq = tpInfo.selectOneMessageQueue()
                    if (notBestBroker != null) {
                        mq.setBrokerName(notBestBroker)
                        mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums)
                    }
                    return mq
                } else {
                    latencyFaultTolerance.remove(notBestBroker)
                }
            } catch (Exception e) {
                log.error(""Error occurred when selecting message queue"", e)
            }
            return tpInfo.selectOneMessageQueue()
        }
        return tpInfo.selectOneMessageQueue(lastBrokerName)//thrown from here
Besides, if topic route info is null which propably means user is send through my seletor method, resend should still respect user's seletor, so the same chosen broker is not enough, the same chosen queue is needed too, which may be another issue.
I guess it is not a very minal effort, since the existing interface does not record any chosen queue info.";non_debt
True. But since I didn't introduce it, I'll keep it like it is (avoiding further downstream merge conflicts).;non_debt
"Is it better to create a new api called getPermissionsOnPartitionedTopic?
I understand if I support searching partitioned topic in internalGetPermissionsOnTopic. The output structure of getPermissionsOnTopic will be changed to
Map<String, Map<String, Set<AuthAction>> which will break API compatibility?";non_debt
[BEAM-4065] Performance Tests Results Analysis and Regression Detection (do not merge - test);non_debt
LGTM;non_debt
"I just merge the patch into Apache Camel master branch with some minor changes.
UnitOfWork can help us to do some clean up work after the exchange is processed by the route. Your test case doesn't show that part, I think we can leave the question there until we need to fix this kind of issue.";non_debt
"Two main goals of this PR:-
- Start Pulsar services (broker, proxy, websocket, discovery) in TLS only mode, such that they only listen on TLS ports. 
- Once ServiceUrlTls is set enableTls becomes redundant information - hence getting rid of the flag in relevant components.";non_debt
[BEAM-6810] Disable CalcRemoveRule to fix trivial projections;non_debt
/pulsarbot run-failure-checks;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3085/;non_debt
"why need to split tags?
servo/spectator's already have tags information.
if lost these informations caused by our mechanism, then we should change our mechanism.
this is why i DO NOT agree work on current mechanism.";non_debt
IGNITE-5963: Add additional check to Thread.sleep to make test correcâ€¦;non_debt
"use `names.mkString("", "")`";non_debt
GEODE-5971 Refactors ShowMetricsInterceptor, DeployFunctionCommand and;non_debt
retest this please;non_debt
"@ptgoetz sorry for being the newbie here, but does this mean a new jar will be built of 0.9.3 that will include this? or will we have to wait for 0.10.0? 
we could really use this fix";non_debt
Run Portable_Python PreCommit;non_debt
I think I mentioned this earlier, but any reason not to use guava's `ByteStreams.copy()`?;non_debt
I don't think it can happen here but we should be confident that none of the integration tests could accidentally delete a user's _real_ keys in the cloud provider's (or Vault's) secrets store.;non_debt
Build Failed  with Spark 2.4.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.4/685/;non_debt
This is the right Jira: https://issues.apache.org/jira/browse/AIRFLOW-3074;non_debt
@pnowojski  thanks for reviewing again. Tests on Travis have passed, so I merged the two commits.;non_debt
can we support while?;non_debt
Merged build finished.;non_debt
[MINOR][DOC] Name refactor SystemML to SystemDS in Documentation;non_debt
"I think this can be done by configuring jets3t properties (which Hadoop S3 FS uses internally). Look for the various httpclient.proxy\* keys:
http://jets3t.s3.amazonaws.com/toolkit/configuration.html#jets3t
I'll research this and comment back.";non_debt
Closing in favour of #8988;non_debt
+1;non_debt
[CALCITE-3891] Remove use of Pair.zip in RelTraitSet;non_debt
Could pass a KeyExtent here, which would provide the information needed to implement `getEndRow()`, `getTableId()`, and  `hasTableId()`.  These are know by the initialization code.;non_debt
If I understand well this class is used to restrict casting timestamp/date to boolean, double, byte, float, integer, long, short values. I am not sure why we should deal with these checks at this point but I if we really need this then I guess it makes sense to extend it so that we apply the same checks for all types under `hive.strict.checks.type.safety` property. Should we create another JIRA for this?;non_debt
Looks OK;non_debt
"Sounds good to me!
Also backing up this decision, I just noticed this:
Pretty clear the previous behavior was a bug!  Glad we've fixed it.";non_debt
51905353-7565 description-0;non_debt
ok, I'll do later.;non_debt
I'll find a better name. IMO `Dummy` means nothing, so if names are important we could probably find a better one here. This isn't a dummy input dstream at all, it's giving access to its rate limit.;non_debt
â€¦irst;non_debt
I turn it on again in new commit in order to test if all tests can pass. I will turn it off by default later.;non_debt
LGTM except a minor comment;non_debt
@wu-sheng I modified some content, can you help me see if this is feasible?;non_debt
@liuhengloveyou;non_debt
@samkum Just this fix or combined with #16387?;non_debt
remove this?;non_debt
"need to fix these linting errors:
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/ChartContainer.jsx
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/charts/TimeSeriesLineChart.jsx
/home/travis/build/airbnb/caravel/caravel/assets/javascripts/explorev2/components/ExploreViewContainer.jsx";non_debt
Again, this is how the projects are named in master. We can change those separately if desired, but this is generally the convention used for java artifacts that depend on specific Scala binaries.;non_debt
Is the 'Caching' part necessary here (even if it always is right now)?;non_debt
ARTEMIS-2697 Avoid using raw types for Persister<T>;non_debt
That is also true for FakeDocumentModelProvider.;non_debt
"FAILURE
 No test results found.
--none--";non_debt
But your idea sounds good. I think we could even remove the underscore, set `True` as the default value of `native` and expose that transform to users (not in this PR though) WDYT?;non_debt
"Hi @bedlaj @omarsmak @DenisIstomin 
Thanks so much for your feedback. I think I fixed all your suggestions. Please, let me know if you have other suggestions.";non_debt
ðŸ‘;non_debt
Nice! Could you add the BM results?;non_debt
ðŸ‘;non_debt
"The reason for change to var from def is perhaps subtle.
Consider the case of :
add for mapIdToIndex with mapId 0
add for mapIdToIndex with mapId 1
add for mapIdToIndex with mapId 0 (on re-execution)
add for mapIdToIndex with mapId 1 (on re-execution)
Now both 0 and 1 will end up with the same index assigned (since it was based on mapIdToIndex.size).";non_debt
 Merged build triggered. Build is starting -or- tests failed to complete.;non_debt
Sure thing.;non_debt
"thanks @neykov for your comments. I've addressed them but I think there are still problems 
Failed tests:
  ApplicationResourceTest.testDeployApplicationYaml:236 expected [simple-app-yaml] but found [Application (p6lRzMdf)]
  ApplicationResourceTest.testReferenceCatalogEntity:249 response is Client response status: 500 expected [true] but found [false]
not sure what is causing them to fail. can you @neykov help on that?";non_debt
"# [Codecov](https://codecov.io/gh/linkedin/pinot/pull/3121?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/linkedin/pinot/pull/3121?src=pr&el=continue).";non_debt
Build finished. 0 tests run, 0 skipped, 0 failed.;non_debt
Same as above.;non_debt
Run Seed job;non_debt
see above comments, need more work on 'depends_on';non_debt
can you kill this blank line;non_debt
are `row_groups` also integers? 0-based?;non_debt
[FLINK-6117]Make setting of 'zookeeper.sasl.disable' work correctly;non_debt
[FLINK-1544] [streaming] POJO types added to AggregationFunctionTest;non_debt
#7611 is out to cherry-pick the Python pre-commit fix to the release branch.;non_debt
"This issue is based out of comment https://github.com/apache/zeppelin/pull/3370#issuecomment-511281165, where Injellij shows unknown error.
[Improvement]
* CI should be green
* Intellij IDE should not show any error";non_debt
"""Failed to unsubscribe subscription %s of topic %s""";non_debt
fixed, thanks;non_debt
I suggest to list some of the options, otherwise, users may still don't know how to use it.;non_debt
"Yes, more work is required to fully remove the JS support.  But, as an initial step, we could just stop running javascript target during the build process.
I'll move all these commits to main and 3.x
Thanks.";non_debt
For timestamp and date type, we already add `Instant` and `LocalDate` from java8 time API. In that case, we are very likely to add `Duration` and `Period` later for CalendarIntervalType later too, then it seems more reasonable to add `CalendarInterval` encoder support first as the encoder has half-implemented already.;non_debt
"Hi, All.
This seems to break all Jenkins job in `branch-3.0`. Could you make a follow-up?
- https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/spark-branch-3.0-test-maven-hadoop-2.7-hive-2.3-jdk-11/278/";non_debt
Build Success with Spark 2.3.2, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/10457/;non_debt
Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1133/;non_debt
"This fixes a bug in the plugin_verifier so that it does notreport a malformed config when a plugin uses a long option
parameter in a @pparam field.
- Traffic Ops ORT
See the traffic_ops_ort/plugin_verifier/README.md and execute the
plugin_verifier against a copy of the remap.config which has entries
for the cachkey.so plugin using long options in its @pparam fields.
You should not see any errors.
- master (b31db7)
None
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
""License"") you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->";non_debt
Kill off old templates;non_debt
@PramodSSImmaneni I don't see any difference with master?;non_debt
Note alternatively we could change it to not fail on fetch failure. This would seem better to me since there is no reason to throw away all the work you have done but I'm sure that is a much bigger change.;non_debt
oh yes I got messed up :P;non_debt
Can one of the admins verify this patch?;non_debt
Done.;non_debt
[SPARK-25737][CORE] Remove JavaSparkContextVarargsWorkaround;non_debt
close/reopen to rebuild.;non_debt
Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/3421/;non_debt
It was formatted;non_debt
The suggestion to put concrete examples of probability is great. I was considering removing the comment altogether after your comment above about the birthday paradox, but this would be very useful to add.;non_debt
[AIRFLOW-1255] Fix SparkSubmitHook logging deadlock;non_debt
"I didn't mean ""The second aspect of re-running only the necessary tasks"". What I mean is -- assume the first MR job scheduled 100 mappers, 90 of them succeeded and 10 them failed, so the first job successfully updated 90 regions but the whole job failed. The second job (the retry job, assume it succeed) still schedule 100 mappers but only 10 mapper should actually update the stats on the 10 regions which failed in the first job and the other 90 mapper should skip stats update and succeed.";non_debt
@hvanhovell  Thanks for the catch. I will try to test hive behavior more, like how `select 5 div 3.0` behaves in Hive.;non_debt
Allow to use $or with a mango JSON index;non_debt
ARROW-2720: [C++] Defer setting of -std=c++11 compiler option to CMAKE_CXX_STANDARD, use CMake option for -fPIC;non_debt
33884891-15112 review-605095651;non_debt
@ijuma Thanks for the review, merging to trunk.;non_debt
Same thing, where is the `#state{}` record saved on disk?;non_debt
Anyone got an idea why Jenkins suddenly fails on JavaDoc issues in classes that aren't related to this PR ?;non_debt
What does it return?;non_debt
"Thank you I found it [here](https://github.com/apache/couchdb/blob/COUCHDB-3326-clustered-purge-pr5-implementation/src/couch/src/couch_db.erl#L521)
This means that return values from couch_db are inconsistent ([POLA](https://en.wikipedia.org/wiki/Principle_of_least_astonishment)). Here is a list of functions which deal with different sequences:
- `get_update_seq` returns `integer()`
- `get_purge_seq` returns `{ok, integer()}`
- `get_oldest_purge_seq` returns `{ok, integer()}`
- `get_compacted_seq` returns `integer()`
- `get_committed_update_seq` returns `integer()`
I think that since we are updating purge feature we should take an opportunity to make it consistent, change return type and update all callers.";non_debt
"Interesting to see this, the author of the CLI implementation assumed that there can't be a next schedule if there has not been any execution yet  
which contradicts my implementation where when this is the case, I derive the first planned run date from the tasks and normalize it to calculate the actual first execution date: 
@ashb @Fokko and all others, I must admit that I'm not sure which point of view is the right one here since the CLI implementation has been accepted in the code base already, does that mean that it's right?";non_debt
On one hand I agree with this and this is something I have implemented in past too but on the other hand, user would not want to change `hbase.regionserver.slowlog.ringbuffer.size` that frequently. The default value for the config is not too high, not too low either. Hence, the reason to have enable/disable config is to ensure we provide reasonable default value to ringbuffer size, something user doesn't have to spend much time with and let user enable this config without touching ringbuffer size.;non_debt
Hi @cmccabe , when you said enable batching in FindCoordinator, do you mean enable batching in FindCoordinatorRequest class? Or you still mean to modify and improve on what we currently have? Thanks!;non_debt
Just pushed the changes. Please take a look and comment.;non_debt
reviewing;non_debt
"Thanks @virajjasani .
Let me take a look at the failed UT.
And then roll a RC2 with @joshelser 's patch in place.";non_debt
[SPARK-26975][SQL] Support nested-column pruning over limit/sample/repartition;non_debt
"actually, its not covered there... So, if any of my expression is calling if expression ( nested, with different data types..) .. its failing because of this issue.
 Because if expression datatype only considers true expression.datatype, and not considering false expression's datatype.
As date type is represented by int and timestamp type is long, thats why there are two scenarios:
2. While in the reverse case, long cannot be assigned to int without explicit type casting.. Hence this is teh issue.";non_debt
Probably worth a comment why guard `super.setup()`;non_debt
Capitalize ZooKeeper (here and elsewhere);non_debt
Fix graphbinary format description of Long;non_debt
@Ishiihara Sounds good. I'm just getting started with this code, but the current threading model seems reasonable to me. I see the Worker as just the task manager, which is primarily responsible for managing the task lifecycle. Currently this just means starting and stopping tasks, but it'll probably have to be extended to handle task failures. I assume it'll also need hooks for status tracking. Also, I like the tasks being single-threaded since it makes it easy to reason about their state.;non_debt
@oscerd do you mind rebase so we can test against quarkus 1.2.0 RC ?;non_debt
fixes #1411;non_debt
I think in some instances we need to recreate the schedule because the operations are reordered. I can update to not create a new schedule every time, and add a comment about incremental improvements.;non_debt
Can one of the admins verify this patch?;non_debt
okay, fixed!;non_debt
+1. Can you please squash. Thanks.;non_debt
you are right. added back the test with assertion.;non_debt
[FLINK-20909][table-planner-blink] Fix deduplicate mini-batch interval infer;non_debt
https://issues.apache.org/jira/browse/IGNITE-1681 is closed, closing the pr;non_debt
we could delete this.;non_debt
[FLINK-9672] Fail fatally if job submission fails on added JobGraph signal;non_debt
grpc proxy support;non_debt
"Co-authored-by: Sugandha Agrawal <sugandha.agrawal18@gmail.com>
This PR is related to https://github.com/apache/incubator-openwhisk/pull/4388. Seeing only a http response 400 Bad Request isn't enough to understand the root cause, but the detailed error and reason information which may exist in the error response will do.
This code change adds the error and reason info of the error response to the the HTTP status code.";non_debt
@rxin IIRC at one point we changed this before and it caused a performance regression for our perf suite so we reverted it. At the time I think we were running on smaller data sets though. Maybe in this case were are willing to take a hit?;non_debt
20675636-487 description-0;non_debt
"@Ben-Zvi looks like Oleg changed the code. Is it ok now? Could you please take a look.
@oleg-zinovev please do not forget to tag reviewers when you have made the changes, otherwise nobody will know that you have addressed code review comments.";non_debt
@ScrapCodes, would you mind triggering a build of this PR?;non_debt
Run Java PreCommit;non_debt
"add more tests for MarkerFiles,RollbackUtils, RollbackActionExecutor for markers and filelisting.
also fix the bugs for RollbackActionExecutor with markers mode or filelisting mode:
in HoodieWriteClient.java should not deleteMarkerDir, the rollback with markerfiles mode will failed
in ListingBasedRollbackHelper.java ""(path.toString().endsWith(HoodieFileFormat.HOODIE_LOG.getFileExtension()))"" can not check file is logfile
in ""MarkerBasedRollbackStrategy.java"" baseCommitTime should use FSUtils.getCommitTime(baseFilePathForAppend.getName())
This pull request is a trivial rework / code cleanup without any test coverage.
This change added tests and can be verified as follows:";non_debt
I like it, I'm going to leave this a for a bit and see if anyone has any comments overnight :);non_debt
[SPARK-18425][Structured Streaming][Tests] Test `CompactibleFileStreamLog` directly;non_debt
Jenkins, test this please.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/10267?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/10267?src=pr&el=continue).";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3286/;non_debt
Co-authored-by: Dave Barnes <dbarnes@pivotal.io>;non_debt
"https://issues.apache.org/jira/browse/HDDS-2411
and you need to set the title of the pull request which starts with
the corresponding JIRA issue number. (e.g. HDDS-XXXX. Fix a typo in YYY.)
-->";non_debt
Build Success with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/1629/;non_debt
Fixed.;non_debt
Can we merge this with the  `if` statement immediately above it? That way `scoreMode` can stay final;non_debt
"Yes, I tried to identify this case.
For this PR (or such PRs), the author looks still responsive and active so I do not disagree with re-opening personally because this was the point in https://github.com/apache/spark/pull/18017. Probably, I should have left a comment about this in each PR for clarification though.";non_debt
@yanboliang Thanks for the updates.  I responded above about a tiny fix.;non_debt
Thanks @felixcheung;non_debt
yea, that is what I said a no-op aggregate. I think it is okay to add it. Maybe I can add it in follow up.;non_debt
Can't get a PG to work.;non_debt
"You don't need to provide those messages_XX.properties if you don't translate them.
Only messages.properties and messages_fr.properties are mandatory.
Provide the additional ones that you translate";non_debt
Looks good, merging, thanks!;non_debt
Fixes #3540 by embedding the Trykker fonts as directed on this [Google webfonts helper app.](https://google-webfonts-helper.herokuapp.com/fonts/trykker?subsets=latin,latin-ext);non_debt
"Currently, we print auth data in info logs - roleToken, private key info etc.
Print ClientConfiguration without authentication field.
We will no longer print sensitive info.";non_debt
Maybe it should also be part of the `TableProvider` contract that if the table can't be located, it throws an exception?;non_debt
@karuturi @NuxRo @ustcweizhou could you please check [#962](https://github.com/apache/cloudstack/pull/962) for the details as #954 could then be closed and #962 could be merged if all LGTY ?;non_debt
"* Fixes issue with `pip` install failing when dependencies are duplicated across requirements files.
* Removes `restart: always` from tests worker as it crash-loops during ""normal"" operation. 
* Adds `Dockerfile-dev` which depends on `preset/superset:dev` which acts as a cache for local development";non_debt
I guess we do not need the tail </p> before in code formatting rule. Anything changed now?;non_debt
Needs rebase;non_debt
"# [Codecov](https://codecov.io/gh/apache/airflow/pull/6073?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/6073?src=pr&el=continue).";non_debt
CI report for commit 18b9bf24d58870dd64c6f5e511785967ef2b9452: FAILURE [Travis](https://travis-ci.org/flink-ci/flink-ci/builds/554871984?utm_source=github_status&utm_medium=notification);non_debt
What does `//do not want to change TOperationState in hive 1.2` means?;non_debt
@maolingÂ - Latest JLine jar available is `3.0.0.M1` which has major implementation changes. Have upgraded to second latest jar `2.14.6` to address this issue with minimal code changes. Kindly review.;non_debt
ditto;non_debt
Picking back...;non_debt
"One other advantage of System.nanoTime is that it is monotonically increasing. With regards to the performance cost, here's a thorough and reliable analysis on the cost of `System.nanoTime`:
http://shipilev.net/blog/2014/nanotrusting-nanotime/
I was unable to find an analysis of similar quality for `System.currentTimeMilllis`. We should definitely quantify the performance difference (particularly on Linux) before we choose one way or another.
Also, it would be good to do KAFKA-2247 before we do this.";non_debt
Pass compile and tests, please review, Thanks!;non_debt
Ping. PR has been ready for more than a month now.;non_debt
[Hotfix]Add suffix L to WatermarkGeneratorCodeGenTest#testAscendingWatermark long type fields;non_debt
run java8 tests;non_debt
I think you're right, I'll change it.;non_debt
"This looks good to me.
But it needs rebase";non_debt
@seelmann , no worry, thanks.;non_debt
Thanks!;non_debt
LGTM;non_debt
`regionsToVisit.addAll(regionConsumers.get(regionToRestart))`;non_debt
"â€¦umer]
XXXXX
XXXXX
XXXXX";non_debt
I added a session/system option in latest commit. In my opinion if a `LIMIT 0` query is taking longer than 10mins, we should try to fix it, otherwise it is not an interactive experience when using Drill with BI tools.;non_debt
log: initialize logging earlier;non_debt
Fixed Failure of Enhanced Kafka Interceptor to Acquire SkyWalking DynamicField;non_debt
There are 4 different options. Which one do you recommend ?;non_debt
Fixed -- lots were remnants from when we annotated indexes;non_debt
"1. updated the docs. The only place we don't do Head and dir marker is in create()
1. also added a test to verify that the empty set of probes skips all http requests
Now. can you create a Path with a trailing / ? I was about to say no, but remembered https://issues.apache.org/jira/browse/HADOOP-15430 .. one of the constructors of Path does let you get away with it, which is something which breaks S3Guard already";non_debt
Thanks for the review and merge @xiguantiaozhan. It LGTM now. @becketqin, can you please take a look as well?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/<h2>Failed Tests: <span class='status-failure'>1</span></h2><h3><a name='beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark' /><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/org.apache.beam$beam-runners-spark/testReport'>beam_PreCommit_Java_MavenInstall/org.apache.beam:beam-runners-spark</a>: <span class='status-failure'>1</span></h3><ul><li><a href='https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/8495/org.apache.beam$beam-runners-spark/testReport/org.apache.beam.runners.spark.translation.streaming/ResumeFromCheckpointStreamingTest/testWithResume/'><strong>org.apache.beam.runners.spark.translation.streaming.ResumeFromCheckpointStreamingTest.testWithResume</strong></a></li></ul>
--none--";non_debt
CAMEL-11959: Add new camel-yql component;non_debt
[SPARK-7567] [SQL] Migrating Parquet data source to FSBasedRelation;non_debt
@mxnet-label-bot update [pr-awaiting-review, cmake];non_debt
"Sorry, I mean we don't need the implicit casting, as it's not the same behavior in Hive.
Or are you trying to implicit casting?";non_debt
checkout an old commit, `make -f Makefile.docker run-python`, come back to master, see the test fails because the old libraries are installed in /opt/conda/lib...;non_debt
"That is untrue, because `shardingKey` could be either empty or ""/"". Adding delimiter first makes the logic easy to understand - after adding the delimiter, all logic applies to sharding keys with leading delimiters.";non_debt
It's two levels to match match existing indentation for callbacks specs. (Two levels for the response to break it up from one level indentation for when the function head is too long).;non_debt
This change seems unrelated and takes us out of sync with the batch version.  I don't think this means a JVM interface, but rather the `interface` in API.;non_debt
"Re-write of StarTree and StarTreeIndexNode.
Implemented new versions for star tree (StarTreeV2) and star tree node
size (1.1GB to 469MB). The feature is currently OFF by default, and
controlled via StarTreeBuilderConfig.
1. StarTreeV2 is a compact memory representation of StarTree with native
   serialization/de-serialization support. And can be loaded either in
   direct memory, or via memory mapped file. Used the Xerial LBuffer
   library to be able to support data sizes larger than 2GB.
2. Added custom serializer/de-serializer for StarTreeV2, as opposed to
   V1 that uses JAVA serialization/de-serialization.
3. Modified StarTreeIndexOperator to be able to work off of
   StarTreeInterf, which can be implemented either by StarTree or
   StarTreeV2.
4. Added a utility to convert a pinot segment with star tree v1 into a
   pinot segment with star tree v2.
5. Added unit tests to test:
   - Reading/Writing of StarTreeV2.
   - Query processing using StarTreeV2.";non_debt
isn't `initialize` already called when the factory is loaded? this is a double-initialize action.;non_debt
[SPARK-16973][SQL] remove the buffer offsets in ImperativeAggregate;non_debt
THRIFT-3637: Dart compact protocol;non_debt
retest this please;non_debt
Yeah sure go ahead and merge.;non_debt
should the server connection exposes SASL protocol directly? isn't ServerAuthenticationHandler a better place to manage the saslServer instance?;non_debt
makes sense, if the contents is covoured. I didn't give that any attention, @rafaelweingartner.;non_debt
[WIP][SPARK-34736][K8S][TESTS] Kubernetes and Minikube version upgrade for integration tests;non_debt
Allow streaming update for Python on Dataflow.;non_debt
Thanks @amaliujia for the review.;non_debt
"Load bookkeeper parameters from environment variables prefixed with BK_
Add the way to configure bookkeeper through environment variables to simplify deployment
Load bookkeeper parameters from environment variables with the prefix BK_ before load bk_server.conf
Master Issue: #2341";non_debt
Same here, is the logging diff required?;non_debt
Merged to master again;non_debt
Jenkinsfile for multibranch job;non_debt
ok;non_debt
"@john-bodley, I tried my best to add a test here, but wasn't able to get it to work due to an issue (maybe with FAB?). Here's my test code (also added in a commit to this PR):
And the error I saw:
It looks like when I'm trying to create an annotation inside the annotation layer, it's rendering a template instead of creating the new annotation. Maybe @dpgaspar has thoughts here?";non_debt
31006158-8940 review-603982014;non_debt
Excellent point.  I updated the test to use your round trip logic.  As far as empty strings, my original code would have eliminated them from the output, but since the test was written expecting them, I changed it to preserve the doc attribute when specified empty or not.;non_debt
@lwtdev merge into one line.;non_debt
"LGTM. Thank you.
Merging into 0.10.2 and trunk.";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2006/;non_debt
[SPARK-30973][SQL]ScriptTransformationExec should wait for the termination;non_debt
I was noodling over a way to dynamically enable the tracing logs I've been using while developing this. As metrics I think the cardinality would either be high enough it'd crush most metric collectors or coarse enough that it wouldn't be super helpful.;non_debt
@carlvine500 CI fails...;non_debt
"FAILURE
 6695 tests run, 1 skipped, 2 failed.
--none--";non_debt
Ah, I miss this one. Thanks @dongjoon-hyun;non_debt
17165658-21200 review-185268813;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1266/;non_debt
I see it in UTF8StringSuite, make sense.;non_debt
We need to be compatible with 2.x client I think. And also, we need to change a lot of tests if we want to change the behavior here. So I suggest that we do it in 4.0.0.;non_debt
Should be ApiKeys.INIT_PRODUCER_ID;non_debt
@xinyuiscool Please take a look.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/897/";non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2482/;non_debt
Created.;non_debt
"@zsxwing that's right, we will have to coordinate to make sure the Jenkins pyarrow is upgraded to version 0.8 as well.  I'm not sure the best way to coordinate all of this because this PR, jenkins upgrade, and Spark Netty upgrade all need to happen at the same time.
@holdenk @shaneknapp will one of you be able to work on the pyarrow upgrade for Jenkins sometime around next week?  (assuming Arrow 0.8 is released in the next day or so)";non_debt
"hello, Install Ambari 2.5 install HDP 2.6 , spark_client install error ""parent directory /usr/hdp/current/spark-client/conf doesn't exist"", copy other node `/etc/spark` to current node solve the problemã€‚
Detail Errors";non_debt
is this a placeholder?;non_debt
+1;non_debt
Let's not rename this here;non_debt
Thanks, fixed.;non_debt
I think it was part of out rollout plan.. we wanted to enable this feature in two phases: rollout broker first and later on enable at client-side.;non_debt
"Missing `.` after ""partitioned"".";non_debt
[FLINK-19135] Strip ExecutionException in (Stream)ExecutionEnvironment.execute();non_debt
use import to avoid full path;non_debt
also add autofunction to below(see the filed in topi.nn);non_debt
Updated;non_debt
@drcrallen file issues for new test failures you encounter;non_debt
"How does that eliminate the need for `distinct`?
e.g. take a look at the below:";non_debt
We want to make `close()` idempotent and not throw an exception but we will log a warning, but only for close so that is why these logs are not in the `setState()` method.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6845/<h2>Build result: FAILURE</span></h2>[...truncated 6598 lines...]	at hudson.remoting.UserRequest.perform(UserRequest.java:153)	at hudson.remoting.UserRequest.perform(UserRequest.java:50)	at hudson.remoting.Request$2.run(Request.java:332)	at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoFailureException: You have 1 Checkstyle violation.	at org.apache.maven.plugin.checkstyle.CheckstyleViolationCheckMojo.execute(CheckstyleViolationCheckMojo.java:588)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)	... 31 more2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.933 [ERROR] Re-run Maven using the -X switch to enable full debug logging.2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.933 [ERROR] For more information about the errors and possible solutions, please read the following articles:2017-01-27T02:45:40.933 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException2017-01-27T02:45:40.933 [ERROR] 2017-01-27T02:45:40.934 [ERROR] After correcting the problems, you can resume the build with the command2017-01-27T02:45:40.934 [ERROR]   mvn <goals> -rf :beam-sdks-java-io-google-cloud-platformchannel stoppedSetting status of 9f062e3350c7fa7df393d99bbf38ad3b3d83e8b8 to FAILURE with url https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/6845/ and message: 'Build finished. 'Using context: Jenkins: Maven clean install
--none--";non_debt
"[cloudstack-pull-analysis #309](https://builds.apache.org/job/cloudstack-pull-analysis/309/) SUCCESS
This pull request looks good";non_debt
Should this be a method of location? Btw, including the number of partitions would be nice too, since it gets truncated.;non_debt
"Yes that's true, but this will never be null right? Here we have a default app ID if the scheduler doesn't provide us with one.
Yes we'll have to update the relevant backward compatibility test in `JsonProtocolSuite` to make it take in a string rather than an option.";non_debt
The attributes reference by filter is also needed.;non_debt
@HyukjinKwon All the involved reviewers will get a ping. This is annoying to see many pings within one hour, right? My suggestion is to read the comments before triggering the test;non_debt
Merging now, thanks for your contribution.;non_debt
@aljoscha you merge since you asked first.;non_debt
That is not what you change does, though.;non_debt
"Build failed with checkstyle errors.
@vvcephei I also expected that the test is fixed with this PR instead of disabled. Not sure if we can get the information about user privileges.
What I am wondering thought is: why does a root user not respect read-only flag?";non_debt
PG2 492 blue before this morning's rebase.  Let's see what Travis has to say.;non_debt
This pull request adds support for [RocketMQ](http://github.com/alibaba/rocketmq), a Kafka-like messaging system as data ingesting source.;non_debt
Done;non_debt
I will be back after testing/looking into other databases tomorrow.;non_debt
"I have moved `mtime_` out of `TailState` as it is not really part of the state, and I'm using an ad-hoc `{TailState, mtime}` struct in the one place where it is needed (`findRotatedFiles`).
I have also renamed `timestamp_` to `last_read_time_`.";non_debt
rerun java8 tests;non_debt
How can we distinguish 0 partitions after pruning, and not being partition pruned?;non_debt
@1ambda Can you check the personalized mode as well?;non_debt
"Hi Alexis, 
I'm glad we have got that cleared up.
@osma has touched an important point - the integration with full ACID transactions.  That's probably a major part of the fact the merge doesn't work.";non_debt
THRIFT-3879 Undefined evaluation order;non_debt
Good catch!;non_debt
"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---";non_debt
@rxin Please help double check! Many thanks!!;non_debt
ðŸ‘   BUT would recommend to merge https://github.com/druid-io/druid/pull/3499 first.;non_debt
Looks good +1;non_debt
@RobberPhex  Any word on getting this merged?  Using the extension in PHP 7.0 is kinda broken otherwise... this is fixing a common issue that affected many PHP extensions moving to PHP 7.0.;non_debt
"For data sources without extending `SchemaRelationProvider`, we expect users to not specify schemas when they creating tables. If the schema is input from users, an exception is issued. 
Since Spark 2.1, for any data source, to avoid infer the schema every time, we store the schema in the metastore catalog. Thus, when reading a cataloged data source table, the schema could be read from metastore catalog. In this case, we also got an exception. For example, 
This PR is to fix the above issue. When building a data source, we introduce a flag `isSchemaFromUsers` to indicate whether the schema is really input from users. If true, we issue an exception. Otherwise, we will call the `createRelation` of `RelationProvider` to generate the `BaseRelation`, in which it contains the actual schema.
Added a few cases.";non_debt
LGTM;non_debt
52039373-1874 comment-280988481;non_debt
suggest to move internal util functions to the beginning.;non_debt
SerializableCoder#structuredValue returns the object itself;non_debt
@agrawaldevesh - updated.;non_debt
"Yes, in fact this is a ""fix"" so that a conda-installed version of pyarrow can be used in another project's setup.py (since `pyarrow.get_include()` will need to work both in pip and conda)";non_debt
Thanks @mumrah for taking a close look to these changes here as well. I replied to your two points, and if I didn't miss something, it seems that no code change is required.;non_debt
"Changes
Brings up a test bed that contains embedded kafka broker and zookeeper to test the following scenarios.
A) Rolling upgrade of stream processors.
B) Reelection of leader upon failures.
C) Registering multiple processors with same processor id.
D) Zookeeper failure before job model regeneration upon leader death should kill all running stream applications.
NOTE:
Some tests are commented out since zookeeper exceptions are swallowed in ZKJobCoordinator/ZKUtils.";non_debt
I thought it would be good to have the same Test. Any new create scenario will also have associated destroy command.;non_debt
Reports try finally statements which can use Automatic Resource Management of Java 7 or higher.;non_debt
"if user create view like:
`select k1,k2,k3,k4,k5 from tbl`
without order by clause and aggregation method, we should select first few columns as sort columns, not all columns.
Check this commit: `https://github.com/apache/incubator-doris/commit/bf31bd238b05eae4fb096533b14861827a139c61`";non_debt
Shouldn't this be couch_replicator_multidb_changes?;non_debt
+1 it sounds nice.;non_debt
i donot think so. because it is FIFO, we can remove head of request queue because head of queue is allocated by Yarn when we receive allocated Containers.;non_debt
@functioner , we could make 10 as the default pool size.;non_debt
Will we have other tags? If not, we can simply use tag;non_debt
"@jburwell  @remibergsma  Pls note all again that the changes to core are very minimal and are limited to convenience extensions only. I would expect running the CI should be good in verifying that the plugin changes don't break anything in core.
Thanks";non_debt
@piiswrong i'll add it these days.;non_debt
70746484-6230 comment-670308769;non_debt
"Merging this one as the tests are looking good and we have 3 LGTMs.
Thanks for the PR @wido, and for the reviews @rafaelweingartner and @dhlaluku.";non_debt
perhaps we should add a default case to handle other types which are not short, int or long.;non_debt
70746484-1047 description-0;non_debt
yes,no problem :) close this issue;non_debt
Ack.;non_debt
okay;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/29954/
Test PASSed.";non_debt
I don't think this should be a property of the data type. It's specific to the `OpenHashSet`. How about we add this method to `object OpenHashSet`?;non_debt
I wish we had a merge bot, which could detect a LGTM from a committer and do the need full.;non_debt
Does this class need to be `public`?;non_debt
Yes, it's what `DataFragment` is for. See `FileBasedDataFragment` -- it has a reference to an implementation of `FileFormat` which has the logic for scanning that kind of file;non_debt
"Hi, @gatorsmile .
Could you review this `CREATE TABLE ... LOCATION` document issue when you have sometime?";non_debt
"We should let Thrift Server take these two parameters as it is a daemon. And it is better to read driver-related configs as an app submited by spark-submit.
https://issues.apache.org/jira/browse/SPARK-7031";non_debt
@mgaido91 Thanks for fixing this.;non_debt
"@kkonstantine and @C0urante: thanks for the review. I think I've incorporated all of your feedback and addressed all of your questions. @C0urante, I've even tried to improve the failure message to say what needs to be done if the topic has an unacceptable `cleanup.policy`.
I'd appreciate another pass. Thanks!";non_debt
@Attsun1031 Can you close this?;non_debt
@jorisvandenbossche PTAL;non_debt
17165658-9741 comment-157553204;non_debt
"@markusweimer  @jwang98052 
This is the pull request for creating IPartitionedOutput dataset. Please review.";non_debt
"Get the ""query"" option from configuration not options which is from command line only. But when running the test by test suite(like --suite=DEFAULT), there is no query option from the command line. Thus we should always get the ""query"" from the ""configuration"" which covers both the command line and test suite.";non_debt
I don't see a lot of value in adding another interface for the sole purposes of differentiating between the 2 current implementations.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/1629?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-dolphinscheduler/pull/1629?src=pr&el=continue).";non_debt
i think `||` will be better, if `!(Objects.equals(query.fetch, target.fetch)` is true, it will return null,  not need to consider second condition.;non_debt
Alright merging into master and 1.3.;non_debt
"Why is one branch of the code checking for anyNull, and the other allNull ?
because in LongHashedRelation
kevEv.value it a java long type, it certainly could not call allNull
as in UnsafeHashedRelation
kevEv.value is a UnsafeRow, this is why separately branch needed.
anyway, I will drop these code to remain ""single column"" focussed";non_debt
See kafka consumer config;non_debt
No, I insist it be there :-);non_debt
"The JVM keeps a cache of URL connections, which keeps
the underlying files open for the lifetime of the VM
unless the `setUseCaches(false)` method is used.
This change makes it possible to reuse the Groovy compiler,
e.g. embedded in the Gradle daemon without preventing files
from being deleted on Windows.";non_debt
"I think that we don't need to care about AppVeyor because:
  * AppVeyor has few jobs now
  * AppVeyor cancels pending jobs when we push new commits to pull request";non_debt
Thanks. Fixed now.;non_debt
He @trotterdylan please review https://thrift.apache.org/docs/HowToContribute as we need a Jira ticket for this change.;non_debt
We will support histogram for numeric types, but the logics in that agg function will be very different from this. You can refer to discussion on the [jira](https://issues.apache.org/jira/browse/SPARK-17074).;non_debt
"I have deployed this in a development cluster, and inconsistently see errors around the dreaded ""IllegalStateException: unread block data"", as well as some issues where my custom deserialization code in classes is appearing to receive incomplete blocks. I didn't see it for every topology, several went along fine, even after several redeploys. But occasionally I would hit deserialization problems. It's only with the gzip implementation. I have configured the DefaultSerializationDelegate and not seen any issues since.
@revans2 I believe you said you've been running this at your place for a while now (before the pull request), did you have any similar experiences?";non_debt
NIFI-1037 Ported processor for HDFS' inotify events to 0.x.;non_debt
thanks, merging to master!;non_debt
ðŸ‘;non_debt
WXScrollerComponent scrollViewDidScroll, Collection NSHashTable was mutated while being enumerated;non_debt
28738447-1063 description-0;non_debt
GG-11133 TTL should be tracked in off-heap page structures;non_debt
probably need to wrap e inside a SamzaException and rethrow here;non_debt
"Call for review @abbccdda @guozhangwang 
System test run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3856/";non_debt
Check the e2e, whether this change breaks test. We test e2e w/ zookeeper;non_debt
I can enable it by checking MSVC version seems was due to the introduction of advanced template here https://github.com/dmlc/nnvm/blob/master/include/nnvm/tuple.h#L615;non_debt
They weren't before this PR don't see a need to expand the API surface as part of this refactoring.;non_debt
"BarrierCoordinator uses Timer and TimerTask. `TimerTask#cancel()` is invoked in ContextBarrierState#cancelTimerTask but `Timer#purge()` is never invoked.
Once a TimerTask is scheduled, the reference to it is not released until `Timer#purge()` is invoked even though `TimerTask#cancel()` is invoked.
I checked the number of instances related to the TimerTask using jmap.";non_debt
ah i see what you mean, sounds good.;non_debt
retest this please;non_debt
FreeBSD build _successful_! See https://ci.trafficserver.apache.org/job/Github-FreeBSD/538/ for details.;non_debt
`[@code` -> `{@code`;non_debt
the name is hidden inside BaseTable, yeah, we could use `Table.toString()`  to get that `name`.;non_debt
"Make offsets immutable to users of RecordCollector.offsets. Fix up an
existing case where offsets could be modified in this way. Add a simple
test to verify offsets cannot be changed externally.
*More detailed description of your change,
if necessary. The PR title and PR message become
the squashed commit message, so use a separate
comment to ping reviewers.*
*Summary of testing strategy (including rationale)
for the feature or bug fix. Unit and/or integration
tests are expected for any behaviour change and
system tests should be considered for larger changes.*";non_debt
"@DaanHoogland I don' known the best way to explain the transifex process.
For all versions of CloudStack, we have a version of resources files on Transifex [1].
When you update the main resource file with the new version, the transifex config file is updated with all languages with already translated.
I don't know the best way to explain the internal behavior of the transifex client, but the config file is update after the first download of the main resource file (en_US) and the upload of the current L10N files to the new L10N files for the new versions. This PR reflect the changes.
You can retry the behavior from the master (up to date 2015/11/21) with theses commands:
To fetch the latest original L10N file form Transifex (already done because I'm put the 4.7 main resource files on the TX website today)
cd tools/transifex/
 ./sync-transifex-ui.sh download-source-language CloudStack_UI.47xmessagesproperties
To upload all L10N translations files (from 4.6 files) to transifex (already done by me today, but you can re-made the work during few days before the translator team changes the strings on Transifex)
./sync-transifex-ui.sh upload-l10n-languages CloudStack_UI.47xmessagesproperties
[1]https://www.transifex.com/ke4qqq/CloudStack_UI/content/";non_debt
"     with your text. If a section needs no action - remove it.
     Also remember, that CouchDB uses the Review-Then-Commit (RTC) model
     of code collaboration. Positive feedback is represented +1 from committers
     and negative is a -1. The -1 also means veto, and needs to be addressed
     to proceed. Once there are no objections, the PR can be merged by a
     CouchDB committer.
     See: http://couchdb.apache.org/bylaws.html#decisions for more info. -->
This change should allow users to supply all params in [POST](http://docs.couchdb.org/en/stable/api/ddoc/views.html#post--db-_design-ddoc-_view-view) that can be supplied for [GET](http://docs.couchdb.org/en/stable/api/ddoc/views.html#get--db-_design-ddoc-_view-view) now. This way we could avoid the `?key=""foo""` things that would probably cause a lot of pain for users.
     what problem it solves or how it makes things better. -->
So far this has been tested manually and it seems to be working.
     Does it provides any behaviour that the end users
     could notice? -->
     repositories please put links to those issues or pull requests here.  -->";non_debt
"A suggested improvement is to add a Canary Test tool to the Phoenix Query Server. It will execute a set of Basic Tests (CRUD) against a PQS end-point and report on the proper functioning and testing results.
@joshelser It would be great if you could review this.
@vincentpoon @karanmehta93";non_debt
@siddharthteotia Addressed all the comments;non_debt
Most the properties are being set in accumulo-env.sh.  Some of the log forwarding properties are set in Java but that should change with your new appender.;non_debt
"Thanks a lot for addressing the comments.
I'll push the changes to travis, once its green I'll merge them.";non_debt
"fix #2619: is there a problem in NettyBackedChannelBuffer.setBytes(...)?
XXXXX
XXXXX";non_debt
356066-2458 description-0;non_debt
For endRow, null represents max.   For prevEndRow, null represents minimum.;non_debt
@fjy the console auto renders based on your query as it does with all of them. If you set `resultAsArray: true` in the query context of a grouBy (this is a new option) the query response will have array rows. With this PR the console will be able to auto render it. Before this PR it would give you an error in the table.;non_debt
Packaging result: âœ”centos6 âœ”centos7 âœ”debian. JID-2389;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/543/;non_debt
I think the list is sorted in alphabet order. so you have to add them to be after `data-generate` and before `elastic-search`.;non_debt
[NETBEANS-5161] Prevent IAE when resolving composite project dependencies;non_debt
As per subject;non_debt
"I can not really assess what this change is doing in detail, meaning how it affects typical workloads. Would need a bit more context.
The change looks small an innocent and seems to have decent tests -)";non_debt
Ignite-gg-12751;non_debt
34864402-4405 description-0;non_debt
Did you mean a temporal object could exist without a catalog ? For example, the view ?;non_debt
"Thank you, @viirya , @HyukjinKwon , @mridulm .
Merged to master.";non_debt
@dstandish this approach seems acceptable to me :+1:;non_debt
Build Failed  with Spark 2.3.2, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1077/;non_debt
hitCounter should not be assigned from the return of setDictionaryTermMatch.  It should simply be incremented prior to the call.;non_debt
why are we doing + 1?;non_debt
Fix STORM-2017;non_debt
@karuturi the interfaces are in 'tls' sub-package, but the impl are still in 'ssl'?;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/madlib-pr-build/450/";non_debt
"JAXB has no word about JAX-RS so don't think so as well, it is a CXF specific feature AFAIK
will close this one to open another PR (using my IDE instead of github online edit since it needs more work than I thought)
thanks guys for the feedback";non_debt
I think so, will re-purpose this PR to fix the thead-safy issue about UnsafeProjection. (revert the changes about broadcast);non_debt
Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/2673/;non_debt
@abbccdda @guozhangwang Sounds good to me to resolve this problem separately. I'll do another pass on the PR today and hopefully we can merge this week.;non_debt
"Hey @RyanSkraba ,
Really appreciate the help on this.  It's quite annoying.
I just pulled down the latest changes to the master branch and did ""Import [Maven] Project"" and when it's doing it's initial build, I get this error message.  Nothing special to it.  May be a Eclipse version issue. 
if you add both a `<version>` and `<versionRange>` tag, does that build?  That may be the resolution to make all versions happy.";non_debt
"Why not using assert?
Do we expect this test suite will be skipped if this condition is not true?";non_debt
Maybe `Controlling downstream updates from a KTable` or something similar, just a thought.;non_debt
"cc @aarondav @kayousterhout @pwendell 
This should go into 1.2?";non_debt
ðŸ˜‚ðŸ˜‚ðŸ˜‚ Completely forgot.;non_debt
"Related to [ARROW-6206](https://issues.apache.org/jira/browse/ARROW-6206).
Specifically, ""-Dio.netty.tryReflectionSetAccessible=true"" for JVMs >= 9 and BoundsChecking/NullChecking for get.";non_debt
Reopen this pull request to trigger a ci build.;non_debt
@DaanHoogland Done;non_debt
20587599-12556 review-438019081;non_debt
run java precommit;non_debt
[CALCITE-92][CALCITE-486] Optimize away Project that merely renames fields;non_debt
ARROW-1554: [Python] Update Sphinx install page to note that VC14 runtime may need to be installed on Windows;non_debt
ZOOKEEPER-3410:./zkTxnLogToolkit.sh will throw the NPE and stop the process of formatting txn logs due to the data's content is null;non_debt
HDDS-1870. ConcurrentModification at PrometheusMetricsSink;non_debt
@marmbrus Any more comment on this before merging? It will be great appreciated if you merge this soon, as I did take lots of time in rebase again and again. :);non_debt
I added branches that expect requests for MIN and MAX aggregation accumulators.;non_debt
LGTM. Thank you Ahmet!;non_debt
Build Success with Spark 1.6, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/354/;non_debt
Oh, I know why! The CI queue was full (my test runs were taking hours to go through). By the time the runner got to my PR (Wed, 25 Nov 2020 02:15:47 GMT), my final commit with the manual cmake autoformat had gone in about an hour before.;non_debt
retest this please;non_debt
:+1:;non_debt
will merge this if there are no more discussions;non_debt
FINERACT-1133 [Backport PR] Added Mustache templates;non_debt
Adding regex predicate support;non_debt
"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25620/
Test FAILed.";non_debt
Run python tests;non_debt
"it turns out `$0` is not reliable (http://stackoverflow.com/questions/59895/getting-the-source-directory-of-a-bash-script-from-within).
Updated and tested on Ubuntu and Mac, with relative path, full path etc.";non_debt
These maps are used to initialize some static class members in PutS3Objects. If `static` is removed, because of the order of the initialization those members will not be initialized.;non_debt
17165658-17255 comment-285846909;non_debt
"Honestly I would use _aux myself. Allen said somebody hates _ so i made it ts_aux. honestly it does not matter it just cannot be prn, aux, or con
It is useful for ""generated"" items to be in directories that are easy to find and delete outside some tool being used.";non_debt
note this is not always honored: if server side has this configured then the client side one will be ignored;non_debt
LGTM after fixing the import orders.;non_debt
Yeah, your suggestion can work well, but I'm more prefer to my way, since it's more clear for developer to understand what happened.;non_debt
Hi @siju-samuel Thanks for the effort. Can you please give a usecase where we use quantized division? is there any quantized division operator?;non_debt
I don't think it is enough to go always with the cast path, since it allows many format/strings, not allowed by the parse method. Thus I think it not safe to avoid the parse method.;non_debt
Move `Commons.comparator(..)` code into `ExpressionFactory.comparator(..)` method;non_debt
clang format *failed*! https://ci.trafficserver.apache.org/job/clang-format-github/305/;non_debt
Run Python PostCommit;non_debt
HBASE-22808 HBCK Report showed the offline regions which belong to diâ€¦;non_debt
"Largemessage can be concurrent accessed and used. In case of topic with multiple queues. 
Remember how we had to fix loads of concurrent access issue on core message the other year";non_debt
nice catch. thank you so much.;non_debt
"LGTM
@chenliang613 kindly reivew";non_debt
Done :);non_debt
[TE] Self-Serve tuning flow 04: Alert page overview;non_debt
Add checks for definition of signing algorithm field;non_debt
"https://issues.apache.org/jira/browse/TINKERPOP-1297
Also Improved output of HTTP request errors to gephi. It is likely that this change will not allow Gephi to work with 0.8.x anymore.
Tested manually and ran tests with: 
VOTE +1";non_debt
"Changes to broadcast_to, broadcast_axis, linearregressionoutput, logisticregressionoutput, maeregressionoutput function documentation.
@mli, @zackchase, @nswamy @madjam";non_debt
"The existing SArg application only happens on File, Stripe and RG statistics so even in the worst case it will RowIndexStride times better than what we see with filter.
With the filter the evaluation happens on every row.";non_debt
Not anymore;non_debt
This is something you'd need to discuss on dev. Currently between MSVC, Clang and gcc, some things generate errors and some don't.;non_debt
@becketqin : Interesting, in theory, using more than 1 thread should still help since those threads can drive the I/Os on different disks in parallel.;non_debt
[Enhance] Show brokers' hostname;non_debt
Done;non_debt
New line after line 48.;non_debt
Jenkins test this please;non_debt
I don't think it matters much, but the caller may be in a different place from where the object was instantiated.;non_debt
comma here should be a ;non_debt
I apply your suggestion.  I will introduce similar changes in other places in the project as seperate PR. I add your suggestion to the list of my tasks.;non_debt
changes log;non_debt
You're not changing anything here now, are you?;non_debt
cc @cloud-fan;non_debt
LGTM - merging to master. Thanks!;non_debt
alignment;non_debt
@potiuk Don't be sorry, I rebase and squash commit into one commit. Waiting for CI.;non_debt
I think the feature request has been denied, so we can close this PR;non_debt
Bugfix xml schema completion (Fix catalog handling and tests);non_debt
This looks incorrect assignment.  V4 is assigned to V3 ?;non_debt
good idea;non_debt
Everything LGTM. Thanks again @kamilwu Merging :);non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/26921/
Test PASSed.";non_debt
I see, this is the fix then.;non_debt
Are some of these fields supposed to be `final` (existing code, I know)?;non_debt
"I see, thanks, did not know about DoFn possibly having setup/teardown, just updated... I'm planning to have another PR at the next stage which will address all of the Configuration related improvements (custom content handlers, etc, and now including the possibility of passing the XML configuration fragment as you suggested). 
Re the shortcut and ParseResult success/failure, I've np with continuing looking into it in this PR, but may be it will be easier, esp for the reviewers, to merge what is already available, this IO is still Experimental so I guess it will be safe enough, but it's up to the team";non_debt
ping @mjsax and @vvcephei for review;non_debt
The maximum QUIC packet size depends PMTU. So, it can be the same as the maximum payload size of UDP.;non_debt
356066-522 description-0;non_debt
ditto;non_debt
"Fixes #833
In this PR, the routing table provider has been changed in a way
to include customized view feature.
TestRoutingTableProvider.testExternalViewWithType
TestRoutingTableProvider.testCustomizedViewWithoutType
TestRoutingTableProvider.testCustomizedViewCorrectConstructor
TestRoutingTableProvider.testGetRoutingTableSnapshot
TestRoutingTableProvider.testSnapshotContents
[INFO] Tests run: 914, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3,559.344 s - in TestSuite
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 914, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  59:24 min
[INFO] Finished at: 2020-02-27T16:27:09-08:00
[INFO] ------------------------------------------------------------------------";non_debt
"A separate PR https://github.com/apache/spark/pull/27952 was opened to focusing on HiveClient related code. 
For the fallback solution of Hive 2.2- versions that @cloud-fan suggested, I plan to handle it in `HiveExternalCatalog.listViews` rather than `HiveClient.listViews`. So will update here in this PR later. Thanks!
@maropu @dongjoon-hyun @cloud-fan";non_debt
Perfect, working on it now;non_debt
Merged build started.;non_debt
Pad value is not provided when BucketSentenceIter.next() returns DataBatch, and this may broke module.predict(). Since BucketSentenceIter will discard a few data insufficient for a mini-batch at the end of each bucket, so set pad = 0 seems okay.;non_debt
Thanks @GlenGeng for the offer.  Is it repeatable (ie. after the test can you restore prior state), or do we have only one shot?;non_debt
This demonstrates the use of Travis-CI to run tests.;non_debt
Test added.;non_debt
I am using Hadoop's GlobPattern instead of java.util.regex, because I could not find any API in java.util.regex which tells if the string is a plain string or contains special characters. Do you know any API in java.util.regex? or should I put this reason in comment?;non_debt
cc @uce and @zentol;non_debt
Initial commit of the ASYNC_JOBS was not correctly initializing the ASYNC_JOB features.  Need to read some configs before attempting to initialize.;non_debt
I mean creating a variable `_allHTTPMethods`, which is:;non_debt
Microsoft recommends not to refer system table like change_tables instead use sys.sp_cdc_help_change_data_capture. https://docs.microsoft.com/en-us/sql/relational-databases/system-tables/cdc-change-tables-transact-sql;non_debt
Fixed.;non_debt
merged to master;non_debt
"Ok sounds good. I gave the Integration Test thing (and the QueryCassandra -- actually in AbstractCassandraProcessor) fix a try, the ITs don't work (yet) but feel free to take a look: https://github.com/mattyb149/nifi/tree/cassandra_time
I'll write up the Jira for QueryCassandra, and if I get a chance to test the fix I will submit the PR myself and you can review if you like :)
Will do a final look-around then merge, thanks again!";non_debt
[docs] Set USE_LLVM OFF when build VTA on pynq board;non_debt
We need to handle the case for distance > 0, and not return the same point;non_debt
rerun cpp tests;non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12444/job/ApacheCarbonPRBuilder2.3/5543/;non_debt
@FloChehab Can you look at it?;non_debt
"`legendDesc`
`legendAsc`
`valueDesc`
`valueAsc`";non_debt
Don't submit yet, there are some breaking internal tests;non_debt
@shwstppr possible to optimise multiple checks for _getHostId()_ and _getStorageId()_ not null here ?;non_debt
"    âš ï¸ Please make sure to read this template first, pull requests that don't accord with this template
    maybe closed without notice.
    Texts surrounded by `<` and `>` are meant to be replaced by you, e.g. <framework name>, <issue number>.
    Put an `x` in the `[ ]` to mark the item as CHECKED. `[x]`
-->
     ==== ðŸ› Remove this line WHEN AND ONLY WHEN you're fixing a bug, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ”Œ Remove this line WHEN AND ONLY WHEN you're adding a new plugin, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ“ˆ Remove this line WHEN AND ONLY WHEN you're improving the performance, follow the checklist ðŸ‘† ==== -->
     ==== ðŸ†• Remove this line WHEN AND ONLY WHEN you're adding a new feature, follow the checklist ðŸ‘† ==== -->";non_debt
"It's not good to have all the channels start with ""On the ...""
Better to change the prompt above to 
""Converse with the MXNet community via the following channels: ""
 dev@mxnet.apache.org.
Note those email addresses need to be mailto links (I believe Markdown (on GitHub at least) converts automatically), not code snippets.";non_debt
Looks good to me! Thanks!;non_debt
"I will combine function call.
union_block and merge_block will be remained.";non_debt
in cluster mode, AMActor donot need to subscribe to disassociated event. because sometime driver has some errors, Now AMActor donot understand what happened in driver. so if AMActor subscribe to disassociated event and finish with FinalApplicationStatus.SUCCEEDED, that's incorrect to do so. @andrewor14;non_debt
356066-4526 description-0;non_debt
"This seems to have the same problems as before. If this line fails, `current.jst` will either be a partially copied file (if line 247 is not executed, e.g., SLA kill or OOM) or gone.
There doesn't seem to be an easy way to guarantee that if the new `current.jst` isn't successfully generated, the previous `current.jst` must be restored. So I still think modifying `FsDatasetStateStore.getLatestDatasetState` is a better approach. I don't think it violates any contract, because `current.jst` is just a convenient way to find the latest `jst` file (so that we don't need to list all `jst` files and sort them). If somehow `current.jst` failed to be generated, I don't see a problem of using the latest `jst` file.";non_debt
Rebased, AutoValue use, etc. Not fully ready for review anyway.;non_debt
@pwendell can you help take a look?;non_debt
Done.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2176/
Test PASSed (JDK 7 and Scala 2.10).";non_debt
Can the type be more specific than `any`?;non_debt
Add pointer helpers for missing types;non_debt
sure;non_debt
"@kishorvpatil @tgravescs It seems this pr is breaking functionalities of `--files` or `--archives`.
Using `--files` or `--archives` with files which are not included to `--jars` doesn't work.";non_debt
Can you use [get_docs_link](https://github.com/apache/airflow/blob/master/airflow/utils/docs.py) function to generate link to fixed version?;non_debt
how about adding an example like;non_debt
document all public fucntions;non_debt
"Thanks, LGTM.
R: @chamikaramj for merge.";non_debt
Add a .devcontainer configuration for 3.x;non_debt
Add a `<p/>`? Otherwise it will be reformatted by others easily.;non_debt
LGTM;non_debt
@jgao54 would it be possible for you to run this in Py2?;non_debt
Schema should not be mandatory;non_debt
"Fix issue #5052. When generic invokes, the beginning `invocation` object (before any filters) will be used to be a callback method's parameter, such as like `onResponse`, `onError` of interface `org.apache.dubbo.rpc.Filter.Listener`. However, `invocation`'s function `getMethodName` will return value:`$invoke` when it is a generic invoke.
Thus, it will cause `endCount` method not counting the origin method status. Then the `executes` limit is not accurate enough.";non_debt
Run Java PreCommit;non_debt
Website docs for 2.3.1;non_debt
"@tgravescs 
What about  `SignalUtils.scala`
`log.error(""RECEIVED SIGNAL "" + sig)`
when we kill the app using yarn kill we get this:
_ERROR ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM_
can we use it to trigger cleanup?";non_debt
Fwiw, it's important to point out the the intent of this plugin is to provide some statistics for future analysis, our known use case is to produce an update to the set of WKS for 7.0.0.;non_debt
@omkreddy Sorry, we should update `currentConfig` in `DynamicBrokerConfig.initialize`. Thanks for raising the JIRA. While it would have been good to have this in 1.1, I am thinking we don't need to make this a blocker since it only impacts the first time a dynamic config is added. We will need a restart the first time and thereafter it should work as expected. We could highlight this in the docs for 1.1. What do you think?;non_debt
Thanks all, it should be fixed in master only, my mistake.;non_debt
I don't know why we pick -20000 and 20000 as the boundaries, just to be safe to always test the boundary values.;non_debt
Done;non_debt
  https://issues.apache.org/jira/browse/THRIFT-556;non_debt
Fix couch server race condition;non_debt
"This PR for ticket SPARK-13019 is based on previous PR(https://github.com/apache/spark/pull/11108).
Since PR(https://github.com/apache/spark/pull/11108) is breaking scala-2.10 build, more work is needed to fix build errors.
What I did new in this PR is adding keyword argument for 'fractions':
`val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)`
`val exactSample = data.sampleByKeyExact(withReplacement = false, fractions = fractions)`
I reopened ticket on JIRA but sorry I don't know how to reopen a GitHub pull request, so I just submitting a new pull request.
Manual build testing on local machine, build based on scala-2.10.";non_debt
Actually isRegionOnline() is waiting in a loop until this region's status become opened and that server is online.  So there is no bug as such right?  isRegionOnline() might return false iff the server is being stopped;non_debt
what is BatchCompatibleStreamTableSink?;non_debt
@dlg99 any ideas on my comment?;non_debt
Can one of the admins verify this patch?;non_debt
"LGTM
After https://github.com/apache/skywalking/pull/4214 about es index number reduced. This is another optimization.";non_debt
 STORM-586: TridentKafkaEmitter should catch updateOffsetException.;non_debt
ISSUE 7415 fix sidebar v2.5.0;non_debt
Normalize `FunctionIdentifier` when looking up it too?;non_debt
Is setting unfinished here needed?;non_debt
STORM-1200. Support collations of primary keys.;non_debt
Sorry. I still do not understand the reason of having this. What's wrong of having DROP VIEW in SQLContext?;non_debt
@mxnet-bot run ci [unix-cpu, centos-gpu];non_debt
"I understand the issue you're pointing out, but it hasn't been a practical problem, even with hundreds of thousands of terms. The inclusion of explicit zeros in the output SparseVectors has been a practical problem, which is what led me to submit this JIRA and PR.
This is my first contribution to Spark, and I'm trying to adhere to the contribution guidelines. The guidelines suggest that ""simple, targeted"" changes are more likely to be accepted than ""big bang"" changes. It sounds like you're telling me this PR won't be accepted without making an additional optimization which adds ~100 lines of code, requires additional tests, and fixes an issue that was already present in the code before my changes. Is that what you're saying?";non_debt
AWESOME!;non_debt
"Can you add ""[SPARK-1495][SQL]"" to the PR title?";non_debt
Totally agree. Would be nice if we the template changes depending on the files that changed.;non_debt
It incorrectly converts strings to numbers. @pcadabam can you review ?;non_debt
"This is the change we discussed some time ago and captured in #12261. The Production image during CI build is now built from wheels rather than directly from sources to reflect a ""real"" installation case. 
This change modifies the ""CI"" production image build to first:
1) Using 'pip download"" + constraints file it downloads all .wheel packages that are needed to install airflow with the chosen ""extras"" for the production image
2) Buillds all providers packages that are selected for the production
3) Builds airflow .whl package
4) Prepares the production image using those wheel files rather than PyPI.
This way this production image for development tests reflects the exact production ""content"" - all packages (including airflow) are installed, but at the same time we are using latest sources to build those packages, so the image can also be used in K8S tests because it is build from the current (PR) sources. 
After preparing the image I am also checking if all the providers are installed as expected";non_debt
Set http level to INFO as default;non_debt
[FLINK-7350] [travis] Only execute japicmp in misc profile;non_debt
unrelated change;non_debt
CXF-7354:Â Use SLF4J markers to differenciate payload-logging;non_debt
"Since both @shivaram and @felixcheung signed this off, I'm merging this to master and branch-2.0.
Thanks @keypointt for working on this and @shivaram and @felixcheung for the review!";non_debt
@merlimat addressed comments.;non_debt
"ok, I added the requested test for bw compatibility
btw you can see the buggy behavior there (lost information whether baseFilename is file/directory)";non_debt
SAMZA-2004: Add ability to disable table metrics;non_debt
"[Spark-12485][Rename ""dynamic allocation"" to ""elastic scaling""]";non_debt
Out of curiosity, why is it better to handle the fix here than in the `AnnotateTarget` pass? It seems that it would work if `AnnotateTarget` just didn't generate the extra `compiler_end` annotations.;non_debt
Naming fun: shorten to `register` and all the overloads can be the same, though you could have `registerCoderFactory` and `registerCoder` separate, too.;non_debt
Build Success with Spark 1.6.2, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder/1690/;non_debt
fixed;non_debt
Port #14591 to release-1.12;non_debt
This looks like a pure formatting change.;non_debt
"https://issues.apache.org/jira/browse/TINKERPOP-2023
Gremlin Server no longer supports automatically creating self-signed certificates.
Cluster client no longer trusts all certs by default as this is an insecure configuration. (TINKERPOP-2022)
To revert to the previous behavior and accept all certs, it must be explicitly configured.
Introduces JKS and PKCS12 support. JKS is the legacy Java Key Store. PKCS12 has better cross-platform support and is gaining in adoption. Be aware that JKS is the default on Java 8.  Java 9 and higher use PKCS12 as the default. Both Java keytool and OpenSSL tools can create, read, update PKCS12 files.
Other new features include specifying SSL protocols and cipher suites.
The packaged `*-secure.yaml` files now restrict the protocol to `TLSv1.2` by default.
The implication of all of the above changes means that the packaged `*-secure.yaml` files no longer ""just work"". Minimally, the server files must be configured with a key/cert.
PEM-based configurations are deprecated, to be removed in a future release.
`mvn clean install -DskipIntegrationTests=false  -pl :gremlin-server` passes all tests
VOTE +1";non_debt
I think here can add SchemaType.JSON;non_debt
"Looks You guys updated dockerfile to use 1.3.0 release... but it's not available yet at :
https://archive.apache.org/dist/nifi/";non_debt
gru;non_debt
This PR also changed `__repr__`. Thus, we need to update the PR title and description. A better PR title should be like `Implement eager evaluation for DataFrame APIs in PySpark`;non_debt
yes they should be nullable;non_debt
[MINOR] Fixes bug causing stats output to be cleared in JMLC;non_debt
[FLINK-2232] StormWordCountLocalITCase fails;non_debt
Feature/geode 2113c - implement SSL over NIO for peer-to-peer communication;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4335/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
Yes;non_debt
[not that important] I'm surprised this doesn't cause an `eslint` error, was it complaining about not being a boolean? `!!this.props.database && this.props.database.allow_run_async` or `Boolean(this.props.database) && this.props.database.allow_run_async` could be alternatives.;non_debt
"+1
Thanks @jihoonson and I found that it worked successfully. 
Ship it. :)";non_debt
Looks like some compilers is unhappy with this initialization :-/;non_debt
Fix LANG-948;non_debt
"OK, again, here we unwrap the optional by checking whether it is null and inside the method we checl it again...
Where does this Optional come from? If we could not change the root, let's keep it as is?";non_debt
I think you missed this one.;non_debt
It wasn't a complete push. I updated the commit. I think you are right about using isNullOrEmpty.;non_debt
@samskalicky its already merged in v1.x;non_debt
Forward-port #19972 to master.;non_debt
Good catch :-);non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Thanks all!!;non_debt
cc @hvanhovell;non_debt
Can one of the admins verify this patch?;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25472/
Test PASSed.";non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/536/;non_debt
[USERGRID-613] Added fix mixed cases in two-dot-o.;non_debt
@tqchen Done.;non_debt
Run Load Tests Python Combine Flink Streaming;non_debt
cc @dongjoon-hyun;non_debt
More goddamn merge conflicts.  Any objections to commit-then-review on this one?;non_debt
NIFI-5456: AWS clients now work with private link endpoints (VPC);non_debt
sure, I'll change it;non_debt
"Move this as the second one. 
The first line of this page includes 6 too, https://github.com/apache/skywalking/blob/master/docs/README.md";non_debt
@flinkbot  run travis;non_debt
"Thanks @trxcllnt. It will take me a day or two to work my way through the diff and give comments. If @TheNeuralBit could also chime in about the direction for taking the JS codebase that would be great. 
We should have a mailing list discussion about creating a JS roadmap / laundry list of JIRAs so that we can work toward integration tests and other proof of compliance with the Arrow specification (e.g. having binary data files here is OK, but the ideal scenario would be to have an N x N integration test matrix, where N is the number of different Arrow implementations). Currently we only have a 2 x 2 matrix where C++ and Java test against each other (and with themselves)";non_debt
@dudaerich Thanks Erich.  This looks good.  I will merge.;non_debt
"How about detecting `GType` from `arrow::fs::FileSystem::type_name()` like we did `garrow_array_new_raw()`?
It's useful when we create a binding for a function that returns a generic `arrow::fs::FileSystem` such as `arrow::fs::FileSystemFromUri()`.";non_debt
@blueorangutan test;non_debt
Just as `grantingVoters()` and `rejectingVoters()`, `unrecordedVoters()` method be optimezed by using `votersInState(State)`;non_debt
It may be useful to set up SegmentIndexCreationDriverImpl (via config) so that it can log a message every (say N) rows. I understand this is not something you did, but maybe it is useful to add that information. You decide.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4782/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
"Testing s3a ireland
-a full run of everything (kicking off another) with s3guard and ddb 
-this test suite with s3guard off, on and local. Verifying that without s3guard, the guarded versions of the tests are not executed";non_debt
+1 by humbedooh in hipchat;non_debt
s/uint/size_t/;non_debt
"If line 277 crashes, the file will still be open.
         FileWriter x = null
         try{
                x= new FileWriter
          }finally{
                if(x != null) x.close()
          }";non_debt
"R: @rohdesamuel 
CC: @melap";non_debt
"Hi @aljoscha 
I have rebased master and addressed comments.";non_debt
+1 good catch.;non_debt
"added SELU and ELU activation functions as recommended on this thread https://github.com/apache/incubator-mxnet/issues/8422
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
"Choose one
In some cases, a result existed in the results backend but a query with the corresponding `results_key` did not exist in the queries table. This used to throw a 500 because `one()` expected a query with that `results_key` to exist.
Now, it will return a generic error if no query exists. 
As a possible future fix for this error, we could use the information in the query object within the `blob` instead of looking up the corresponding query.
Tested on devbox. Confirmed that a key now returns the desired error instead of 500:
@etr2460 @john-bodley";non_debt
"Hi, sorry this is old but I can't see on master that this is changed: [link](https://github.com/apache/incubator-airflow/blob/3b589a9f73bed018bf7e2c7b7265bfce5da91ca0/airflow/hooks/mysql_hook.py ).
Is there any plans to support `mysqlclient` or another python3 friendly driver?
Thanks!";non_debt
The following checkValue already disallows 0, right?;non_debt
43158694-2071 description-0;non_debt
@kszucs may you please have a look at this when you get a chance. There's a change to the prepare-test Ruby script;non_debt
Merging to master.;non_debt
I think also there is a hive metastore test that downloads spark release jar?;non_debt
No need of the semicolon. Same comment applies to other similar lines in Python;non_debt
yes will do it in a separate pull request.;non_debt
This PR seems a little confused. It seems like a legitimate bug, but this is not set up as a backport of an upstream fix. I'd like to see that fixed so I can merge, but I cannot merge this as is. I am going to close it, but please reopen it if it can be fixed, or open a new one as a proper backport of an upstream fix.;non_debt
I added these in Javadoc too.;non_debt
Sorry, since always work at `SQL`  module;non_debt
retest this please;non_debt
Not sure why you had to change this header file?;non_debt
Hi @turbaszek @mik-laj, any updates on this?;non_debt
"CAMEL-11048 Jetty Producer always uses ""Transfer-Encoding: chunked"" header";non_debt
Clean files for MV is not supported?;non_debt
14135471-493 comment-199080050;non_debt
Support components name generated by SummingBird.;non_debt
"There are 2 Struct UDFs in Hive:
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStruct.java (struct)
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNamedStruct.java (named_struct)
For the previous one, we will give the default names for its fields, but for the later, we will create the struct type by given names. We need to implement both of them, but this PR seems only for solve the bug of the former one.";non_debt
"can we also output the layout : 
""Need CuDNN for layout support"" << param_.layout.value()";non_debt
Revert changes on this file? I think one indent is correct.;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
The other members, too?;non_debt
DO NOT MERGE: Just a test of adding properties to JenkinsFile;non_debt
"quickly and easily:
  `[BEAM-<Jira issue #>] Description of pull request`
     Travis-CI on your fork and ensure the whole test matrix passes).
     number, if there is one.
     [Individual Contributor License Agreement](https://www.apache.org/licenses/icla.txt).
---
Some of the timestamps were not adjusted when [BEAM-145](https://issues.apache.org/jira/browse/BEAM-145) was fixed to respect the `WindowFn`'s timestamps.";non_debt
Is the term guaranteed to be a UTF8-encoded string?;non_debt
"@ijuma I think `KafkaFuture` can implement `CompletionStage`. 
Don't mind below comment since I figured out we don't need any additional execution facility as we already have a CompleteableFuture that we can use.
--------------------------------------------------------------------------------------------
There are CompletionStage.*Async(...) methods that do not accept an executor but, as their documentation says, 
So this means we need to supply that ""execution facility"" maybe in the form of 
 * using ForkJoinPool.commonPool()
 * or firing up a new background thread for each task (which is what CompletableFuture does in some circumstances) 
A full blown ThreadPoolExecutor in clients, I think, would be an overkill";non_debt
Done;non_debt
Fixed, I'll pushed that as soon as I find how to do lazy logging in Python ^^ (it was definitely not a long thing to fix...^^ );non_debt
"â€¦rcept/enrich logged messages
this is continued from https://github.com/apache/camel/pull/1559
Hi @davsclaus , @objectiser , 
Here is an outline sketch for the CAMEL-11054. Let's start from here:
* adds `addCamelLoggerListener()`/`getCamelLoggerListeners()` on `CamelContext`
* `CamelLogger` holds a list of `CamelLoggerListener`
* for Log EIP, `LogDefinition` copies `CamelLoggerListener`s from `CamelContext` to `CamelLogger`
* for Log Component, `LogEndpoint` copies `CamelLoggerListener`s from `CamelContext` to `CamelLogger`
* `CamelLoggerListener#onLog()` to receive a log event and return an enriched log message
Questions:
* do we want to extract `enrich()` from `onLog()` so it could be done separately?
* do we want separated handler like `onWarn()`, `onInfo()` and etc. rather than `onLog()`?
* any other event we want to handle in `CamelLoggerListener`?
* As both of Log EIP and Component use `CamelLogger`, it's easy to add it on both of them. Is it OK or do we want it only on Log EIP?
Any comment would be appreciated!";non_debt
Cherry-picked to 8.0.x;non_debt
[FLINK-2008] Fix broker failure test case;non_debt
If we use a inputFormat that donâ€˜t instanc of org.apache.hadoop.mapreduce.lib.input.{CombineFileSplit, FileSplit},  then we can't get information of  input metrics.;non_debt
could you attach a gif to show the behavior?;non_debt
I think we should squash these three commits to one, right?;non_debt
let's explain more that hive metastore will treat the `EXTERNAL` property as a signal to change table type.;non_debt
45721011-4546 review-396049971;non_debt
"@chamikaramj thank you for your suggestions.  @jbonofre @iemejia could you also take a look? I also added io-it-suite-local profile that was missing and jenkins job definition.
I added only the reshuffle and it seems to be a little bit helpful. I didn't optimise it further due to a  problem: different ""consolidatedHashes"" get calculated for each test run for datasets bigger than 600 000 rows. This makes it unable to determine hash for a large scale dataset (eg. 40 000 000 rows). The amount of read and written rows is the same. I also have the same problems while running JdbcIOIT on larger datasets. Also, as I checked, the database content seems to be all right. I can create a JIRA for that after you review this PR and agree that this behavior is odd, ok?
600 000 is approx. 160 MB. I wouldn't call that a large scale test but I think it is something we can start with and then increase the scale and optimize it gradually if needed and if possible (e.g. after tackling the hash calculaction problem i described). What do you think?";non_debt
WIP Do not review;non_debt
[BEAM-3052] ReduceFnRunner: Do not manage EOW hold or timer, set GC hold and timer always;non_debt
@lunchev, I've not had a chance to get to the review backlog in a little while. Sorry about that! I'll try to get this reviewed in the next couple of weeks.;non_debt
Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/2584/;non_debt
@vvysotskyi, thanks much for the review and for committing the code. Thanks also for showing the disassembly of the generated code for the for loops. You analysis is convincing and I'll change the loops in the next PR.;non_debt
`val labels = vectorRDD.select('label).as[Double].collect()`;non_debt
"This is the fix for the bug which is in IncrementalCheckPointManager. (Comparing key bucket id with time bucket).
Discussion about exclude expired time buckets while saving data happened in the below PR:
https://github.com/apache/apex-malhar/pull/516";non_debt
@blueorangutan package;non_debt
Gentle ping @cloud-fan;non_debt
"Thank you for submitting a contribution to Apache NiFi.
_Enables X functionality fixes bug NIFI-YYYY._
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
â€¦Connected method.;non_debt
fixed.;non_debt
I guess it's changed by auto format.. I will remove it.;non_debt
"1 nameserver, 1 broker, 1 producer
producer send messages to a topic, when broker is down, producer still send heartbeat to broker
because 
org.apache.rocketmq.client.impl.factory.MQClientInstance#cleanOfflineBroker
does not work";non_debt
"Implementation of IScalable scheduler interface.
Fixes #1430";non_debt
@icemelon9 Can you please manage this PR?;non_debt
this no longer supports file consolidation, does it?;non_debt
For #4525.;non_debt
[SPARK-21783][SQL] Turn on ORC filter push-down by default;non_debt
I will rebase and clean up this tomorrow.;non_debt
clang-analyzer build *successful*! See https://ci.trafficserver.apache.org/job/clang-analyzer-github/177/ for details.;non_debt
45721011-3956 description-0;non_debt
"Sure, I can make it a part of Sanity Test since we plan to have that stage whether it is a full build or smoke test. 
@gautamkmr @sandeep-krishnamurthy any opinions?";non_debt
Not necessary to have `@param` and `@return`.;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/2820/;non_debt
@vorburger darrrn... I had a feeling that I forgot something. Thanks for pointer, will do.;non_debt
This is already calculated in the HashJoinBatch.partitionNumTuning() method. You should just use the partitionCount computed from there. You can get the number of partitions from the PartitionStatSet;non_debt
@1teed Thanks for the contribution. Tested with a centos-zeppeling docker image and works fine! Looking forward to the merge!;non_debt
retest this please;non_debt
"Changes LGTM @zentol ! 
When Travis gives a green light, feel free to merge!";non_debt
"Bumps [netty-all](https://github.com/netty/netty) from 4.1.17.Final to 4.1.42.Final.
- Additional commits viewable in [compare view](https://github.com/netty/netty/compare/netty-4.1.17.Final...netty-4.1.42.Final)
Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.
[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)
---
You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/apache/orc/network/alerts).";non_debt
"We have two blocking calls in `NormalizerWorker` thread:
1. While retrieving new table for normalizing from Queue
2. RateLimiter blocking threads trying to submit plans";non_debt
final, pls;non_debt
@drcrallen if added why this is false ?;non_debt
Test failure was due to a random behavior in RDDSuite, which is fixed in https://github.com/apache/spark/pull/387 .;non_debt
Updated the log messages.;non_debt
"@felixcheung 
I think it's ok since all LICENSEs will be merged into a single file
- https://github.com/apache/zeppelin/blob/master/dev/create_release.sh#L93-#L96";non_debt
jenkins, retest this, please;non_debt
I agree the 2nd option sounds lot better, I have changed the PR and updated all the example DAGs by importing just the function from module instead of __init__.py.;non_debt
Remove USE_MKL_IF_AVAILABLE flag;non_debt
CI has passed;non_debt
"â€¦le (#10981)""
This reverts commit e93d92e8ac6d4f85d193943e27d585fb6e01568c.
See linked issue
-CI, manual verification";non_debt
Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/1503/;non_debt
Numpy Identity operator;non_debt
"Mojos use available Aether subsystem (Sonatype or Eclipse) by looking at what's
available in PlexusContainer.
It's easy to add Maven deps for both Sonatype and Eclipse versions of Aether, but it's not that easy to depend on both `maven-core:3.0.3` and `maven-core:3.1.0+`.
Mojos don't get particular Aether component autowired directly by type - they can however get autowired entire `PlexusContainer`. We then look at the available Aether subsystems and the use a `DependencyHelper` layer to access correct Aether (both Sonatype and Eclipse).
`karaf-maven-plugin` may use `maven-core` API directly for Sonatype version of Aether (for dependency on `maven-core:3.0.3`), but it also can use Eclipse Aether and the methods from Maven API which reference Eclipse Aether when used with Maven 3.1.0+ - it however requires a bit of `java.lang.reflect`.";non_debt
Added.;non_debt
"...so that core extensions are not wiped out
currently that profile is unusable as it bundles contrib extensions only whereas expected behavior is to have both core+contrib extensions bundled";non_debt
Oups I screwed the correct title by double clicking merge by mistake. Thanks @timrobertson100 and @rangadi for the proper review.;non_debt
Thanks for the PR it has been merged. Do you mind closing this?;non_debt
Sounds good. Let me know if I can help. The `cxf-specs` feature should be taking care of exporting these packages in the container when they're not provided by the container itself.;non_debt
Thanks to @andrewor14 and @vanzin's comments;non_debt
ok to test;non_debt
"Ah, just remembered that I checked in helpers in both python and java a python/apache_beam/io/gcp/bigquery_io_metadata.py 
You may want to move the santize function to those files.
_is_valid_cloud_label_value";non_debt
ARROW-1541: [C++] Fix race conditions in arrow_gpu with generated Flatbuffers files. Do not put generated files in source tree;non_debt
addressed in #7570;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2885/
Test FAILed (JDK 8 and Scala 2.12).";non_debt
+1 Change looks good. @prashanth-vasudev should we merge now or do you intend to make changes to address comments above? Those comments could be addressed later if you wish.;non_debt
Sounds about right :);non_debt
Good idea! Will try to implement it.;non_debt
SDV Build Success , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/5228/;non_debt
Build Success with Spark 2.1.0, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.1/471/;non_debt
Merged build started.;non_debt
19961085-6009 review-487584167;non_debt
"There are integrations available:
- https://drawio-app.com/create-mermaid-diagrams-in-draw-io/ 
- https://github.com/nopeslide/drawio_mermaid_plugin
Or we could just have the png and steps to reproduce like copy/paste text to https://mermaid-js.github.io/mermaid-live-editor/";non_debt
/pulsarbot run-failure-checks;non_debt
fixed;non_debt
Then we can add a `LOG`, or maybe we should even throw an exception. What do you think?;non_debt
"registerUserKeys log based on this change
2015-11-02 17:15:19,061 INFO  [a.c.c.a.ApiServer](2001334745@qtp-678426242-0:ctx-608cad64 ctx-6e00b6fc) (logid:bfb12e9d) (userId=2 accountId=2 sessionId=9g6dq5sjxlk7137t68hfkux8) 0:0:0:0:0:0:0:1 -- GET command=registerUserKeys&response=json&id=e2fdf8e4-73be-11e5-8882-249684d59f9c&_=1446464718702 200 {""registeruserkeysresponse"":{""userkeys"":{}}}";non_debt
@richardcloudsoft @grkvlt can you take a look at this please?;non_debt
35144191-25 description-0;non_debt
Seems unrelated to the change proposed in this PR. Can we remove this?;non_debt
maybe make these values relative to MAX_SLAB_SIZE so that if it's ever changed these tests remain valid;non_debt
"  - https://issues.apache.org/jira/browse/AIRFLOW-2956
 Adds ability to specify kubernetes tolerations for dags using kubernetes pod operator
  - When adding new operators/hooks/sensors, the autoclass documentation generation needs to be added.";non_debt
run java8 tests;non_debt
Can one of the admins verify this patch?;non_debt
what if it's null?;non_debt
"I ran into this issue developing a PR I have yet to submit.  The discrepancy in model outputs is caused by the fact that when cudnn calculates the running variance, it uses the 'sample variance', while this test is comparing in all cases to the 'population variance'.  The difference is that the sample variance uses a factor of N-1 in the denominator, while the population variance uses N (where N is the number of elements in the sample).
My upcoming PR will include a fix for this, and after it's merged, if you want you could revert this commit that changed the problem sizes, since that is not the real issue here.";non_debt
Run Python Dataflow ValidatesRunner;non_debt
ok;non_debt
Handing it over to @betodealmeida who has a better sense of what should be in Superset vs in the dbapi driver and/or SQLAlchemy dialect;non_debt
Tested this in isolation and it does work for directories.  A new test that I added should exercise this path.;non_debt
"Update breeze to 0.13.1 for an emergency bugfix in strong wolfe line search
https://github.com/scalanlp/breeze/pull/651
N/A";non_debt
@fjy sure, I wanted to make the PR reviewable and write docs while review progressed.;non_debt
I tried to fix this but the formatter put it back this way;non_debt
"@sirpkt thanks for updating your patch. 
I've tested the following query and found that the result of Tajo is different from that of pgsql.
**Tajo**
**PostgreSQL**
As you can see, some values are null in Tajo.";non_debt
Run Python PreCommit;non_debt
[SYSTEMML-540] [SYSTEMML-445] Initial implementation of conv2d/maxpooling builtin functions and GPU backend;non_debt
@rxin try running `./python/run-tests` locally. It fails for me. Even if `./dev/run-tests` _might_ pass because of packaging/timing, I think having `./python/run-tests` fail in a release is not acceptable.;non_debt
2211243-2079 description-0;non_debt
Enable coverage reports for PR diffs;non_debt
"Thanks @ChenSammi for working on this. The change just enables the read path with topology awareness feature enabled. Can we just close this issue as resolved with HDDS-1713 and open up a new jira to just enable the read with topology awareness feature on?
The actual change does not fix the problem as described. What do you think?";non_debt
"To be ruthless about even my own code
* 3.2.2. is lowest risk
* 3.3.3 has a lot of s3 and abfs changes, but there's enough changes elsewhere to make it more traumatic
* And a 3.3.1 is really needed/due for the stabilisation there.
It would probably be safest to build/release with a 3.2.x but allow 3.3.x to be built with if someone really wanted it.";non_debt
Thanks!;non_debt
Could you please switch the test to work with `classpath:` URI so that the it is runnable outside Camel Quarkus source tree without any additional setup?;non_debt
Update maven-dependency-plugin to version 3.0.1;non_debt
Changed Discuss URL to - http://discuss.mxnet.io;non_debt
"Thank you for submitting a contribution to Apache NiFi - MiNiFi C++.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
I'm not sure if this check is worth having a separate class, especially as we move to more modular IO that does not necessarily have to be at the root.;non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
"The original fix only took care of the generated file for the service, if the service has a method that uses a structure from an included thrift file, use clauses were emitted.
The issue was re-opened because if a structure in one thrift file contains a field for a structure defined in another thrift file, the perl generated code doesn't work properly as described in the defect since these live in Types.pm, and it needs the same use clauses emitted.";non_debt
ARROW-4337: [C#] Implemented Fluent API for building arrays and record batches;non_debt
@mengxr @jkbradley test this please;non_debt
34864402-2791 description-0;non_debt
Test Passed.  https://jenkins.esgyn.com/job/Check-PR-master/703/;non_debt
"Yes I'd suggest just removing the tutorials portion for now until the
process is better settled.
On Thu, May 11, 2017 at 9:29 AM, Naveen Swamy <notifications@github.com>
wrote:";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5650/
Test PASSed (JDK 8 and Scala 2.12).";non_debt
Thanks, all. I've closed voting and marked the KIP as accepted.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/carbondata-pr-spark-2.1/65/<h2>Build result: ABORTED</span></h2>[...truncated 633.00 KB...]	at java.util.concurrent.FutureTask.run(FutureTask.java:266)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)	at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.maven.plugin.MojoFailureException: Timed out after 0 seconds waiting for forked process to complete.	at org.scalatest.tools.maven.AbstractScalaTestMojo.runForkingOnce(AbstractScalaTestMojo.java:319)	at org.scalatest.tools.maven.AbstractScalaTestMojo.runScalaTest(AbstractScalaTestMojo.java:242)	at org.scalatest.tools.maven.TestMojo.execute(TestMojo.java:106)	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207)	... 31 more[ERROR] [ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR]   mvn <goals> -rf :carbondata-spark-common-testBuild was abortedchannel stoppedSetting status of ea1b4a7f4d8ec8bf0a80cfcc989097949c0065a2 to FAILURE with url https://builds.apache.org/job/carbondata-pr-spark-2.1/65/ and message: '(Spark 2.1) 'Using context: Jenkins (Spark 2.1): Maven clean install";non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/25329/
Test PASSed.";non_debt
HBase does this with their `TableName` class to avoid tons of objects hanging around needing GC. The API is pretty nice as an end-user:;non_debt
alignment.;non_debt
Run Load Tests Python GBK Flink Batch;non_debt
@MaxGekk @maropu Please let me know if there's anything else I need to do to get this moving.;non_debt
LGTM;non_debt
HDDS-3475. Use transactionInfo table to persist transaction information.;non_debt
Merged build started.;non_debt
Sorry, I don't follow. `SparkListenerApplicationEnd` is posted by `SparkContext.stop`, which is the same place where you're adding the hook to clean up the listener. So it should behave exactly the same way, no?;non_debt
It looks like this flag is designed to allow the adjustment to only happen once, is that actually what we want? If the row size is growing it would seem like a good idea to allow for several batch size adjustments. It also removes another boolean state to manage.;non_debt
@mistercrunch Any thoughts? Line chart annons seems use moment library to parse dates and it would work fine with standard date/time formats;non_debt
ok;non_debt
DISPATCH-1005 - Fixed system_tests_ssl.py to work on rhel6 and other â€¦;non_debt
KYLIN-4485 Create a self service interface for cube migration;non_debt
[SPARK-1997] mllib - upgrade to breeze 0.8.1;non_debt
Could we do this by creating a new class loader instead of a whole new process?;non_debt
Scala style (braces):;non_debt
Run Spark ValidatesRunner;non_debt
Can one of the admins verify this patch?;non_debt
Build Failed  with Spark 2.3.2, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/982/;non_debt
@andygrove  I personally find them helpful -- I don't follow all notifications for rust/arrow updates so it is helpful;non_debt
@zentol do you agree to merge this now. Its the last thing I would like to get into RC1.;non_debt
Shouldn't we have a wrapper for this?;non_debt
Style.;non_debt
Yes, I was trying to find resources about how dependency scopes relate to ITs, but I could not find anything useful. What I read implied that it would include `<scope>test</scope>`. We need to solve this without changing the SDK, because this sort of module is not generally going to be part of our project, but just some externally developed IO.;non_debt
The tests that failed are known transient failures that have been fixed in trunk.;non_debt
"# [Codecov](https://codecov.io/gh/apache/airflow/pull/6287?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/airflow/pull/6287?src=pr&el=continue).";non_debt
193065376-2385 description-0;non_debt
add space after comma;non_debt
@franz1981 ok to merge this?;non_debt
I consider this to be part of a private helper method where `FlinkDistributionOverlay` is the private helper method. Hence, I believe this is ok. Happy to reduce the access of the constructor to package private.;non_debt
Hey @taftster I made the changes and cherry picked them to your branch, not authorized to push them however. Can you either give me access or if you have another suggestion would be fine with me.;non_debt
/pulsarbot run-failure-checks;non_debt
"Suggest changing this to something like: com.example.fixedwidthparser.Ad
to emphasize that it should be the fully qualified class name (note that this is what is done in the example).";non_debt
MAVEN MIGRATION - ZOOKEEPER-3226 - add profile for C build;non_debt
retest this please;non_debt
"Dict *labels* doesn't contain *try_number* key, as it's not set in *WorkerConfiguration* make_pod. That's the reason why pods are not deleted.
https://github.com/apache/incubator-airflow/blob/v1-10-stable/airflow/contrib/kubernetes/worker_configuration.py#L197";non_debt
17165658-18865 review-137930066;non_debt
"Sorry for late response because of a little busy these days.
I agree with your above comments. So whether the task executor can be released is based on whether there are active channels in this executor. The task executor can only exit after all the tcp connections are closed gracefully.
In theory as long as the downstream received all the data from the network, then the upstream side can be released normally, no need to wait all the received data are processed completely by downstream side. But we have on existing ack mechanism to notify upstream side of this, so it is easy to rely on close request currently.  Based on downstream's consumption to release upstream's resource, it may get extra benefits in failover scenarios in future for persistent output files in upstream side, because the upstream do not need to restart during consumption exception in downstream side.
But I just a little wonder it might bring potential effects in future via close request. For example, if there are 10 downstream tasks reuse the same tcp connection, and 9 tasks are finished earlier and only one tail task delay long to finish, then all the 10 partition views must be released together until the last downstream task finished. Although it might be no bad effects for delay releasing partition views currently.
I would continue reviewing other parts of this PR and it may take some days on my side. :)";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/541/;non_debt
clang format *successful*! https://ci.trafficserver.apache.org/job/clang-format-github/334/;non_debt
so cool!;non_debt
@zymap @Jennifer88huang Glad to help.;non_debt
832676-807 description-0;non_debt
1.1.2 release;non_debt
"Â  ""CAMEL-11617:spring-boot - service-call tests uses hardcoded port numbers""";non_debt
Done;non_debt
@rmetzger this is good enough for me, since there are also tests. What do you think?;non_debt
Remove the correct entry from priority queue and insert the new node into the queue;non_debt
"â€¦feature' section are automatically enlarged when clicked.
'colorbox' is licensed under the MIT license
https://github.com/jackmoore/colorbox/blob/master/LICENSE.md
src/content/download/nb90/index.asciidoc contains an example of usage
when the document is in asciidoc format";non_debt
"Thanks for taking care profiles for MapR distribution.
Looks good to me and merge if there're no more discussions.";non_debt
@akkio-97 please update the latest CI UT build on these changes;non_debt
Thanks for the patch. LGTM;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/2666/;non_debt
Thanks for the PR, cc @junrao.;non_debt
LGTM pending test;non_debt
Oh, right! Here. Thanks for explanation.;non_debt
Merge to master if there're no more discussions.;non_debt
`HCFS path where files with the client:// scheme will be uploded to in cluster mode.`;non_debt
R: @davorbonaci;non_debt
"This PR is no longer WIP. Build is passing, there is good amount of test coverage and I have also tested it successfully on small test clusters running on kubernetes without a zookeeper cluster at all. This PR is ready to be merged.
While I am sure caveats would pop up when this gets used in long running large clusters, this is a good starting point to be released as an experimental feature in next Druid release and noted as such in the docs introduced.
sidenote:
if someone has experimented with running k8s clusters in travis builds, please feel free to work/comment on https://github.com/apache/druid/issues/10542 as that would set the stage of testing the code here on real k8s cluster in the travis builds.";non_debt
[FLINK-4414][cluster management] Remove restriction on RpcService.getAddress;non_debt
"This issue is present in branch-2.4. It's a good to have but not critical, can we still backport it to branch-2.4? 
cc: @cloud-fan @dongjoon-hyun";non_debt
"# [Codecov](https://codecov.io/gh/apache/skywalking/pull/4228?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/skywalking/pull/4228?src=pr&el=continue).";non_debt
PHOENIX-4918 Apache Phoenix website Grammar page is running on an oldâ€¦;non_debt
@rhtyd rebased.;non_debt
"My interpretation of fini_received flag means that we have seen a FINI frame or otherwise the client side has indicated that the connection is gone.  
The call to release_stream with a nullptr argument just goes through and sees if the state of the world is ready to shutdown (all stream counts set to 0 and a FINI has been seen).";non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/1568/;non_debt
I got the overhead, so I pushed the commit to support it.;non_debt
Still worth checking for nonEmpty? the behavior may be different in the second change here otherwise.;non_debt
"If system property java.home is defined, please use `System.getProperty(""java.home"")+""/bin/java""` instead.  Only use `java` if system property java.home is not defined";non_debt
Jenkins, add to whitelist.;non_debt
"That's why I explicitly tested reading the last doc ids (`_lastSequentialDocIds`).
Modified the test to test sequentialDocId starting from 0 - 31";non_debt
LGTM, thanks @liuxunorg;non_debt
ARROW-5209: [Java] Provide initial performance benchmarks from SQL workloads;non_debt
Yes, the last one is in Dispose. Driver may shut down before receiving it. I just relaxed the checking little bit.;non_debt
+1;non_debt
"1ï¼Œdestroy clean adapter
1ï¼Œif resuse";non_debt
Build Success with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/1689/;non_debt
Yup :-);non_debt
I did a bunch of testing and I am +1 on merging this in.  There is still the issue with double escaping the contents returned for a file, but I think we can fix that after if we want.;non_debt
[CALCITE-2421] Improve RexSimplify when unknownAsFalse is true;non_debt
[WIP] [SYSTEMML-445] Upgraded CUDA/CuDNN versions and added LSTM, batch normalization kernels;non_debt
so how can we get the version info for a table?;non_debt
@mark800 are you following up on this?;non_debt
@gatorsmile I opened a new pr, so if you get time, could you check #19188? Thanks!;non_debt
Thanks much @mcgilman !;non_debt
@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.;non_debt
"The names here are generally conventions.
In your client, you can do
    git remote -v
to list all your ""remotes"" i.e. other repositories that you push and pull
to. You can also do
    git branch
to list all your branches.
By default when you clone a fresh repository, you get a remote called
""origin"" that points to wherever you cloned it from. New (and most
existing) branches have a ""master"" branch by default (e.g. beam HEAD is the
master branch on https://github.com/apache/beam). So
    git pull --rebase=interactive origin master
pulls the branch ""master"" from the remote ""origin"" (presumably
github.com/apache/beam) into your own branch, and the --rebase=interactive
means take all your local commits and attempt to apply them on top of
origin/master, rather than creating a merge commit, interactively.
pulls all the remote changes into my repo (but not into any of my branches)
then ""git rebase [-i]"" rebases my current branch on top of it.
    git push [remote_name] [branch_name] [--force]
will push branch_name (defaults to the current branch) to remote (branches
can have a default remote set up) and the force means overwrite what's
there if it differs (needed after a rebase).
Another thing you can to save your work is if you're on a branch and
concerned about messing it up, you can do ""git checkout -b
some-branch-backup"" to make a copy of your current branch and then if you
mess up your current branch horrendously you can delete it (git branch -d
branch-name) and re-create the branch from your backup (git branch -b
some-branch some-branch-backup). Hopefully that's useful for being able to
experiment more freely, which will help in learning git.
On Fri, Jan 18, 2019 at 1:16 AM CraigChambersG <notifications@github.com>
wrote:";non_debt
Huh. It looks like removing this results in not seeing all the tombstones we expect. I'll leave it in.;non_debt
Use StringBuilder? And perhaps rename method too?;non_debt
Merging in as #2577 #2579 #2580 closing this one to avoid confusion;non_debt
Added -J-Djdk.lang.Process.allowAmbiguousCommands=true to netbeans.conf because of Maven spacing issue: https://lists.apache.org/thread.html/bf415874d97739bd23eef134a246a8a7241c011372b36cc8650bf901@<dev.netbeans.apache.org>;non_debt
Why creating this class other than use DefaultCouchbaseEnvironment.Builder  directly?;non_debt
yah, I should put the resources into it.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3705/
Test FAILed (JDK 7 and Scala 2.10).";non_debt
Rewrite federation user GET/POST/DELETE;non_debt
FilterBox,BigNumber,WorldMap: Handle empty results;non_debt
nit: `if {} else {log.trace()}`.;non_debt
Packaging result: âœ”centos7 âœ”centos8 âœ”debian. JID-1878;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/3577/;non_debt
After second level headings a `{% top %}` is recommended.;non_debt
For team city run;non_debt
I tested this with a host with 8 CPUs and it failed trying to allocate 16 on XCP-ng 7.6;non_debt
+1 on Trident support.  Thanks!;non_debt
"Currently, `SimplifyConditionals` handles `true` and `false` to optimize branches. This PR improves `SimplifyConditionals` to take advantage of `null` conditions for `if` and `CaseWhen` expressions, too.
**Before**
**After**
**Hive**
Pass the Jenkins tests (including new extended test cases).";non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/3263/;non_debt
ARROW-5880: [C++][Parquet] Use TypedBufferBuilder instead of ArrayBuilder in writer.cc;non_debt
Merged build started.;non_debt
why is this a function that returns a function, rather than a function?;non_debt
"Test FAILed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/23670/
Test FAILed.";non_debt
Thanks @xiaoyuyao 's review.  I have submitted a new commit fix the above issues.;non_debt
"1. Ignore .vscode dir
2. Ignore C++ file in be/src/gen_cpp/ dir";non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4924?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-pinot/pull/4924?src=pr&el=continue).";non_debt
I've created SPARK-1201 (https://spark-project.atlassian.net/browse/SPARK-1201) to cover optimizations in cases other than DISK_ONLY.;non_debt
Right. For distributed training we want to reduce the network traffic and use `row_sparse_pull` instead. We may change it if it's not user-friendly. One distributed trainig example with sparse weight is here: https://github.com/eric-haibin-lin/mxnet/blob/sparse/example/sparse/linear_classification.py#L171;non_debt
[SQL] Add support for GCS entries in DataCatalog;non_debt
17165658-17227 comment-285531634;non_debt
It's possible to just change the certificate file;non_debt
This comes from the original patch from @ivankelly. In production I usually use PKCS12. I can make a new option for the format. Do you think it wild be useful?;non_debt
run seed job;non_debt
Yea, it looks making sense to me.;non_debt
IGNITE-11233 Fix for .NET build: Ignite Build for Java 11 does not reuse ignite-tools from Build Apache Ignite;non_debt
LGTM;non_debt
fixed some issues and improved testing on .../about endpoint;non_debt
Thanks, @omkreddy!;non_debt
In favor of another one;non_debt
done;non_debt
Actually just one comment. Is the producer also consistent in accounting for the message set overhead?;non_debt
Update CHANGES.txt and spark-ec2 and R package versions for 1.6.1;non_debt
Similar comment to the IN filter, it might be worth only doing this if a long predicate is actually requested. (but just once if a long predicate is requested more than once);non_debt
If we revert #7690 changes, the deadlock issue will reappear, so we should not it. Can the issue of #7706 be reproduced on the master branch? If so, could you fix the code in the master branch?;non_debt
This command was always run when executing the `` airflow worker`` command.  The option to disable autostart of this server has been introduced recently, so we can assume that everyone who starts worker must have this port free.  These logs should be shared from workers, so no one could technically run it without playing with strict process isolation (linux net namespace) and other tricks.;non_debt
remove TablesContext.find();non_debt
I sent DM to you. -);non_debt
@robertwb This fix Python postcommit failure ([build link](https://builds.apache.org/view/Beam/job/beam_PostCommit_Python_Verify/1019/));non_debt
Or we can have a GeneratedRowCoder interface too.;non_debt
forgot this, will remove;non_debt
jenkins, extra tests;non_debt
Moved the fallback path into this method (mimicking GetReadableBuffer).;non_debt
[STORM-3657] set storm.messaging.netty.authentication to topoConf OR daemonConf;non_debt
17165658-28345 review-415390714;non_debt
retest this please;non_debt
Makes sense. If you need to override for a single test operation, can always add a @VisibleForTesting setter.;non_debt
"Actually I just copied over the implementation from some other class, and wasn't getting any warnings...I should probably do a thorough pass over all the warnings to see what else is missing, but this (and the raw types) is enabled now.
Thanks for catching";non_debt
Looks good, thanks!;non_debt
Got it. Thanks for the clarification.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3647/
Test FAILed (JDK 8 and Scala 2.11).";non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
done;non_debt
This should probably log a warning to notify administrators that the value provided was invalid and a default value will be used.;non_debt
@merlimat I am not sure about which `failover tests` we should run ?;non_debt
"No this test was added for testing calcite based rewrite. The text based rewrite modified the plan because I forgot to turn it off in this test case.
Turned off and reverted the out.";non_debt
"Done and done @davies, @yhaui
On Sat, Nov 15, 2014 at 5:17 PM, Yin Huai notifications@github.com wrote:";non_debt
There is a possibility that you might have some patches against your 5.3.x that might make this work. But it won't compile against 5.3.2 provided by Traffic Server.;non_debt
[SPARK-21293][SPARKR][DOCS] structured streaming doc update;non_debt
KAFKA-6054: Add 'version probing' to Kafka Streams rebalance;non_debt
Same here.;non_debt
"Sorry for late comment, but today I learned that Hadoop has a [TimedOutTestsListener](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/test/TimedOutTestsListener.java).
It's configured in the `pom.xml files`, for example here: https://github.com/apache/hadoop/blob/1189af4746919774035f5d64ccb4d2ce21905aaa/hadoop-hdfs-project/hadoop-hdfs/pom.xml#L236
Wouldn't it be more effective to use a similar listener? (If yes, I would prefer to fork it instead of adding one more Hadoop dependency, especially after HDDS-3353 and HDDS-3312).";non_debt
"---
**^ Add meaningful description above**
Read the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines)** for more information.";non_debt
Several files were missing licenses, I added them except for solaris.patch because I do not know how to add comments to a patch file;non_debt
Ref https://issues.apache.org/jira/browse/WEEX-52;non_debt
Remote Procedure Call (as opposed to a native Go API for example).;non_debt
Any updates to the state will be stored and passed to the user given function in subsequent batches when executed as a Streaming Query.;non_debt
"Document what it means when this is not set and that this must be >= 1.
Also say this is not set by default.";non_debt
"[ActiveMQ-Artemis-PR-Build #476](https://builds.apache.org/job/ActiveMQ-Artemis-PR-Build/476/) SUCCESS
This pull request looks good";non_debt
@paul-rogers I added metrics for merge join also. I refactored AbstractRecordBatchMemoryManager to handle batches from multiple streams. Please review when you get a chance.;non_debt
DRILL-6951: Row set based mock data source;non_debt
Use github actions cache;non_debt
"Thank you for submitting a contribution to Apache NiFi.
The default value for array fields was always being set even if not specified in the schema
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
I was using SplitText for testing, but I agree with you, 0 is preferable. Changed default value from 1 to 0.;non_debt
"Jenkins job that uses Java 11 enabled Dataflow Worker harness to run validatesRunner test array on Dataflow
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/)<br>[![Build Status](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python3_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) <br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/) | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.";non_debt
Hello Sachin, could you explain what the discrete sampler does?;non_debt
^^ @mik-laj;non_debt
BTW, my thought was;non_debt
"Add CTE hint resolve in `org.apache.spark.sql.catalyst.analysis.ResolveHints.ResolveJoinStrategyHints#apply`
Add a UT in `org.apache.spark.sql.test.SQLTestUtils#test`
Branch 2.4, when resolve CTE in `org.apache.spark.sql.catalyst.analysis.Analyzer.CTESubstitution`, we have a chance `executeSameContext` to apply all rules to CTE include hint resolve.
Branch 3.0, because `CTESubstitution` is moved to a separated class, we miss the feature as follow:
`
scala> sql(""create temporary view t as select 1 as id"")
res0: org.apache.spark.sql.DataFrame = []
scala> sql(""with cte as (select /*+ BROADCAST(id) */ id from t) select id from cte"")
org.apache.spark.sql.AnalysisException: cannot resolve '`id`' given input columns: [cte.id] line 1 pos 59
'Project ['id]
+- SubqueryAlias cte
   +- Project [id#0]
      +- SubqueryAlias t
         +- Project [1 AS id#0]
            +- OneRowRelation
`
No
Unit test.";non_debt
Sounds fine to me;non_debt
"Retest this please
I'll wait for Allen's review here. Thanks.";non_debt
Retest this please;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/28231/
Test PASSed.";non_debt
Add a github webhook relay for projects;non_debt
Storm 2913 2914 1.x;non_debt
is this defined else where?;non_debt
Should it be detected by pylint or other linter?;non_debt
JENA-1585: Fuseki core reorg;non_debt
thanks;non_debt
@eolivelli - import static added. Let me know if this looks ok !;non_debt
@PaulAngus @DaanHoogland this is ready for merge once the cloudstack-common rpm is verified to not install the libuuid i386 dependency.;non_debt
":confetti_ball: **+1 overall**
This message was automatically generated.";non_debt
Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/3286/;non_debt
[ZEPPELIN-1832] Fixed a bug in zombie process when Zeppelin stopped.;non_debt
"Java 11 failed with `kafka.server.LogOffsetTest.testGetOffsetsBeforeNow` 
retest this please";non_debt
cc: @lazylynx;non_debt
Add contributing guidelines and PR template;non_debt
pg3/742;non_debt
206417-2101 review-314395651;non_debt
@rhtyd a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests;non_debt
"DRILL-7683: Add ""message parsing"" to new JSON loader";non_debt
ok;non_debt
I don't think this function will have the correct signature after rendering. Please build and check, and override the function signature if needed.;non_debt
@junrushao1994 you're the man!;non_debt
34864402-10074 review-173891556;non_debt
DDL statements are e.g. CREATE, ALTER, DROP. The HELP command will show all the types of statements you can run, which also includes DML and DQL.;non_debt
Jenkins, retest this please.;non_debt
Do not merge;non_debt
"Great progress @nevi-me !
I'm traveling this week but will start helping with integration testing next week.";non_debt
"R: @aaltay Could you take a look?
Website staging is not working at the moment, fix is in progress: https://github.com/apache/beam/pull/11796";non_debt
"suggestionï¼š  convert ""_temp"" to constants";non_debt
Adds Instacart to readme.md only;non_debt
LGTM;non_debt
"@madjam, we have updated the tutorial with new content, same for the symbol, 
Your changes are based on old content.";non_debt
+1 to emphasize the distinct clause in SQL.;non_debt
[Endpoint slice](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/) resources should be listened to either.;non_debt
Five improvements;non_debt
Oops. I forgot to add changes in top directory. updated.;non_debt
"I would rather rely on the logback.xml to control log levels rather than system options. You can also use markers to tune the output at the debug level even further within a class if necessary.
https://examples.javacodegeeks.com/enterprise-java/slf4j/slf4j-markers-example/";non_debt
70746484-4867 comment-584951383;non_debt
Merged build finished.;non_debt
"beam_PerformanceTests_Python passed in [this build](https://builds.apache.org/view/A-D/view/Beam/job/beam_PerformanceTests_Python/1650/). Squash commits to one and wait for merging since LGTM is received.
@aaltay";non_debt
20587599-12815 review-449585541;non_debt
LGTM;non_debt
 Merged build triggered.;non_debt
@dafrista , could you review this?;non_debt
TS-4601: Connection error from origin_max_connection with origin_max_â€¦;non_debt
If the key is already in the case insensitive map, we should fail and say duplicated keys detected.;non_debt
It seems to be wrong.;non_debt
ASF: added license header.;non_debt
LGTM. Thanks @Baunsgaard - overall, this is a fine patch. Down the road, we might want to create consistency of parameters among all classification and regression scripts, and add further svm debugging tools (or an verbose option), where we actually compute the introduce confusion matrix and show it to the user (as done in the msvm-predict algorithm).;non_debt
Ditto.;non_debt
is this still WIP?;non_debt
@koeninger , I can't visit [this url](https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/28872/) , it's 404. ??;non_debt
"@cloud-fan If I use Literal(d20170102), then this literal has dataType ""DateType"" as it is consistent with the corresponding column.  I tested it with a SQL statement: SELECT key1 FROM testTable WHERE key1 = cast('2017-01-02' AS DATE) and found that the literal has an internal type SQLDate (which is an integer).   Hence, I probably should use Literal( DateTimeUtils.fromJavaDate(d20170102), IntegerType).  Am I right?  Please advise.";non_debt
no, not really. must be a result of intellij's extract method;non_debt
"https://issues.apache.org/jira/browse/IGNITE-13288
Marked the following events as internal:
- EVT_CLUSTER_ACTIVATED
- EVT_CLUSTER_DEACTIVATED
- EVT_BASELINE_CHANGED
- EVT_CLUSTER_STATE_CHANGED
There are discovery events that are listened to by all nodes.
It will be useful to include these events to listen on all nodes by default too. 
All of them are rare, system and cluster-wide.";non_debt
Nit: If this is a float, shouldn't we keep it that way in Schema?;non_debt
[SPARK-34965][BUILD] Remove .sbtopts that duplicately sets the default memory;non_debt
Fails in appveyor after rebase;non_debt
@villebro part of the tabs migration process, which happens once the user visits sqllab when this feature is enabled, [involves using the tab id(int) cast to a string as the `sql_editor_id`](https://github.com/preset-io/incubator-superset/blob/master/superset/views/sql_lab.py#L260). Once all tabs are migrated over the type will be consistent (numeric strings). The before migration and in between state is where the error occurs (or doesn't in the case of MySQL/SQLite/others probably).;non_debt
"So JPS has some down sides. 
 1 if the workers are running in containers it might not work as it relies on /tmp and if that is mounted to something other then your /tmp you won't see the process.
 2. if someone is debugging a worker or its not responding jps can hang.
We may want to look at  using kill -0 for linux/mac as it will quickly tell you if its alive, not sure on windows, assume it would require something else.  Here is an stack overflow post that talks about it:
https://stackoverflow.com/questions/21460775/verify-if-a-process-is-running-using-its-pid-in-java
Note this is assuming you are saving the PID in the allocation file or using it from the PID directory.
I also think we should only run kill -0 or whatever when it is absolutely necessarily. Meaning the Worker can't allocate anything and then I would only look at the exact PIDS that have something allocated.";non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6215/;non_debt
Icon please;non_debt
Keep ID as ref ID in the readSampledRecords;non_debt
scratch;non_debt
Merging now. I'll address my most recent comments in a new PR later this evening.;non_debt
@TobKed closing as you will take over this on your terms. As we discussed offline - thank you once again!;non_debt
"@mik-laj This PR have to way to pass token, One is pass token by `Operator`, another is set it in `dingding_webhook_default` connection.
Second way have no safe problem, allow pass to operator just provide other way to send message.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4435/";non_debt
"This follows the way it is done in the `fetchBlocks` API.
Should we remove it?";non_debt
This doesn't quite make sense. We created 3 mapping with various group values. I would expect the list only show the 3 mappings we created. Here there are 4 mappings in the list result. I would consider one of them duplicate.;non_debt
"To avoid race of `eventProcessor.allocate(sizeof(PollCont))` vs `new ((ink_dummy_for_new *)get_UDPPollCont(thread)) PollCont(thread->mutex)`
Fix #2226";non_debt
Bein' grumbly about wedged tests and timeouts, as you do;non_debt
Already in master;non_debt
"How can I run a batch pipeline with V2 runner? I got the following error when submitting the job:
`
The workflow could not be created. Causes: (c0594c9b86bfe05d): The workflow could not be created due to misconfiguration. If you are trying any experimental feature, make sure your project and the specified region support that feature. Contact Google Cloud Support for further help. Experiments enabled for project: [enable_streaming_engine, enable_windmill_service, shuffle_mode=service], experiments requested for job: [use_runner_v2]"",`
I used `--experiments=use_runner_v2` without `--streaming` being set.";non_debt
Run Java KafkaIO Performance Test;non_debt
the column name changes with timezone, but what about the value? can you also check the result?;non_debt
Yes for sure, good point. Well, definitely if `ALTREP` ends up working out as expected, etc. we can reconsider, but at least for now, it would help to lower the version. To give you more background, a bunch of scripts in Spark rely on older versions of R, while it's possible to upgrade those clusters, it adds overhead that would be nice to avoid at least while we can.;non_debt
31006158-546 description-0;non_debt
This is going to crash updater with `illegal_docid` on any design doc.;non_debt
33884891-4091 review-228726712;non_debt
retest this please;non_debt
Added test cases for both.;non_debt
"AIRFLOW-1501 introduced GoogleCloudStorageDeleteOperator (https://github.com/apache/airflow/pull/5230)
This PR adds the operator to the integration docs.";non_debt
The C++/Python build ran in 31 minutes this time;non_debt
"I think it will be a little hard to write a good tests for `pathPrefix`. 
One way would be to intercept RestClient HTTP calls and check that correct prefix is set.";non_debt
@SolidWallOfCode , it should be a simple addition. Can you please review this?;non_debt
"All automated tests passed.
Refer to this link for build results: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/14575/";non_debt
The evaluation order of these filters must be the same? If the orders are different, they are still the same, right?;non_debt
"@max3163 , are you using GraphiteBackendListener or something else (I don't fully understand your comment on ""I'm thinking to send a PR to share it too"".
Thanks";non_debt
[SPARK-30481][CORE] Integrate event log compactor into Spark History Server;non_debt
OK;non_debt
Opened #7660 for 8.1.x.;non_debt
"@jablko --
This is reproducible for me with two different Ubuntu 14.04 LTS server 64-bit VMs. The OS OpenSSL is 1.0.1f, and I have compiled and installed OpenSSL 1.1.0f into $HOME/opt/openssl.
Result is:
Additional info:
The -rpath or -R containing $HOME/opt/openssl is NOT contained in the libtool command above so the linking fails. If I manually run the above command adding the -R then it works. So I added the -R via the change to the Makefile.am.";non_debt
LGTM;non_debt
"this would mean that if you have your running application accessing different namespaces and you want to add a new namespace to connect to, if you just add the namespace you need the application can break as we are not getting anymore the tokens for the other namespaces.
I'd rather follow @jerryshao's comment about avoiding to crash if the renewal fails, this seems to fix your problem and it doesn't hurt other solutions.";non_debt
Hmm... there are test failures in REST API tests. Could you take a look at those @akalash ?;non_debt
`#' @family subsetting functions`;non_debt
34864402-5430 description-0;non_debt
LGTM;non_debt
Probably one small section that lists the current limitations will be useful. The one which is probable worth highlighting is about the container image used by the interpreter other than spark. It will be the same as Zeppelin container image.;non_debt
"@nazeer1100126 In regards to two-factor authentication, I was thinking something along the lines of using global configurations or a configuration endpoint just for two-factor. Ultimately the goal of this PR is to make the SMS & Email more generic so they can be used not only for login email and sms campaigns.
As for the SMS service, setup of tenant & sms bridges in the message gateway directly from Fineract is part of my GSoC proposal but it should be discussed further.";non_debt
can you rebase the pr to solve the conflict? namely https://github.com/dmlc/mxnet/blob/master/docs/community/contribute.md#submitting-a-pull-request;non_debt
Fix python functions;non_debt
We generally transition when we enqueue the relevant request, and we enqueue the abort and init at the same time. To my mind that means we go straight from `ABORTING` to `INITIALIZING`, but I'm not wedded to it if you think going through `UNINITIALIZED` makes more sense.;non_debt
I will create another JIRA to support on-demand rule matching.;non_debt
Sure~;non_debt
retest this please;non_debt
STORM-1885. python script for squashing and merging prs.;non_debt
so if I understand this correctly, the key change here is the change from using different creation methods for bitmap results to using a unified `BitmapResultFactory` interface in order to facilitate better metrics collection of the bitmap operations. I think that makes some good sense overall, but I'm a bit unclear on what the future intentions of `BitmapResultFactory` are. Should there be specialty methods for every type of bitmap operation someone might want to do? How do you want to handle future additions to that factory?  Is there a way someone can have a custom `BitmapResultFactory` and not have it break in arbitrary future versions?;non_debt
add comment explaining why this should not be inside a lock. so that this is less likely to regress in future.;non_debt
"Its just that the message from the failure will not be the top exception so users will be:
failed to validate MyIface.class
...lines of stack trace...
actual validation failures
...some more lines of stack trace...";non_debt
Great work @roitvt;non_debt
"Hi, I noticed that there has a red title ðŸ¤” we will not use it.
Also, we have 40+ users and there have 25 users in this picture, we need all users.";non_debt
Why not just rename `body` to `response` in the first place?;non_debt
@phrocker Just wanted to confirm this was done before a final review.  I think it likely makes sense to get this one in before we merge #73;non_debt
this method has been removed;non_debt
"Make `curl` less verbose to avoid connection-related messages being mixed into the response from Recon.
https://issues.apache.org/jira/browse/HDDS-3958
Verified output in [acceptance test](https://github.com/adoroszlai/hadoop-ozone/runs/869256000):";non_debt
@tristantarrant may you check if I missed something on infinispan side;non_debt
Still want to revisit this.;non_debt
NIFI-7178 - Handle the case when schema is not available;non_debt
Getting linting error here saying component should be written as a pure function, but in stateless components 'this.props' can't be accessed;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/27536/
Test PASSed.";non_debt
@jan0sch flink 1.5 has a critical issue in scala-shell for yarn mode. I have fixed it in FLINK-9554, and will update this PR to 1.5.1 if it is released before this PR merging.;non_debt
: ) Just found one in `AstBuilder.scala`. Let me know if anything I still missed.;non_debt
what's the point of replacing with null ? that will raise NPEs for no reasons;non_debt
17165658-28085 review-407334754;non_debt
"I'm not sure if the Jenkins job is running distributed or in one slave. If it's the later case, I think it should be bounded by max number of cores which is four.
@jasonkuster can you provide more details?";non_debt
"1. remove some logs on critical path
2. register ""TimestampedValue"" in Kryo to reduce the serialized size of event value
---";non_debt
I've fixed it.;non_debt
hmm, I thought I did.  Maybe I didn't commit;non_debt
Yeap...;non_debt
CAMEL-9166 for 2.15.x;non_debt
Why do we want to remove this check?;non_debt
Yessir!  Fixed;non_debt
Yes. I've added it.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/5022/;non_debt
does this reference need to be mutable?;non_debt
Did you look into the possibility of having a single validateIndexingConfig method?;non_debt
Only use Since annotations for public APIs;non_debt
Thanks a lot for the review @tillrohrmann . I've updated PR and verified it with Yarn 3.1.0.;non_debt
Yes, we need to do iniilization.;non_debt
Why not support other function definitions?;non_debt
This is a new feature in the protocol version 3.16.0.;non_debt
hmm. how about just calling toString, which queries them anyway -doesn't it?;non_debt
do you means: static final ImmutableMap<FunctionDefinition, Function<CallExpression, OrcSplitReader.Predicate>> FILTERS ?;non_debt
right they may choose an indexing approach based on a strategy (the names indicate algorithm/metric, but could be anything), or NONE. That can be useful for calculating vector score only for the purpose of ranking hits matched by other means.;non_debt
No this is what was supported in the old SqlParser.;non_debt
add Eclipse's .apt_generated_tests/ directory to .gitignore;non_debt
Treat 0 byte files as an empty ORC file with schema of struct<>.;non_debt
"Hey Vince,
Thanks for reviewing the pull request.
I knew about the license problem regarding project MultiAxisChartFX but we'll need to create a new JIRA to address this, instead of reimplementing the logic within GEODE-342, don't you think?. We could start doing that once the build process is standardized through gradle. Please let me know.
Cheers.";non_debt
Thanks for the new PR. It has been merged. Do you mind closing this?;non_debt
"looks great.  really nice tests.
a few minor comments, that's all.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/trafficcontrol-PR/4045/
Test PASSed.";non_debt
Add label support in include_example.;non_debt
very slick;non_debt
retest this please;non_debt
17971138-426 review-26902968;non_debt
31006158-6509 review-279779959;non_debt
Resources are auto closed within try catch block.;non_debt
19961085-6451 review-559757218;non_debt
I merged. ðŸ‘Œ Could you please close this PR?;non_debt
ok to test;non_debt
Use `.. note::`?;non_debt
I am not seeing Java precommit tests are triggered. I am less familiar on how does that is enabled. Guessing have to set at here: https://github.com/apache/beam/blob/master/build.gradle#L132;non_debt
`groupingSetExprs.flatMap(_.asInstanceOf[GroupingSet].groupingSets)` -> `groupingSets.flatMap(_.groupingSets)`?;non_debt
"@aledsage Thank you for the elaborative comments!
I fixed the addressed issues and added new config for powershell commands for: post-install, pre-launch, post-launch
You can review it again now.";non_debt
Thanks, merged to master.;non_debt
Tested it out and it works very well!;non_debt
Upgrade Gradle to 4.7;non_debt
"Why primitive tuples only?
` CREATE TABLE collect_further_things (   k int PRIMARY KEY,   v tuple<set<int>, map<int,set<float>>, float> )`";non_debt
"nit: offline ""duration"" for consistency";non_debt
"For #3602 .
Changes proposed in this pull request:
- Refactor SyncTaskController
- Refactor LogManager
- Refactor DataSyncTask";non_debt
Points;non_debt
This won't work if there is no alias to table. if there no alias then it should be passed as none;non_debt
"It seems to me we could simply call `Files.newOutputStream(restoreFilePath)` here, which perfectly matches the ""overwrite"" semantic.";non_debt
"Ping @JingsongLi for review.
@fhueske It'd be great if you can have a look too.
Thanks.";non_debt
Use single SQL to load index names for MySQL meta data loading;non_debt
That is weird - you can see the use of SPARK_JAVA_OPTS just a few lines above in the patch you submitted.;non_debt
If this is supposed to return zero rows, would a more obvious false where clause be more descriptive? Something like `where 1 = 0`? The other examples above are easy to reason about as they have a static query that doesn't reference a table.;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/988/;non_debt
"I had to redo most of the formatting after adding the ""no empty lines before comments"" setting, the previous run had damaged the formatting in a way that it could not be fixed even with this config change.";non_debt
Is it going to update in the PR description but forgotten?;non_debt
shading dependencies in pulsar client;non_debt
+1, failure is due to Plasma, I was on a slightly too old master. Not relevant for this PR.;non_debt
Packaging result: âœ”centos7 âœ”centos8 âœ”debian. JID-2169;non_debt
Changed author to MalharJenkins.;non_debt
Already added.;non_debt
A bit curious about how topi calls are collected during optimizationï¼Œis it through certain pass?;non_debt
Forgot to stop the ```StreamingContext``` added in ```KinesisDStreamBuilderSuite```. Updated the code to stop the context after all tests have run.;non_debt
Note that this and other classes for read/write are Experimental;non_debt
io.vertx.core.http.HttpClientOptions#DEFAULT_TRY_USE_COMPRESSION;non_debt
"... not called by multiple threads at a time
See https://issues.apache.org/jira/browse/STORM-2184";non_debt
"isn't it better to throw UnsupportedOperationException ?
also, if I retrieve only a subset of the data, how do I know that there would be more ranges ?";non_debt
cc @amaliujia;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2669/;non_debt
Performance info is not an error, change it to debug info;non_debt
"This PR contains the following updates:
---
:date: **Schedule**: At any time (no schedule defined).
:vertical_traffic_light: **Automerge**: Disabled by config. Please merge this manually once you are satisfied.
:recycle: **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.
:no_bell: **Ignore**: Close this PR and you won't be reminded about this update again.
---
---
This PR has been generated by [WhiteSource Renovate](https://renovate.whitesourcesoftware.com). View repository job log [here](https://app.renovatebot.com/dashboard#github/apache/fineract).";non_debt
2211243-7848 description-0;non_debt
Document the Watson speechToText action;non_debt
206370-131 description-0;non_debt
"# [Codecov](https://codecov.io/gh/apache/arrow/pull/2314?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/arrow/pull/2314?src=pr&el=continue).";non_debt
"Fixes #9617 
https://github.com/apache/druid/blob/master/dev/committer-instructions.md#pr-and-issue-action-item-checklist-for-committers. -->
In each section, please describe design decisions made, including:
 - Choice of algorithms
 - Behavioral aspects. What configuration values are acceptable? How are corner cases and error conditions handled, such as when there are insufficient resources?
 - Class organization and design (how the logic is split between classes, inheritance, composition, design patterns)
 - Method organization and design (how the logic is split between methods, parameters and return types)
 - Naming (class, method, API, configuration, HTTP endpoint, names of emitted metrics)
-->
This PR has:
 kafkaEmitterConfig
PS: We built a custom jar and have been using the custom emitter jar in our production cluster. It works well with ssl enabled Kafka.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/466/
Test PASSed (JDK 8 and Scala 2.11).";non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/7909/;non_debt
It is packaged locally. Run the command you put in the CI control file, all things run automatically on MacOS and Linux.;non_debt
The case will not happen because the node will not be translated to WindowJoin if `WindowSpec` is different.;non_debt
"Math.abs(hash) can give negative result? Sounds strange to me.
bitmask is dangerous because it assumes the var is 32 bits. In case someone changed the data type it can cause other problem.";non_debt
@borisstoyanov a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.;non_debt
@dpcollins-google - Would you be able to update based on the comments?;non_debt
Bump Testcontainers to version 1.14.1;non_debt
ARROW-12051: [GLib] Keep input stream reference of GArrowCSVReader;non_debt
"This change looks non-risky to me.
cc @swoen @HyukjinKwon";non_debt
cc @marmbrus;non_debt
Can we put `@Deprecated` annotation instead of removing the function?;non_debt
ok;non_debt
retest this please;non_debt
"The original `AwsOptions` has a flat `region`, which is in conflict with the one in `DataflowPipelineOptions`
This PR renames the flag, matching the ver1 sdk (io.aws).
------------------------
Thank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:
See the [Contributor Guide](https://beam.apache.org/contribute) for more tips on [how to make review process smoother](https://beam.apache.org/contribute/#make-reviewers-job-easier).
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Dataflow | Flink | Samza | Spark | Twister2
--- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Go_VR_Spark/lastCompletedBuild/) | ---
Java | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Java11/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Java11/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Flink_Streaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_PVR_Spark_Batch/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_SparkStructuredStreaming/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Twister2/lastCompletedBuild/)
Python | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python36/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python37/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python38/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_VR_Dataflow_V2/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_PVR_Flink_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_Python_VR_Spark/lastCompletedBuild/) | ---
XLang | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Direct/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Dataflow/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Flink/lastCompletedBuild/) | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PostCommit_XVR_Spark/lastCompletedBuild/) | ---
Pre-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
--- |Java | Python | Go | Website | Whitespace | Typescript
--- | --- | --- | --- | --- | --- | ---
Non-portable | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Java_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Python_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonLint_Cron/lastCompletedBuild/)<br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocker_Cron/lastCompletedBuild/) <br>[![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_PythonDocs_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Go_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Website_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Whitespace_Cron/lastCompletedBuild/) | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Typescript_Cron/lastCompletedBuild/)
Portable | --- | [![Build Status](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/badge/icon)](https://ci-beam.apache.org/job/beam_PreCommit_Portable_Python_Cron/lastCompletedBuild/) | --- | --- | --- | ---
See [.test-infra/jenkins/README](https://github.com/apache/beam/blob/master/.test-infra/jenkins/README.md) for trigger phrase, status and link of all Jenkins jobs.
GitHub Actions Tests Status (on master branch)
------------------------------------------------------------------------------------------------
See [CI.md](https://github.com/apache/beam/blob/master/CI.md) for more information about GitHub Actions CI.";non_debt
https://github.com/bkietz/arrow/runs/1670186993 shows a couple of other warnings still, though they're not related to the recent dataset work;non_debt
Test fixed and extended. Added notes to GridToStringBuilder about infinite looping in additional values.;non_debt
39464018-3346 review-134524525;non_debt
WIP MINOR: move ZK ACL lookup outside of inWriteLock in AclAuthorizer;non_debt
Check added;non_debt
Why not use Arrays.asList()?;non_debt
Yeah that looks sound.;non_debt
"@doanduyhai 
Also i'd like to know if you're planning to add more commit here for Java to Scala and some improvements. Or if you prefer to merge it first and make new pull request for further improvements.";non_debt
[SCB-1917]add a test case for testing provider invoke it's own servicâ€¦;non_debt
/pulsarbot run-failure-tests;non_debt
I don't think it makes sense to tie this to WriteAheadLogFileSegment. On one hand the naming HDFSBackedBlockRDD is supposed to be general, on the other you tie it to recovery through the use of WriteAheadLogFileSegment.;non_debt
Everything at the top of this chunk that's removed is included in the new bit (except for bootstrap);non_debt
0 means that it will retry 1 time.  I didn't change the current difficult to understand behavior (see above @maskit comment).;non_debt
"Tuple types can be returned by the KeySelector. Enabling tuple types to be used as KEY for grouping.
Added test cases using TupleComparatorTestBase. Testing Tuple3<Tuple2,Tuple2,Tuple2>.";non_debt
"The use of 'zookeeper' as the Livy recovery store does not seem to be supported by Ambari. The Ambari script is trying to create a HDFS directory with the value of livy2-conf/livy.server.recovery.state-store.url which is actually a Zk quorum. Ideally, it needs to look at livy2-conf/livy.server.recovery.state-store and only create the directory if it is 'filesystem'. In this case, the store is 'zookeeper'.
Manually tested.";non_debt
I can happily report that the iterator is the only issue and the build passes locally for me when I add a `const` at the end of the declaration and to all implementations.;non_debt
I think the best way to verify is by checking that the test fails if we remove `ignoreResiduals()` from the action.;non_debt
merged;non_debt
Decouple LogicSQL and schema;non_debt
"spark.lda passes the optimizer ""em"" or ""online"" as a string to the backend. However, LDAWrapper doesn't set optimizer based on the value from R. Therefore, for optimizer ""em"", the `isDistributed` field is FALSE, which should be TRUE based on scala code.
In addition, the `summary` method should bring back the results related to `DistributedLDAModel`.
Manual tests by comparing with scala example.
Modified the current unit test: fix the incorrect unit test and add necessary tests for `summary` method.";non_debt
Yeah I can look into that. Maybe we should create an issue for this so that we agree on the how (terminology, objects, location in the values.yaml) before starting the dev? Wdyt ?;non_debt
34864402-14534 review-271872150;non_debt
"I assume you had also tried periodic watermarks in your setting. I'm curious why they didn't work for you. 
A periodic watermark assigner extracts the timestamp from each record and just emits a watermark when it is asked. From a correctness point of view, this should be the same as punctuated watermarks, just with lower overhead and higher watermark latency.";non_debt
"The `factory()` returns a `Future` which means it needs an `ExecutionContext`. Apparently, the `factory` passed in the class constructor has no `ExecutionContext` in its parameter list. Which `ExecutionContext` will it use?
I would expect that it uses the same `ExecutionContext` as the `ContainerProxy` which is an actor - but `ExecutionContext` is wired in `factory`.";non_debt
KAFKA-7235 has been merged. Does that mean that this is unblocked or is there more to be done?;non_debt
ARROW-3770: [C++] Validate schema for each table written with parquet::arrow::FileWriter;non_debt
"This PR adds more comments for ndarray operators and refactors ndarray API docs.
A preview is available at http://ec2-54-152-159-175.compute-1.amazonaws.com///api/python/ndarray.html
The refactor consists
1. operators that should be deprecated are removed from the summary list, including
2. we are agreed to use lower cases for array operators, so several alias are added.
3. bug fix for `rint` and `fix`. 
4. updated mxnet.css
For example, we used several input names including
we can use either 1) `data`, or `data0`, `data1` for multi-input, or 2) `a`, `b` to stay the same with numpy
Also we used `axis`, `axes`, `dim` to refer to dimensions. Suggest to use `axis` for single dimension and `axes` for multi-dimensions. 
some additional changes include `ret_typ` -> `ret_type` or `rtype` for `topk`.
1. comments for neural network layers
2. improve `symbol.md`";non_debt
Maybe we should just say segment deletion? We can also delete segments during recovery and such. Could that not trigger this path?;non_debt
"# [Codecov](https://codecov.io/gh/apache/apisix-dashboard/pull/1571?src=pr&el=h1) Report
Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/apisix-dashboard/pull/1571?src=pr&el=continue).";non_debt
Use {} for formatting.;non_debt
I don't think it is required to have a separate constructor, if (dictionary == null && !isDirectDictionary) then it becomes nodictionary;non_debt
Yes, I will review tomorrow.;non_debt
Thanks @Zhijiang. I agree with you that a shuffle manager will be a better solution for this issue in Flink 1.8. If more users request this before they can upgrade to Flink 1.8, we could still merge this PR to Flink 1.5~1.7 branch.;non_debt
changed;non_debt
"I don't think we should be calling ToUniversalTime() here. Lets keep this exactly what we get from the client? I think we should not get into the timezone business here. If it is absolutely necessary then we can consider changing LastModifiedTime in FileStatus to DateTimeOffset.
Thoughts?";non_debt
Actually an identifier of the shuffle itself, not the stage, right? If you reuse a shuffle, you get the same shuffle id, but different stage id.;non_debt
"@leventov, in reply to:
This patch won't preserve extension compatibility anyway, since Query gained `withQueryMetrics` but BaseQuery doesn't provide a default implementation.
But also: is there a nice migration path from this change now, to something that would be a ""better"" design for 0.11.0? From your discussion with @himanshug, it sounds like there isn't, and in 0.11.0 we'd just want to remove these methods we're adding now and replace them with something else. I think if that's the case, it's fine to make the ""better"" change now and have the next release after 0.10.0 be 0.11.0.";non_debt
this is in a separate section called additional firehoses;non_debt
[CXF-7374] Fix for jpa refreshToken writeLock;non_debt
I have changed it. :);non_debt
CAMEL-8640 on Camel 2.15.x;non_debt
@BasPH PTAL https://github.com/apache/incubator-airflow/pull/4421;non_debt
Folks, post the discussion above, I am assuming this is ready to merge?;non_debt
I'll do that in a follow-up so it's easier to test.;non_debt
I missed the fact that `mergeFinished` is executed under IndexWriter lock. I will dig into this again. Please ignore my previous comments.;non_debt
don't skip. Mock should be present;non_debt
I had to rebase the branch to fix merge conflicts, but not all feedback/requests for changes have yet been applied.;non_debt
I've moved it into the test class where it is used. I would still like to leave the alteration of `DataflowRunner` out of this PR, since my other PR that just adds a `checkState` illustrates that the `DataflowRunner` batch view overrides result in a corrupted graph that sort of works by luck. I don't want to disturb that potentially sensitive situation.;non_debt
Why these values need to be atomic?;non_debt
Good catch. I remember the locking here had a difficulty when I was doing development but @dongjoon-hyun added a test for it so I'll try and minimally scope the lock and make sure it passes.;non_debt
Packaging result: âœ”centos6 âœ”centos7 âœ”debian. JID-2451;non_debt
HADOOP-16794 S3 Encryption keys not propagating correctly during copy operation;non_debt
"That column name is wrong now though -- I wonder if it is worth updating/renaming that to ""last_parsed_time"" or ""last_seen_time""?";non_debt
Thanks for the contribution. I don't think this will work on GPU though as this kernel is shared by CPU and GPU..;non_debt
"Thanks @hepeknet ,
In case you want to contribute another AutoValue PR, HBaseIO does not have autovalue, you can create a JIRA/PR if you feel like doing this too.";non_debt
"This PR aims to remove Python 2 test case from K8s IT.
Since Apache Spark 3.1.0 dropped Python 2.7, 3.4 and 3.5 support officially via SPARK-32138, K8s IT fails.
No.
Pass Jenkins K8s IT.";non_debt
Got it~;non_debt
99919302-1694 comment-524725582;non_debt
@ctubbsii , sorry I little confused again.  I guess I didn't reread this new ticket referenced.  I was working off of accumulo-proxy.  Do you want me to do all of this in accumulo-proxy#3?  OR if accumulo-proxy is what it the goal and you are updating it then I can just refork or pull after your update and pull things together.;non_debt
These values should be configurable. Thus they should be definable in the `flink-conf.yaml` file.;non_debt
"Nooooooo `EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]` still had one failure on the Java 8 build ðŸ˜­ 
But it failed on a different line which seems more in line with real flakiness. FWIW I ran this locally 40 times without failures (technically 80 in total for both true/false variations) ... I think it's worth still merging this PR and we can continue investigating it from there.";non_debt
@rafaelweingartner I couldn't test the view with having existing remote vpns configured. But my assumption was that the api is working identically as other list apis returning the results of the current owner.;non_debt
Run Java Postcommit;non_debt
I see DataFrameReader and DataFrameWriter have been already mixed up. I won't insist on `DataStreamReader.table` as `DataFrameReader.table` has been living from 1.4.0, but I still don't agree we should have another trigger entry except `start()` for `DataStreamWriter`. That was a mistake and this is a great chance to fix before we release.;non_debt
[iOS] Rename the backgroundColor property name.;non_debt
Perhaps it would be more palatable if, instead of exiting, we just log a warning when the wait time expires and not all queries have been matched?;non_debt
I think enough +1's between here and dev thread, going to merge;non_debt
@jkbradley;non_debt
Never mind. I had to close the pull request. I thought about it. The ccAccumulator is not accessible from the vprog which was my goal. I'm going to have to use a broadcast. I will have an update for tomorrow.;non_debt
50 line, two for loops and quadruple nested blocks. please dissect.;non_debt
Why `transformDown`? Wouldn't `transformUp` allow us to resolve most outerReferences in one pass?;non_debt
not needed?;non_debt
SDV Build Fail , Please check CI http://144.76.159.231:8080/job/ApacheSDVTests/5882/;non_debt
Two simple backports to 3.0.x to make life friendlier for operators and devs. These won't be mentioned in the relnotes.;non_debt
That sounds good. We also need suggestion from @omalley about the coding style.;non_debt
Update index.asciidoc;non_debt
"Prevent the duplication of encode/decode cycle in wasted call to groupByKey in spark streaming. It is a waste because after the groupByKey is a call to updateStateByKey which will ensure that the data is shuffled to the correct processing location.
------------------------
It will help us expedite review of your Pull Request if you tag someone (e.g. `@username`) to look at it.
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) </br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_VR_Flink/lastCompletedBuild/) | --- | --- | ---";non_debt
chore(deps): bump animal-sniffer-maven-plugin from 1.18 to 1.19;non_debt
"""Check to ensure that the local version of the Heron topology compiles by deleting your local `~/.heronapi`, updating the project `pom.xml` file, and pulling from Maven Central""";non_debt
Agreed. We always merge by squashing the entire PR into a commit, so we do get a linear history in master.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-gobblin/pull/2867?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-gobblin/pull/2867?src=pr&el=continue).";non_debt
See below.;non_debt
Would BucketTimeMapper be a more appropriate name?;non_debt
@sgolovko Are the latest changes in?;non_debt
@vexilligera as discussed offline, lets try testing locally for WIN_GPU and WIN_GPU_MKLDNN build 10 times each (since 1 run takes 20-30mins) to come up with some basis... (ideally would have tried 100 times but given the resource & time constraints);non_debt
@prakashpc I actually fixed the packages thing in #7022 -- I'll try out the file separator fix today and get back to you.;non_debt
@JoshRosen @rxin Hi folks - any chance of getting a review? Thanks!;non_debt
[SPARK-11629] [ML] [PySpark] [Doc] Python example code for Multilayer Perceptron Classification;non_debt
"The analogy between '*' and breadthFirst() traversal seems wrong. The former only traverses one level, while the latter searches through the next levels as well. See http://stackoverflow.com/questions/42985716/groovy-xml-tree-traversal-breadthfirst-method-using-as-syntactic-sugar:
    def books = '''\
    def response = new XmlSlurper().parseText(books)
    def bk = response.'*'.find { node ->
       node.name() == 'book' && node['@id'].toInteger() == 2
    }
    assert bk.empty
    def books = '''\
    def response = new XmlSlurper().parseText(books)
    def bk = response.breadthFirst().find { node ->
       node.name() == 'book' && node['@id'].toInteger() == 2
    }
    assert bk.title == 'bar' // bk is no longer an empty list of children";non_debt
"As it turns out any of the tests using tcp_client.py or netcat assume the half open semantics.
The slice plugin also does shutdown read and shutdown write.  I think that is probably not what you want to do since it limits you to only one request from client connection.  I removed the calls to TSVConnShutdown here.";non_debt
This is a private method and has its unit functionality. While being called by only one place, I don't see any harm to keep it this way.;non_debt
has updated as your comments, @tqchen please review again;non_debt
Ensures that the client configuration is set on the SQS Client Builder when it's present.;non_debt
"This can fall through because the constructor supports version 0 and 1. The version is passed as the last parameter. 
This is related to the discussion here: https://github.com/apache/kafka/pull/1095#discussion_r60758124";non_debt
Makes sense. The fact that storage status list isn't updated is a bug I forgot to add  super.onTaskEnd to JobProgressListener. However, this will no longer be relevant in the new proposal, which involves JobProgressListener extending the usual SparkListener as you suggest, and listening for the trimmed ExecutorStateChange events. (More details in discussion above);non_debt
@borisstoyanov unsupported parameters provided. Supported mgmt server os are: `centos6, centos7, ubuntu`. Supported hypervisors are: `kvm-centos6, kvm-centos7, kvm-ubuntu, xenserver-65sp1, xenserver-62sp1, vmware-60u2, vmware-55u3, vmware-51u1, vmware-50u1`;non_debt
KAFKA-4555: Using Hamcrest for expressive intent in tests;non_debt
Ah, sure, that's ok, sorry for the misunderstanding. I think it is implicit, but it's ok to have it anyway.;non_debt
@wesm Does this affect the OSX packages (conda, wheel)? Should We run packaging tasks to test it?;non_debt
[AIRFLOW-5499] Move GCP utils to core;non_debt
retest this please;non_debt
"https://github.com/apache/helix/issues/469
Building helix-front is failed because node_modules/@types/lodash/common/collection.d.ts(1783,24): error TS2304: Cannot find name 'Exclude'.
 types/lodash module uses `^4.14.71` which would get the latest release 4.14.138 and the version doesn't have ""Exclude"". Downgrading the module version to a fixed version 4.14.120 is the easiest solution for now.
NO new test added.
mvn test doesnâ€™t run in helix-front as it doesn't have maven tests.
mvn build result:
[INFO] Reactor Summary for Apache Helix 0.9.2-SNAPSHOT:
[INFO]
[INFO] Apache Helix ....................................... SUCCESS [  1.137 s]
[INFO] Apache Helix :: Core ............................... SUCCESS [ 25.523 s]
[INFO] Apache Helix :: Admin Webapp ....................... SUCCESS [  3.209 s]
[INFO] Apache Helix :: Restful Interface .................. SUCCESS [  6.165 s]
[INFO] Apache Helix :: HelixAgent ......................... SUCCESS [  1.370 s]
[INFO] Apache Helix :: Recipes ............................ SUCCESS [  0.044 s]
[INFO] Apache Helix :: Recipes :: Rabbitmq Consumer Group . SUCCESS [  0.782 s]
[INFO] Apache Helix :: Recipes :: Rsync Replicated File Store SUCCESS [  0.915 s]
[INFO] Apache Helix :: Recipes :: distributed lock manager  SUCCESS [  0.660 s]
[INFO] Apache Helix :: Recipes :: distributed task execution SUCCESS [  0.602 s]
[INFO] Apache Helix :: Recipes :: service discovery ....... SUCCESS [  0.608 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  08:03 min
[INFO] Finished at: 2019-09-12T11:11:42-07:00
[INFO] ------------------------------------------------------------------------
Subject is separated from body by a blank line
Subject is limited to 50 characters (not including Jira issue reference)
Subject does not end with a period
Subject uses the imperative mood (""add"", not ""adding"")
Body wraps at 72 characters
Body explains ""what"" and ""why"", not ""how""
Documentation";non_debt
"grammar error for ""The answer definitely YES""";non_debt
I would also note that a workaround for the AWS EKS issue is that user can first manually log in with AWS and then pass the OAuth token explicitly to `spark-submit` i.e.;non_debt
"shall we apply it to all numeric types?
since we don't know the value of the string, using double seems safest to avoid overflow.";non_debt
ping @JoshRosen, I think this should be good to go now;non_debt
Yes, but only if the user clicks on the Metrics tab.  Do we want it to automatically select the Metrics table when the user selects forecast?;non_debt
[FLINK-21836][table-api] Introduce ParseStrategyParser;non_debt
@nlivens I don't care for more scripts, just for a test showing the problem so we know it doesn't regress. Whatever meets the goal easiest.;non_debt
" * ACE 6.5.8
 * SQLite 3.31.1
 * Xerces-C 3.2.3";non_debt
"instead of using env var changed it to read from config
if user defines storm.log.dir log files goto that particular dir otherwise it will be under storm.home/logs";non_debt
That's going to be turned on, right?;non_debt
Build Failed with Spark 2.2.1, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/5260/;non_debt
"@ahgittin Thanks for the suggestions.
It was in the `CreateUserPolicy` at the beginning. I've moved it to `SshMachineLocation` so it is backward compatible as much as it can. It guarantees that customers will make no changes at their end.
I will address your comments taking into account that customers will have to change theirs BPs, because of the package renaming";non_debt
@frreiss Thanks for explanation!;non_debt
!inputColumn.isNull[j] can be merged with the above condition;non_debt
I have decided that #416 is a better fix than this one.;non_debt
[CALCITE-4104] Add automatically link to GitHub PR and 'pull-request-available' label to issues;non_debt
/pulsarbot run-failure-checks;non_debt
/retest;non_debt
R: @robertwb;non_debt
/cc @andrewor14 for review.;non_debt
"I see that setTokenRefreshKeytab method is used in testing..
Looks good to me ðŸ‘";non_debt
:+1:;non_debt
@jai1 this conflicted with my PR, please rebase;non_debt
"Dear Airflow Maintainers,
[Master branch](https://travis-ci.org/apache/incubator-airflow/jobs/201242728): Ran 256 tests in 819.435s
[This branch](https://travis-ci.org/apache/incubator-airflow/jobs/201335186): Ran 360 tests in 912.790s
It also fixes a few subtle bugs not captured by the already enabled tests.";non_debt
Since these are just string comparisons there's probably no need to do any numeric conversions.;non_debt
Fixed bug in suggested InfluxDB config;non_debt
"I thought that also before I made this change, but I don't think that the strong reason we should stop this change.
For most of cases in the code, we returns the same references by well-design the `rules` for a `TreeNode` object, and if the code still keep creating the identical objects(`.equals` returns `true`) in its rule for every iteration, even unnecessary, can this be considered as a bug of the user code? 
I think it's the responsibility for user code to decide whether `TreeNode` object substitutions should be taken (via creating new instance), as user code always knows when a object substitution needed, right? That's also give more freedom for user code to define the `.equals()` in a semantic way for `TreeNode` object.";non_debt
Good catch! Will push a commit removing that to my branch;non_debt
"fixed missing gz extension from wikiticker json quickstart example
Without this fix running the quickstart will fails with the exception
2017-12-24T19:02:25,097 ERROR [task-runner-0-priority-0] io.druid.indexing.overlord.ThreadPoolTaskRunner - Exception while running task[HadoopIndexTask{id=index_hadoop_wikiticker_2017-12-24T19:02:12.487Z, type=index_hadoop, dataSource=wikiticker}]
...
...
Caused by: java.lang.RuntimeException: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: .../quickstart/wikiticker-2015-09-12-sampled.json
...";non_debt
What is the difference between multiple calls to activate() and calling reactivate()? Why will existing operators break? Will not activate() be called on the same instance multiple times only for operators marked as REUSE_INSTANCE and there are no such operators?;non_debt
Fix endpoint grouping bug;non_debt
*Execute docker/setup_demo.sh in any directory*;non_debt
Enable pylint rule to check for missing commas;non_debt
Build Failed  with Spark 2.3.1, Please check CI http://136.243.101.176:8080/job/carbondataprbuilder2.3/8872/;non_debt
can you add cases for lower/upper being null;non_debt
You are surely welcome to look for more javadoc issues to fix, there is plenty of Camel components );non_debt
This is actually the only change I seemed to need to make to the Spark javascript.;non_debt
Thanks for the hints!;non_debt
[SPARK-21913][SQL][TEST] `withDatabase` should drop database with CASCADE;non_debt
you are right;non_debt
Thanks for the tip. I've added the frequently updated counters as counter ref.;non_debt
Added some description about it.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/camel-quarkus-pr/69/";non_debt
Yeah, I think we discussed moving the MLlib driver programs to examples.;non_debt
Ports @aledsage's Troubleshooting blog post to docs;non_debt
Now includes @rabbah's change per @lehoanganh's suggestion.;non_debt
ok;non_debt
Closing this patch since we have #7928 open.;non_debt
Whoops, thanks for catching that.;non_debt
retest this please;non_debt
There is a possibility of losing the data from the fact if you set only segments for fact and read the streaming segments directly from fact streaming segments, in case of handoff streaming segments would be marked for delete and those won't be accessible during fact read. So set all the current fact segments also here along with aggregate segments. During fact read,compare the current segments and set segments and read all intersected segments;non_debt
Retest this please.;non_debt
As you said keeping the DEFAULT prefix to highlight the default behaviour it is assumed that there are non default behaviours associated with this feature, are there any?;non_debt
"Make this
""druid.discovery.curator.compress""";non_debt
@lukecwik thanks for the suggestions!;non_debt
[TRAFODION-2657] Remove Automating Update Statistics for SQL Reference Manual;non_debt
"Semantics of this have changed, but I'm not seeing conversation that indicates that it was intentional.
Before: we would stop all workers, then wait for them all to be stopped. Each worker could stop itself concurrently. Now, for each worker, we request a stop and then wait for it to be stopped, then move on to the next worker.
I don't _think_ this is a big deal, but wanted to call it out.";non_debt
We don't use v3.3;non_debt
We have test suites for the overflow behavior. I don't think it makes sense to test overflow again in all the functions that take integers.;non_debt
"Created .bat script to start Flink task manager.
Uses scripts flink-console.bat and flink-daemon.bat - see PRs https://github.com/apache/flink/pull/6559 and https://github.com/apache/flink/pull/6552";non_debt
#4463 FAQ for Maven compilation failure with error like python2 not found;non_debt
After thinking deeply, I agree with you. Following the checkpoint's sequence is a better choice. Accept.;non_debt
Yes, It is missing in all releases. It can be backported to 4.6.;non_debt
Test Failed.  https://jenkins.esgyn.com/job/Check-PR-master/674/;non_debt
It would require the option parsing to be repeated, first to get the options w/o runner options (where unknown runner options are ignored), then send those to the job service, then repeat parsing with runner options present. I think the return of that isn't worth the extra work and I would like to leave it out of this PR.;non_debt
[Doc] Fix repo paths in Ubuntu build doc;non_debt
Trim query before parsing;non_debt
[approve ci];non_debt
I put SameMajorVersion only because it's often used, but if I understand correctly you'd prefer ExactVersion? The choice is currently AnyNewerVersion|SameMajorVersion|SameMinorVersion|ExactVersion, see https://cmake.org/cmake/help/v3.0/module/CMakePackageConfigHelpers.html;non_debt
Could you change `T` to something more clear like `tries_per_work_item`?;non_debt
private;non_debt
Do we need this file? Presumably not if everything is commented out.;non_debt
done;non_debt
@markap14 understood and agree. I'm +1 on this.  Will also look at NIFI-5222 as well now;non_debt
33884891-1682 comment-234900191;non_debt
[KARAF-6373] Upgrade to CXF 3.3.2 to build example on JDK 11;non_debt
Deprecating the APIs in the major/feature releases, like 2.4.0, 2.3.0, 2.2.0, is common but deprecating the APIs in the maintenance releases, like 2.2.1, 2.2.2, 2.3.4 is rare.;non_debt
"default should be 10 * 60 seconds
https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorConnectorConfig.java#L152";non_debt
Good catch!;non_debt
"Test PASSed.
Refer to this link for build results (access rights to CI server needed): 
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/21293/Test PASSed.";non_debt
Ok, I'll do. Then I'd suggest to do the same also in other places. I can check where an analogous pattern is used and create a PR if it is ok.;non_debt
"Do we no longer care if `ugi.hasKerberosCredentials()` when the case is KERBEROS?
I guess I was kind of expecting this to supplement the verification that the KERBEROS case was logged in, instead of replace it. But, if that's checked elsewhere (at RPC time?), then I guess it's fine.";non_debt
"CSV is the most common data format in the ""small data"" world. It is often the first format people want to try when they see Spark on a single node. Having to rely on a 3rd party component for this leads to poor user experience for new users. This PR merges the popular spark-csv data source package (https://github.com/databricks/spark-csv) with SparkSQL.
This is a first PR to bring the functionality to spark 2.0 master. We will complete items outlines in the design document (see JIRA attachment) in follow up pull requests.
Spark-csv was developed and maintained by several members of the open source community:
@dtpeacock: Type inference
@mohitjaggi: Integration with uniVocity-parsers 
@JoshRosen: Build and style checking 
@aley: Support for comments
@pashields: Support for compression codecs
@HyukjinKwon: Several bug fixes
@rbolkey: Tests and improvements
@huangjs: Updating API
@vlyubin: Support for insert
@brkyvz: Test refactoring
@rxin: Documentation
@andy327: Null values
@yhuai: Documentation
@akirakw: Documentation
@dennishuo: Documentation
@petro-rudenko: Increasing max characters per-column
@saurfang: Documentation
@kmader: Tests
@cvengros: Documentation
@MarkRijckenberg: Documentation
@msperlich: Improving compression codec handling
@thoralf-gutierrez: Documentation
@lebigot: Documentation
@sryza: Python documentation
@xguo27: Documentation
@darabos: License text in build file
@jamesblau: Nullable quote character
@karma243: Java documentation
@gasparms: Improving double and float type cast
@MarcinKosinski: R documentation
@addisonj: nullValue parsing";non_debt
You cannot do so?;non_debt
AssertJ will print out the entire result and why the assertion failed if you change to:;non_debt
retest this please;non_debt
Make a setter that takes a single String where the values are separated by comma so you can pass in the value in the same string. This makes it easier if loading values from properties/config/tooling;non_debt
Test results LGTM, the test_nic is a known intermittent issue probably due to env issue. Further investigation on the env issue that causes test_nic to fail is requested @andrijapanicsb @DaanHoogland @PaulAngus;non_debt
"http://pylint.pycqa.org/en/latest/technical_reference/c_extensions.html
We could add setproctitle to to the list, or disable this warning globally  c-extension-no-member?
Also perhaps ignored-modules=setproctitle may be needed";non_debt
BTW let me use the same fix that Sean mentioned. That will tighten the exclusion so that other methods are still checked by MiMA.;non_debt
@BryanCutler you are correct, for some reason I thought LargeList was coupled with LargeVarChar/LargeBinary.;non_debt
can we describe these fields in a table?;non_debt
"As suggested by @iilyak I added a docker helper script (`dev/docker-cli`), making it easier to orchestrate the docker build. It does the following at the moment:
Let me know, if this makes sense!";non_debt
Build Failed  with Spark 2.4.5, Please check CI http://121.244.95.60:12545/job/ApacheCarbon_PR_Builder_2.4.5/3209/;non_debt
@hillsp is also not keen on doing this silently;non_debt
Hi @mgaido91 , since we are going to have new releases for branch 2.3 and 2.4, do you know if this bug exists in 2.3/2.4 and shall we backport it? thanks!;non_debt
retest this please;non_debt
Hey @tardieu, does this work with web actions, and is there a test to ensure that an action composition can't be used to infinitely invoke a single action?;non_debt
[FLINK-3926][TypeSystem]Incorrect implementation of getFieldIndex in TupleTypeInfo;non_debt
@lukecwik thank you for the review. Before any follow-up changes are made, do you think you could share your thoughts in the discussion in the [JIRA ticket comments](https://issues.apache.org/jira/browse/BEAM-5495)?;non_debt
It's actually dim_t in nnvm. index_t is defined in mshadow. Could you please confirm?;non_debt
"Added after the for loop, do: node.client().admin().indices().prepareRefresh(ELASTIC_INDEX_NAME).get() 
Removed comment of sleep(1000)
Ran it to test in a loop for 10 times, works okay";non_debt
ping @bolkedebruin;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/beam_PreCommit_Java_MavenInstall/7078/
--none--";non_debt
I would not shadow the local variable. One could rename the variable `currentLastState`.;non_debt
end sentence after feed action with period. Start new sentence: The feed action is an ...;non_debt
HDDS-2677. Acceptance test may fail despite success status;non_debt
56750161-107 review-120233311;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/819/;non_debt
thanks, merging to master!;non_debt
function label .. does not reflect what the function does.. ie.. does not really print out the action code;non_debt
:+1:;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1010/
Test FAILed (JDK 7 and Scala 2.10).";non_debt
Hi guys :) any update on this?;non_debt
"Couple quick comments...
- With the new layout, we can probably remove some of the width restrictions (or at least increase the values).";non_debt
[Bug][MemTracker] Cleanup the mem tracker's constructor to avoid wrong usage;non_debt
160999-593 description-0;non_debt
Do you need this extra function, I think just inlining it should be fine.;non_debt
I think this is the last thing that should squeak into 0.11.0, I'll do the release immediately after it's merged.;non_debt
Could you explain the difference between split(sliceChannel) and split_v2 , and maybe show some performance compare between these two OP?   Thanks;non_debt
Thanks for reviewing and merging it, @ijokarumawak!;non_debt
ZOOKEEPER-3987: Reduce fork count for tests to 1;non_debt
[CARBONDATA-3791] Frequently Asked Question doc changes;non_debt
@remibergsma can you spare a bubble for this one?;non_debt
[BEAM-1165] Fix unexpected file creation when checking dependencies;non_debt
What is this change for?;non_debt
"There is not one single `ALTER TABLE` command, there are many (stopped counting at 20). The parser gives us pretty good trees to match on. For instance: `ALTER TABLE table_name UNSET TBLPROPERTIES ('comment', 'test')` gives us:
Lets split this code by matching on the `TOK_ALTERTABLE_*` tokens. The result should be alot easier to understand.";non_debt
"I tied to follow your instruction by doing:
- I created the same file
https://github.com/apache/drill/blob/master/exec/java-exec/src/test/java/org/apache/drill/exec/store/pcap/TestPcapDecoder.java
- I added at line 57 the code:
@Test
public void testMicrotQuery() throws Exception {
  runSQLVerifyCount(""select `timestamp_micro` from
dfs.`store/pcap/cap-test.pcap`"", 1)
}
where cap-test.pcap is the file I added in the pcap folder.
-Then I run the command
mvn -Dtest=TestPcapDecoder#testMicrotQuery test and I got the error:
Failed to execute goal
org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M2:test
Failed to execute goal
org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M2:test (default-test)
on project drill-protocol: No tests were executed!
First time running/writing unit test, so I will need help.
Thank you very much
Il giorno ven 1 mar 2019 alle ore 15:27 Charles S. Givre <
notifications@github.com> ha scritto:";non_debt
Should this be ||?;non_debt
Should we be referencing concrete model classes in migrations? This migration will likely fail to run in the future should any of their structure change. Ideally, partial impl's, or copies should be defined directly in the migration in order to avoid such a situation. The other option is to roll all of these migrations up into a subset of what we currently have.;non_debt
StressNewTestOpenJDK11 fails because the new dunit tests use regex to verify existence of certain stack traces. However, the CI uses a different JVM which has different formatting which doesn't match the regex.;non_debt
"Both flink-json and flink-avro were added to the /opt directory in order to use them as jars for the SQL Client. However, they are not required anymore because we added dedicated SQL jars later that are distributed through Maven central (see FLINK-8831).
`mvn clean install -pl flink-dist -am` is working again
Remove all formats from /opt
This change is a trivial rework / code cleanup without any test coverage.";non_debt
I think adding there lines here is enough:;non_debt
changed this class to be a wrapper class;non_debt
@dikejiang Do you mind creating a JIRA and adding the JIRA number to the PR title? Thanks!;non_debt
Below code are moved from RRDD with no change.;non_debt
You can probably skip these checks, given that `TaskInfo` has these checks already...;non_debt
one more space;non_debt
@feng-tao But `chain` is helpful in some situation. do you think so.;non_debt
What if we have multiple ledger directories and have different/conflicting status and last update times in all those files?;non_debt
âœ”ï¸;non_debt
merge postgres branch;non_debt
Curious - why isn't Travis running the tests?;non_debt
Retest this please;non_debt
Thanks;non_debt
[SPARK-13880][SPARK-13881][SQL] Rename DataFrame.scala Dataset.scala, and remove LegacyFunctions;non_debt
Perhaps we should also extract a `HasThreshold` mixin for binary classifier thresholds;non_debt
"I didn't intend to ask you to add another module to the build. Sorry about not being clear.
In many cases, it's possible to work with both 2.11 and 2.12 with compiled binaries. We have two internal versions of Spark that we do this with, although Spark defines the catalog and table APIs in Java so we have less to worry about. Ideally, we would have one module that works with both and we would have some plan to test that. Otherwise, it's probably a good idea to move this code into Flink as soon as possible because it makes more sense to maintain just source compatibility there.
For now, I think we should consider options for testing this against both 2.11 and 2.12. Maybe we can add a build property to switch between 2.11 and 2.12 and just test both in CI. Until then, I think we should just build for 2.12. 2.11 hasn't been supported since 2017, so it makes sense for new development to target 2.12.";non_debt
@hvanhovell , @cloud-fan , @davies could you please review this?;non_debt
these fields should be private;non_debt
Add through oal script;non_debt
this would be a contains method;non_debt
[AMBARI-23025] NN Federation Wizard: implement step2;non_debt
"Easy to read but buggy.
int bug = (int) Long.MAX_VALUE % 64 
bug will be -1";non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1350/;non_debt
run cpp tests;non_debt
CDN-in-a-Box now caches Carton dependencies by default;non_debt
Remove newline.;non_debt
What user is expected to do when he hits such limitation?;non_debt
This is a new requirement for developers to setup. Can we do without it? E.g. set the REEF folder as a command line option?;non_debt
whole stage codegen;non_debt
nice catch @dongjoon-hyun !;non_debt
Got it. My bad.;non_debt
Cache query cursor do not create value copy.;non_debt
fix test case;non_debt
Can this fall back to python 2 actually?;non_debt
Enable cluster auto-assembly through a seedlist;non_debt
"This pull request finishes up the code that @alt36 started for fixing issues with LDAP Referrals.
This closes #129.";non_debt
And same way below also.;non_debt
 Merged build triggered.;non_debt
"This add missing test data for one of the keys for a recently added
migration test.";non_debt
Specify the appropriate shell for installing GitHub Actions dependencies;non_debt
Would return attribute b in table a.  resolved within the logic of `table.col`;non_debt
Jenkins ran for 1:30:00, seeming to take extremely long on the integration tests. Unclear what this indicates.;non_debt
ARTEMIS-2440 Call timeout should retry the connection asynchronously;non_debt
*Currently, `TypeInfoTestCoverageTest` which checks a test that extends `TypeInformationTestBase` for all type infos. But `TypeSerializer` doesnâ€™t have the same thing that would verify that `TypeSerializer` has tests that extend `SerializerTestBase` and `TypeSerializerUpgradeTestBase`. Therefore this should add `TypeSerializerTestCoverageTest` to check whether to have tests based on `SerializerTestBase` and `TypeSerializerUpgradeTestBase` because all serializers should have tests based on both of them.*;non_debt
got it~!;non_debt
can you externalise this string (like the ones above);non_debt
@mgummelt;non_debt
Im really not. Im suggesting drop trying to be JMS at all if you want to remove JMS api stuff here.;non_debt
This pr is ready for review.;non_debt
@blueorangutan test;non_debt
"You can make another backporting PR against on branch-3.0 while keeping this PR independently.
After that PR is merging, we can revisit here.";non_debt
It matches the one defined in Yarn. So may be just use the same name.;non_debt
"[cloudstack-pull-requests #808](https://builds.apache.org/job/cloudstack-pull-requests/808/) SUCCESS
This pull request looks good";non_debt
Tests passed. Checkstyle and archetypes failed.;non_debt
99919302-2222 review-347693807;non_debt
Modify the log level of functions and move the java_instance_log4j2.xml file from the jar package to the conf directory;non_debt
What are the next steps for this PR? Is this ready to be merged?;non_debt
"@tianchen92 from the logs:
""/home/travis/build/apache/arrow/docs/source/java/index.rst:19: WARNING: Title underline too short.""
I think you need to add an equals sign to that line";non_debt
got this error even after `pip install cython`;non_debt
NIFI-7976: Infer ints in JSON when all values fit in integer type;non_debt
I'm thinking from the perspective of an Apache developer. They shouldn't have to go to external corporate documentation sets or knowledge bases (https://discuss.pivotal.io/hc/en-us/articles/201081408-How-to-collect-core-files-for-analysis) to get useful information about a utility (packcore) included with the HAWQ distribution. As it stands, the utility (packcore) will be automatically bundled with all future HAWQ releases.;non_debt
[SPARK-33819][CORE][FOLLOWUP][3.1] Restore the constructor of SingleFileEventLogFileReader to remove Mima exclusion;non_debt
In response to [comment](https://github.com/apache/accumulo/pull/1746/files#r514604929) from @keith-turner. I added a try block around my previous change on #1746. With the unreserveNamespace call in the finally block.;non_debt
@ueshin @srowen @HyukjinKwon Please, review the PR.;non_debt
"Either the methods in the `if` conditions should be renamed to self-documenting ones, or the inlined comments should be preserved. 
~By looking at this right now, I'm not even sure which condition was mapped to which one (has the order of conditions been preserved?)~ after couple of minutes I figured it out, that the order was preserved, but since it wasn't obvious for me, it proves the point that something is missing here.";non_debt
"[Reef-pull-request-ubuntu #486](https://builds.apache.org/job/Reef-pull-request-ubuntu/486/) SUCCESS
This pull request looks good";non_debt
Merged build started.;non_debt
@rhtyd a Jenkins job has been kicked to build packages. I'll keep you posted as I make progress.;non_debt
Without the changes in this PR, this test still can pass. : );non_debt
I think we already have users with this header, so I will leave it as it is.;non_debt
This also can be debug log.;non_debt
why is the filter removed?;non_debt
"why it is easier if not a real timestamp? I can change it back to a static number, but feeling this might be more realistic.
This change is originally motivated by debugging the Kafka Server. I thought some data is trimmed because of the timestamp is too old. But it is not related.";non_debt
[THRIFT-3445] Generates Throwable#getMessage override;non_debt
@lindong28 Thanks for the comments. Please review again.;non_debt
Similarly here. Call clearTableFromCache.;non_debt
@zhuangchong You can start a discussion in the dev email, and realize this improvement according to the idea you mentioned, not on this 1.3.6-prepare. The current pr is a cherry-pick from dev, and this improvement is the result of the previous discussion on March 17.;non_debt
add `type` for auth,limit, rewrite,log...;non_debt
"https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L451
@andrewor14  here is not outdated. this is same to master's code.";non_debt
Done.;non_debt
"Hawq register si a utility to register file(s) in hdfs into the table in hawq. It will move the file in the path(if path refers to a file) or files under the path(if path refers to a directory) into a new place corresponding to the table <tablename>,
and then update the meatadata in the table.
=>
""hawq register"" is a utility to register file(s) on HDFS into the table in HAWQ. It moves the file in the path (if path refers to a file) or files under the path (if path refers to a directory) into the table directory corresponding to the table <tablename>, and then updates the table meta data to include the files.";non_debt
"Seems like you are claiming only one document here but returning all documents ?
Instead claim each record (document position) before returning. This will guarantee that we do not claim (and return here) documents that belong to splits that have already been given to other workers through dynamic work rebalancing (RangeTracker.split() call).";non_debt
Yea, please describe the problem from an end-user's perspective.;non_debt
SAMZA-2173: Fix NullPointerException in SetConfig MessageFormat when configurations are deleted.;non_debt
Reuse TableMetaDataLoader in SchemaMetaDataLoader;non_debt
"This is a sub-task of [SPARK-9103](https://issues.apache.org/jira/browse/SPARK-9103), we'd like to expose the memory usage for spark running time, this is the first step to expose the netty buffer used both with on-heap and off-heap memory. Also the metrics are showed on WebUI. In this PR, a new web Tab name `Memory` is added. Which is used to show the memory usage of each executors (can be in more details in future). the screenshot is like the following:
This is for each stages memory info:
This is History View:
This is WIP because with unit tests left.";non_debt
I went and double-checked this file. It was contributed in #5440 by @njhartwell, and it looks like he authored this file and intended to contribute it. So I think we are good to add this header.;non_debt
I will merge this.. but if there's any failures from the full testsuite.. I may have to revert this.;non_debt
Build Failed  with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/2882/;non_debt
23418517-1001 review-296306101;non_debt
Update CountDownLatch2.java;non_debt
4766638-170 description-0;non_debt
Add pad param to DataBatch returned by BucketSentenceIter;non_debt
similar comment about javadoc link to specific function;non_debt
Hi @bowenli86, sorry about this. I had the commit ready to merge but was waiting for another test PR to be merged first. Merging this now!;non_debt
"@mutdmour  Above comment is not for you :) .  By commenting  ""retest this please"",  we can re-trigger the Jenkins builds/tests.";non_debt
two lines between functions;non_debt
"OK, I see your point now @StephanEwen.
@nssalian, I'm sorry for opening the jira I didn't think it through from this perspective at the time.";non_debt
@kayousterhout BlacklistTracker has it's own logging that is concerned with blacklisted nodes, won't it be enough? on the other hand, if blacklisting is disabled, which is default, then we will lose this information.;non_debt
Yeah, it is.;non_debt
 `numericPrecedence` doesn't contain `StringType`, you need to write a separate case like `case (t1 @ StringType(), t2 @ NumericType())`.;non_debt
the `SC` prefix is clearly a convention, but what does it stand for?;non_debt
You could check that `parser.HELP` is in the final command.;non_debt
"Each cell in table supposed to print html. For example
This PR
In master branch";non_debt
I added an `ink_release_assert(netvc)` to give clang a valid code path.;non_debt
â€¦types implement serializable.  Renamed to FluoInputFormat to FluoRowInputFormat;non_debt
@StephanEwen any opinion about this PR?;non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/3090/;non_debt
"Now we can argue about the name! ""Guard"" is usually used for protective mechanisms, such as locks or guards against garbage collection. This will normally do the opposite and guarantee destruction. Something like ""Always"" or ""Afterwards"" or ""PostAction"" or ""OnExit"" or ""PostScope"" or ""ScopeAction"" or ""PostScript"". I kind of like ""Afterwards"" or ""PostScript"".";non_debt
"I see your point, but if we introduce a method that not throws then we should return a tuple from it - T2(className, errorMsg).
Moreover, the second value of the tuple required in rare cases when we can't resolve the class name from type id.
AFAIU `getClassName` executed several times on every interaction between DotNet and Java so the creation of an extra object on each invocation will hurt the performance.
So I propose to keep changes as is. What do you think?";non_debt
Escape not working;non_debt
Thanks @xiaoyuyao for the review.;non_debt
"Yes, sorry, I mean defaults to `initLimit`.
With my comment I wanted to emphasise that we usually don't add every single new config option to sample config file (keeping the size at the bare minimum) and this new one doesn't seem to me any exceptional. It's more like an advanced, low level setting to me while the sample config is for newbies.";non_debt
"std::set::insert supports exactly what you need:
So you can avoid using find.";non_debt
#14740 is merged now. Can you rebase? @yajiedesign;non_debt
Jira issue here https://issues.apache.org/jira/browse/THRIFT-3318;non_debt
General comment about RandomUtils: shouldn't there be methods that take a Random instance? Sometimes you want control over what RNG you use (e.g. Random vs. SecureRandom).;non_debt
"This feature aims to allow user to specify how many CPU sockets the target VM would have by using the 'cores per socket' parameter.  
Assume user assign 16 vCPUs to a new VM,  without manually specify the 'cores per socket',  CloudStack would try to divide the 16 vCPUs by 6, 4 and 1 in order.  In this case,  CloudStack would assign 4 sockets (16vCPUs /4) to this VM and every scoket has 4 vCPUs.
In most cases,  it shall casue no issue, however if there's sort of CPU limits due to Guest OS or application, e.g. SQL Sever version,   it could turn into a big performance issue. 
In our real case,  user deployed an Windows 10 guest OS with 16 vCPUs,  and CloudStack assgins 4 virtual CPU sockets by default.    Due to Windows 10 only supports 2 CPU sockets (https://answers.microsoft.com/en-us/windows/forum/windows_10-win_upgrade/windows-10-versions-cpu-limits/905c24ad-ad54-4122-b730-b9e7519c823f?auth=1) ,  it turns out this guest OS is only able to utilize two sockets (8 vCPUs) even though we have assigned 16 vCPUs in toal.   If user has CPU intensive application running on this VM,  this could introduce bottelneck.";non_debt
"I would expect the output should be translated from ""insert into"" (TableAlter relNode?) to sendTo() operator.";non_debt
"Problem statement:There's a `runtime.props` contained in hive table properties. The value of this props is all the metadata k-v pair that we obtained in runtime, including kafka topic for example. Keeping this value in final metadata is not necessary. This is a minor fix for removal.
@ibuenros Can you help review? Thanks.";non_debt
The Ignore annotation says getFileInfo.. I would have thought you wanted the getListing row in the permission chart?;non_debt
sure, updated, it's safer.;non_debt
Thanks for your help, Vihang!;non_debt
"- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
it seems like [Python-test](https://builds.apache.org/job/pulsar_precommit_cpp/1707/) build is stuck. will try to do force push again.;non_debt
The feature of uploading a volume/template and with this PR iso requires that the env has ssl/tls certificates setup. The failure to upload a template/volume/iso when endpoint is incorrectly setup with a wrong certificate (default) is a known case and should not limit the acceptance of the PR. Instead in a separate PR we can attempt to allow users to upload a template/volume/iso when admin has not setup a certs by providing it a non-https endpoint of the ssvm.;non_debt
Initially we want to be strict about the loss of committed data for the `@metadata` topic. This patch ensures that truncation below the high watermark is not allowed. Note that `MockLog` already had the logic to do so, so the patch adds a similar check to `KafkaMetadataLog`.;non_debt
Sure. will move the test to a new class;non_debt
"Problemï¼š
24274.0 (TID 664711) | org.apache.spark.internal.Logging$class.logError(Logging.scala:91)
java.lang.NullPointerException
        at java.util.ArrayList.<init>(ArrayList.java:177)
        at org.apache.carbondata.datamap.bloom.BloomCoarseGrainDataMap.prune(BloomCoarseGrainDataMap.java:230)
        at org.apache.carbondata.core.datamap.TableDataMap.prune(TableDataMap.java:379)
        at org.apache.carbondata.core.datamap.DistributableDataMapFormat$1.initialize(DistributableDataMapFormat.java:108)
        at org.apache.carbondata.spark.rdd.DataMapPruneRDD.internalCompute(SparkDataMapJob.scala:77)
        at org.apache.carbondata.spark.rdd.CarbonRDD.compute(CarbonRDD.scala:82)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:99)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:325)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Solutionï¼š
when filteredShard is empty, return arraylist<>(), and protect null exception of other scenes
        protection of null exception, existing test cases are fine";non_debt
I'm not sure if this is the correct issue but it looks like manylinux 2010 is now very close https://github.com/pypa/manylinux/issues/179?;non_debt
[SPARK-2287] [SQL] Make ScalaReflection be able to handle Generic case classes.;non_debt
"not that big of a deal - how come you didn't use ""i""?";non_debt
I think we should avoid having `tempfile()` as output path in example, as that might point users into the wrong direction - anything saved in tempfile will disappear as soon as the R session ends.;non_debt
Why not return the number of fetches that were sent?;non_debt
Merging to master;non_debt
I think there's a `dag_bag.bag_dag()` method that we should use instead?;non_debt
some errors during succesfull build:;non_debt
"Yep. It exists in master, too.
I'll update like the example by @cloud-fan .";non_debt
Hmm, not sure about the comment on HW. With KIP-101, typically the truncation point is >= the local HW.;non_debt
"Thank you to @romainfrancois and others who have pushed forward the R side of this project!
This PR is my attempt to address [ARROW-3336](https://issues.apache.org/jira/projects/ARROW/issues/ARROW-3366), providing a testing container for the R package.
This follows up on work done by @kszucs in #2572 in an R-specific way.
**NOTE:**  This PR is a WIP. `R CMD INSTALL` currently fails because it cannot find wherever I installed `arrow` to. But I felt that this is far enough along to put up for review.";non_debt
Fix ClassCastException in ShardingSpherePreparedStatement and add spring namespace example for shadow;non_debt
Added class name;non_debt
The logic of case T_TableScanState in VExecEndNode is not the same as VExecInitNode, what's the difference ?;non_debt
"@sijie ok. I open issue #9081 for adding the validation when uploading a package and will do it in another PR.
Can we merge this PR first? Because this PR will allow us to use the package name to create functions.";non_debt
done in 4d0872b;non_debt
done;non_debt
160999-1085 description-0;non_debt
@villebro @mistercrunch Thanks for the backstory! Wow, I can see that there was a lot of work done to fix the previous issues. One question: Will there be a regression if the option value is not specifically an object {table: ..., schema: ...}? I did not use fully-qualified table names. I just pulled out table and schema into separate properties in the parent object and it seems fine. Am I missing something?;non_debt
[SPARK-11073] [core] [yarn] Remove akka dependency in secret key generation.;non_debt
"Maybe reword it as - ""Number of failed attempts to establish heartbeat when the AM changes""";non_debt
Merge pull request #1 from apache/master;non_debt
Is there a better way to check this condition?;non_debt
Good idea. Updated.;non_debt
"If it's across thread boundaries or even in the same thread but far away then I think I agree with you.  But here the `throw` is just a few lines after the method call which originally threw and caught the exception.
What would be the right solution?  Some sort of `Exceptions.propagateFromExecution(exception)` which you invoke if not in the original thread context and which throws an `ExecutionException` (and does not interrupt the current thread)?
Since this is all same thread can we keep this code and in another PR address the places where we blithely do `Exceptions.propagate` across thread boundaries?";non_debt
ok, but My pr is closed by community...;non_debt
Yes - I think it'd make sense to decouple this from the RPC itself.;non_debt
"@cloud-fan There are exists a problem the index of partition and the order of data are inconsistent.
I have a new implement but not works file as I can't assurance the order of output produced by child plans.";non_debt
@bschell HUDI-539 was for fixing this for spark sql.. did you test that as well? does it actually fix anything for spark queries;non_debt
"Since when `UnsafeRow.getString` return UTF8String, but `Literal.apply()` don't support UF8String.
In this patch support it.
Make Literal's constructor support UTF8String
When s is instance of `UTF8String`
user can use `Literal(s)`
Not need";non_debt
@ashb I'm not sure why the static checks are failing on the docker lint. Any insights?;non_debt
add brpc-java plugin;non_debt
Add sum/min/max reducers;non_debt
Is there a way to explain this list? Or is it just a fairly arbitrary laundry list of reserved words that are somehow too aggressive?;non_debt
"I think this cache is mainly used for file status, not `decoding the metadata fetched from the external Catalog`. Thus `MetadataCache` may not be a good name.
And this cache is useful whatever the external catalog is, why not put it in sql core module?";non_debt
Run Python PreCommit;non_debt
Added in the latest commit now.;non_debt
Replace with FileStatus fileStatus : fileStatuses ? You don't need index...;non_debt
"We require that the user update the entry using the cache so that it can
reevaluate the weight. That could be a put or computation. The weight is
stored with the entry and not re-evaluated on eviction.
On Mon, Nov 13, 2017 at 12:45 PM Keith Turner <notifications@github.com>
wrote:";non_debt
Renamed to `URIGuacamoleProperty`.;non_debt
We are storing a primitive value in a boxed value. Why not using `null` here?;non_debt
"Maybe we could put this to `INFO` but change the message slightly to say ""This could be an issue for IQ"" or something along those lines.  Just a thought.";non_debt
All fixed up and green.;non_debt
@andreweduffy @rxin Maybe I can go for the simple benchmark quickly (maybe within this weekend) and open a PR to disable Parquet row-by-row filtering if it makes sense and this can be the reason to hold on this PR?;non_debt
"While this looks good to me I've asked a team member here at google to look at it too.
The use of spinlocks makes me uneasy, but your justification seems reasonable.  Some profiling eventually to make sure we're not spending much time spinning would be good.";non_debt
@fjy / @xvrl  : any comments on this one?;non_debt
Unforunately, it seems like the compile engine can't find any schedules if I do this:;non_debt
I think you are right. Marking this conversation as resolved.;non_debt
Merged to master.;non_debt
Would like to see either a RAII object to ensure open/close happens, or at least a try {} catch {} with the closedir call in all exception paths to ensure things are closed if there are exceptions.;non_debt
Prefers `IndeterminateCheckboxProps`: https://github.com/apache-superset/superset-ui/pull/586;non_debt
no problem, sorry I never get these right -- I actually thought package imports were supposed to be first and checked some other places to realize you are right.;non_debt
"@cameronlee314 This PR is not for open review. Sorry about not making that explicit. We have couple of ways of implementing side input stores and this was one of the illustration of the prototype.
We decided to drop this prototype and go with a separate class for side input storage manager. I will take a look at the comments and apply whatever is applicable to the new prototype.";non_debt
Rebased after merging #10368 - I think it is ready for review and very accurately describes what we have in CI now.;non_debt
Generally similar comments to LastAggregatorFactory;non_debt
832676-1267 description-0;non_debt
I see the docker compose starts every service individually. Would it be better to not use docker compose and instead use a single docker running the `bin/start-quickstart` script?;non_debt
LGTM! +1;non_debt
Thanks for the PR @uce! Could you explain how the user would retrieve the user class loader to load classes during runtime? Is the user class loader exposed to the user?;non_debt
"Instead of passing the `configuration` map and then potentially extracting the keystore and truststore path & provider from it why not simply pass through the `keystorePath`, `keystoreProvider`, `truststorePath`, & `truststoreProvider` which were passed in to `getSSLContext` (i.e. the caller)?
Keep in mind that the individual configuration values that are passed to the caller aren't simply those pulled directly from the map. There is series of checks to ensure the client isn't overriding things with system properties, etc.";non_debt
"Python does not support constructor overloading, so I have to create a new function for this.
I think both `auto_scheduler.create_task` and `auto_scheduler.task.create` are okay.";non_debt
@markusweimer Do you see any test failures that should block this PR?;non_debt
coverity 1021707: Uninitialized scalar field;non_debt
Scala example:?;non_debt
"We are *not*. That's the whole reason why I'm adding the SQL-specific option to extend the behavior of the core options.
If I just wanted to revert the original behavior change, which added more unwanted default redactions to the core options, I'd just have changed the default value of the core option back to what it was.";non_debt
Since this PR added a test that creates mutable execution context, it needs to be rebased and updated after https://github.com/apache/arrow/pull/7464 get merged.;non_debt
update comment?;non_debt
"https://github.com/apache/incubator-mxnet/issues/9419
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable";non_debt
31006158-3385 description-0;non_debt
We should add a usage function in `madlib_keras.py_in`, and a version of `madlib_keras_fit()` here with no parameters that calls it.  Same for `madlib_keras_predict()` and `madlib_keras_evaluate()`.;non_debt
"Well that was a surprise @yifan-c The new custom junit executor was wrong. Adding that new test method you suggested I noticed I couldn't get any failures. The tests were green despite I knew they were broken. I found out the new executor was swallowing all errors. So I removed that and overrode the `@After` method, which on retrospect I don't know how I didn't think of it before.
Now everything works as expected, it's cleaner and has the extra test method.
CI [j11](https://app.circleci.com/pipelines/github/bereng/cassandra/144/workflows/8e0704dc-c5c2-4f34-8455-8625a715922c)
CI [j8](https://app.circleci.com/pipelines/github/bereng/cassandra/144/workflows/17c720fd-af9e-424b-b510-fa386266e9e1)";non_debt
Jenkins retest this please;non_debt
@symat , as @anmolnar and @lvfangmin mentioned - the intention of this counter was to help in monitoring and I would suggest me keeping it in the code unless you still have some concerns on this ?;non_debt
As above: use `assertThrows` and verify error message;non_debt
"Use print(x) not print x for Python 3 in eval examples
CC @sethah @mengxr -- just wanted to close this out before 1.5";non_debt
We have some strange classpath issues with ORC in our test classpath, to make things worse, when I attempt to debug the issue the debug console does not have the same issue and can call the missing method without issue.;non_debt
"@flinkbot attention @zentol 
@flinkbot approve description";non_debt
Iâ€™m not convinced this would be useful, could you close this please?;non_debt
@fhueske hi, Stephan mentioned you in the email reply to me, let me find you help, I don't know if you have time to discuss this jira, but I still look forward to your reply.;non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12545/job/ApacheCarbonPRBuilder2.3/1313/;non_debt
Removed.;non_debt
This can unexpectedly break callers, please add it to [CHANGES.md](https://github.com/apache/thrift/blob/master/CHANGES.md#breaking-changes).;non_debt
This provides some basic methods for interacting with a Flight RPC server via `pyarrow` and `reticulate`.;non_debt
"Instants are comparable. Earlier/Later are synonyms with Less/Greater.
------------------------";non_debt
"1. `protected` I don't think it's needed, as this is looks like valid public api of this class. Note,  `StreamOperatorStateHandler` is a private field of abstract classes.
2. ` KeyedStateBackend<?>` wouldn't work as some `PublicEvolving` apis need this to be casted. I think global type parameter wouldn't work, as this field/class is being used also in non keyed context.";non_debt
45721011-4972 review-446476018;non_debt
"Just run an interpreter, just need this `zeppelin-interpreter-api-${Z_VERSION}.jar`?
Do need `zeppelin-${Z_VERSION}/lib` directory?";non_debt
Yes, same as above. ðŸ‘Œ;non_debt
Run Python2_PVR_Flink PreCommit;non_debt
"Renamed all metric tables that were changed to have UUID column
METRIC_RECORD -> METRIC_RECORD_V2
METRIC_AGGREGATE -> METRIC_AGGREGATE_V2
and so on...
Manual (Install ams with hdfs) + UTs";non_debt
feat: provide `echarts.graphic.registerShape` and `echarts.graphic.getShapeClass`;non_debt
@eolivelli yep, I know. I meant to wait for gcr repo to update to another version.;non_debt
No, it isn't for every message sent. It's only used when a handler is set (either on the session or passed-in when sent).;non_debt
I closed KAFKA-6538 and created https://issues.apache.org/jira/browse/KAFKA-7015 to track the `RecordCollectorImpl` issue -- this makes it easier to assign fixes to different versions.;non_debt
"I think CI test failure is not related to this PR.
I'm merging it if there is no more discussions.";non_debt
"It seems like the `startFunctionExecution()` and `getTime()` should be hoisted outside the try/catch so that `startFunctionExecution()` is guaranteed to be called before `endFunctionExecutionWithException()`. I also don't like how we're invoking `getTime()` twice in this method because it makes the recorded elapsed times for successful and failed executions inconsistent.
Again, I'm not sure if this would cause any problems for users who rely on the current stats implementation.";non_debt
"Right now, vendoring is easier for us to minimize the dependencies. We'd like contributing back these changes to upstream later.
@rgbkrk Have you tried Dill ?  https://github.com/uqfoundation/dill";non_debt
[numpy] random.rand;non_debt
retest this please.;non_debt
use ===, not ==;non_debt
[FLINK-11902][rest] Do not wrap all exceptions in RestHandlerException;non_debt
Build Success with Spark 1.6, Please check CI http://88.99.58.216:8080/job/ApacheCarbonPRBuilder/286/;non_debt
Adjust the weights on the subtitles to be less than the titles;non_debt
"Currently there is no test with legitimate kryoBufferProperty value expressed in 'm'
This addition fills the gap.";non_debt
"@viktortnk Thanks for your work on this. I made some minor changes and submitted a pull request to your repo.
I confirmed that the issue is reproducible in 0.9.3-rc1, and that it is fixed with (y)our changes.
I'm +1 with the suggested changes.";non_debt
@aljoscha sorry for the ping. did you happen to get a chance taking a look at the PR?;non_debt
You can use some other function to find out whether the number has integer part rather than comparsion between the absolute value of the number and 1.0 .;non_debt
saas tests cancelled because of oom;non_debt
For Sql aggregation, the `spillSize` here is 0 because the data are stored in a map instead of this sorter. So `incMemoryBytesSpilled(spillSize)` actually increase 0. We need update the `MemoryBytesSpilled` after freeing the memory in the map.;non_debt
Recommend using exception to pass the failure message instead of passing in a logger. The problem of using logger is that the caller cannot control the logging level. For user input, in most cases we don't want to log error which usually stands for severe problems.;non_debt
You are right.;non_debt
cc @yidawang @JennyChou @junrushao1994 @jroesch @jermainewang;non_debt
@srowen @jkbradley updated with comments. I used the spark version to sniff the version as suggested by @jkbradley, although I'm happy to continue the conversation about the best way to handle versioning. I'm assuming this will make it into spark 2.2, so that's what I used as the cutoff.;non_debt
HI @andersonjonathan, you have to use `dev:helium` instead of for 0.8.0p-SNAPSHOT. It was `visdev` in 0.7.x, but changed.;non_debt
This is the primary fix. Instead of relying (hoping) on TaskManager to put the changelog reader into restoring_active, we just idempotently make sure it's in that state any time we're in partitions_assigned.;non_debt
Rolled it back;non_debt
"Add Jiajie Zhong (@zhongjiajie) to committers list
---
Make sure to mark the boxes below before creating PR: [x]
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.";non_debt
AIRFLOW-105 Implement SqoopHook;non_debt
I have modified code according to your suggestion;non_debt
[SPARK-14015][SQL] Support TimestampType in vectorized parquet reader;non_debt
Great work! This is awesome.;non_debt
Streams system tests above are green.;non_debt
How come the code didn't need/use `stale` before?;non_debt
retest this please;non_debt
No. But we want to test the behavior about what happens during a deletion (ex: operational error).;non_debt
False: it is the new name that must remain. I've put it back.;non_debt
Lint issues are fixed;non_debt
Thanks, guys! I appreciate you (especially @alamb and @andygrove) for taking the time to help me navigate the process. I look forward to continuing to participate in this community!;non_debt
Run Python ReleaseCandidate;non_debt
Why are you converting the representation of `times` to nanoseconds? I thought POSIXct was represented internally as seconds;non_debt
Packaging result: âœ”centos7 âœ”debian. JID-1090;non_debt
@aaltay PTAL as this is already done in Java now.;non_debt
+ [ios] update prerender logic;non_debt
But, yes, IMO these are the right settings to make and the correct way to do it.;non_debt
@nvazquez @serg38 how about we remove vmware.nested.virtualization.perVM and instead if the user vm detail exists and is true, it overrides the global setting?;non_debt
[Relay][Op] Add operators full and full_like;non_debt
"If it still does not work, you could 
1. re-clone a new repo of Beam
2. add UnionMergeRule
3. change `BeamCostModel.FACTORY` to `null` at https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/CalciteQueryPlanner.java#L116
Then quickly test";non_debt
"Intentional.
CHANGES.txt has changes up to 0.94.0 if you are interested.";non_debt
maybe it's a good idea to have a separate page talking about the metrics storage format, and move this note and the explanation there;non_debt
19961085-5910 review-479596102;non_debt
what specifically are you checking here?;non_debt
"In [SPARK-19669](https://github.com/apache/spark/commit/0733a54a4517b82291efed9ac7f7407d9044593c) change the sessionState access privileges from private to public, this lead to the compile failed in TestSQLContext
this pr is a hotfix for this.
N/A";non_debt
I'm working on it https://github.com/apache/arrow/pull/4649;non_debt
[AMQ-7351]Â Update to Apache pom parent 21;non_debt
"Thanks @ptrendx @eric-haibin-lin this is no FP16 GEMM from Intel MKL/MKL-DNN till now so I think we have to fall back to FP32 GEMM or simulated FP16 GEMM (maybe from `mshadow` or other BLAS library).
The current design is good for APEX.  I am thinking about a general solution for mixed precision training, such as INT8, FP16, BF16, FP32. The different HW supports for the different data type, like INT8 in CPU and small GPU chip and BF16 on CPU and FP16 on GPU. So we need more flexibility for data type change or other library integration.
We are going to support BF16 and the ideal situation is the user can switch FP16 and BF16 based on the devices transparently.
We are looking into the changes and will give back more details later.";non_debt
"This is unrelated to the fix applied in the invoker http call right?
But if it works now let's enable the test I agree.";non_debt
"I worked on test program demonstrating the JDK issue and I see now that this has already been reported:  https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8177021    (we should put a comment about this in the code).  It's nice to see it has been fixed in JDK 9 (and my test program revealed that too).
During my exploration of this, I discovered that a lowercase 'z' in the pattern can parse various formats, and so can ""VV"".  I replaced the 5-'Z' patterns in the test file with a single lower 'z' and the tests passed.  FYI this part of the javadocs is helpful: https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatterBuilder.html#appendPattern-java.lang.String-  (builder has more detail than the DateTimeFormatter).";non_debt
"Fixes https://issues.apache.org/jira/browse/KAFKA-3182 by:
* Turning off Nagle on the sending sockets to force the socket to physically acknowledge after the first write in `sendRequest`
* (Still had to do this on Linux though not on Mac) Adding a `200ms` delay between write attempts (my guess would be [Slow Start](https://en.wikipedia.org/wiki/TCP_congestion_control#Slow_start) is to blame for this, but with Nagle off 200ms should more than suffice here imo)";non_debt
"Nice @costimuraru. No problem with the ""copyright"". Just as long as this gets merged I'm happy (one less reason to keep our internal parquet-mr fork!). cheers!";non_debt
"@koscejev , it is up to users what they want to convey in headers of camel exchange. Here, the restlet component uses various headers (if they are set before getting applied in the code). i think you can do what you want to do by using anoym processor to set as camel header. I am not sure what you mean about camel being protocol agnostic vs setting header. setting of a header value is users preference and setting special  headers like HTTP_QUERY is also user's preference as it is optional and if you set explicity, i think you should know what you need to do with path param. (it should be URL encoded in your case).
This is just IMHO. @davsclaus , @zregvart @WillemJiang @stsiano  may also have a comment on it.
@koscejev i may be getting you/your purpose wrong, i guess it would be great if you can elaborate on the issue and your test case more .";non_debt
Linux build *successful*! https://ci.trafficserver.apache.org/job/linux-github/1932/;non_debt
`pre_process` param is missing here.;non_debt
@elek @avijayanhwx Please review;non_debt
`TypedWrite<Void, V>`;non_debt
Nice catch!  Done.;non_debt
99919302-1642 review-313838992;non_debt
Signed-off-by: spacewander <spacewanderlzx@gmail.com>;non_debt
"Hi Matthias,
Thanks for your comments. Should be ready now.
Regarding JDK9 & extending the Maven binaries I fully agree. I think it's worth keeping this as close as the original Oracle donation as possible. After all this module has been tested in the field and seems to be used by other modules. Later on, once the donation is complete, we can add more features... and bugs -)
Thanks,
Antonio";non_debt
"Yes, but the problem is, it (almost) always evaluates it with NULL when the columns have dots in the names because column paths become nested (`a.b` not `` `a.b` ``) in the Parquet predicate filter up to my knowledge.
You are right for `IsNull`. I pointed out this in https://github.com/apache/spark/pull/17680#discussion_r112285883 as it looks they (almost) always evaluate it to `true` in Parquet-side but it is filtered in Spark-side. So, for input/output, it is not an issue in this case but I believe we should disable this for this case too.
I think this example explains the case";non_debt
":broken_heart: **-1 overall**
This message was automatically generated.";non_debt
Fixed record latency before recycling in broker-side producer handler;non_debt
"yes @cloud-fan , you're 100% right, we want to treat `(...)` differently when it is in front of IN.
Here you are the previous example in Postgres:
In Oracle/MySQL you cannot create structs using `(...)` but you have to define a custom data type for structs, so this situation is prevented to happen.";non_debt
"Fixes #5827
Fixes #5828
UDP protocol is not working for netty connector
Added a specific handler for UDP and use Channel instead of SocketChannel in NettyChannelInitializer
Successfully tested";non_debt
retest this please?;non_debt
ping @cbalint13 can you update the PR?;non_debt
APEXCORE-496 make operator name available to StatsListener.;non_debt
The Smoke tests deployment by provisioner would be broken due to [BIGTOP-2742](https://issues.apache.org/jira/browse/BIGTOP-2742), but actually zeppelin server has been started.;non_debt
Jenkins, retest this please.;non_debt
"I tested it ,if the sql is `select * from mytable where data = null` ,it do not supports filter push down in flink,and we do not can get any data.
if the sql is `select * from mytable where data is null`, it is normal ,and It will enter the `IS_NULL` branch of switch";non_debt
Protect against LRUHash x = x;non_debt
"Well, we can have a completely normal exit code from the `run` execution, but the `-p` option completely ignored if we change the CLI to simply not recognize the option.
This is an extreme case, though.";non_debt
Done.;non_debt
Both seem fine, fortunately.;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/12147?src=pr&el=h1) Report
Flags with carried forward coverage won't be shown. [Click here](https://docs.codecov.io/docs/carryforward-flags#carryforward-flags-in-the-pull-request-comment) to find out more.
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/12147?src=pr&el=continue).";non_debt
Isn't this scenario covered by the `requestUpdate()` in `Sender.run()`  which checks whether there are unknown leaders when there is data to send?;non_debt
@hnfgns can you please review?;non_debt
It's better to use `get` to load note and verify it.;non_debt
@dongjoon-hyun Ah.. got it.. thanks a lot.;non_debt
@zmhassan Hi zmhassan. As I mentioned in a JIRA issue, this is in progress by @zjffdu. Could you concede? @zjffdu already contacted me for a few days ago for this issue and I asked for him to work it after I merge ZEPPELIN-1012. I hope he already worked and is waiting for merging ZEPPELIN-1012. Sorry for late reply, and duplicated works.;non_debt
+1 looks like a trivial change. Thanks.;non_debt
20587599-1591 description-0;non_debt
Fixed, thanks;non_debt
"Sorry for delay, got busy. 
Thank you @StephanEwen for your review.updated.Please have a look.thanks cc @greghogan";non_debt
I would expect those additional values to also be part of `y`.;non_debt
193065376-280 description-0;non_debt
Trying having a parent class called JobSpec. AnomalyJobSpec and MonitorJobSpec should inherit from the parent class;non_debt
now we never use the `context` and `extension` parameters?;non_debt
Agree that ordering shouldn't matter in general - without inspecting those tests, I wonder if the tests you're referring to are actually looking at precedence order (which properties should win when there are conflicts between two maps).;non_debt
No, relationships have their own identifier to allow for multiple edges between the same pair of nodes.;non_debt
"For #1482, ShardingTransactional annotation support switch transaction type for proxy.
Changes proposed in this pull request:
- Send switch transaction type SQL to sharding-proxy.
- Support DataSourceTransactionManager while using MyBatis.
- Support JpaTransactionManager while using Hibernate.
- Unit test";non_debt
Nit: -> `.map(Utils.classForName)`;non_debt
50229487-900 description-0;non_debt
This is basically the same approach used elsewhere right? if so that's fine.;non_debt
The current version in master branch is actually 2.4.0-SNAPSHOT. Also we would have to make sure this is updated at each release;non_debt
use `~==`;non_debt
Updated.;non_debt
"Its supposed to support copying it from other locations other then just file://, like another HDFS location.  The local variable is named badly (localPath).  Its actually reading what is specified by the user for the user app jar, spark jar, etc.
I guess it depends on the definition of remote here.  In general when running on yarn I wouldn't consider the hdfs installation on that yarn cluster as remote.   But its all in the perception.  I'm fine with the name as long as we are clear in description of what it does.";non_debt
Thanks! It was my mistake. I've fixed.;non_debt
@ewencp I adjusted the build to only create/publish the test jars if there are test sources.;non_debt
@dusenberrymw can you please review this PR ?;non_debt
Susan: I need some input here if this is as expected, it seems odd that it could (before) possibly insert an empty (nullptr) entry, right ?;non_debt
Oh, the original one was 3.0. Although this doc change can go to branch-2.4 alone as well, let me revert it in branch-2.4 for management simplicity.;non_debt
Should this be removed?;non_debt
[tvmc] linting error on onnx command line driver frontend;non_debt
Modify the start time of the history session to match the GUAC_DATE and GUAC_TIME in the parameter tokens;non_debt
field number should be ordered;non_debt
you forgot to remove watch.start();non_debt
LGTM, simple change;non_debt
+1 looks good.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder2.1/3838/;non_debt
You are correct `NessieNotFoundException`  refers to the `ref`. If the table were deleted it would be a conflict exception. This is similar to comparing the error modes of git if you committed to a non-existent ref compared to a merge conflict in the case of changing files in the repo;non_debt
remove?;non_debt
@smengcl Thanks the patch. @adoroszlai @avijayanhwx @GlenGeng Thanks for review, I have merged the patch.;non_debt
7748336-310 description-0;non_debt
Agreed. I'm basically asking to keep both `fill` and `set_to`.;non_debt
"""For running tests on {@link FlinkPipelineRunner}""";non_debt
make sense to me.;non_debt
@reuvenlax, done;non_debt
You should enable or remove this line;non_debt
@rhtyd done (force pushed again, to trigger build);non_debt
done;non_debt
Love it!;non_debt
â€¦eft grid;non_debt
"Super nit-picky, but can we add borders to images? https://stackoverflow.com/questions/37349314/is-it-possible-to-add-border-to-image-in-github-markdown
Looks a bit rough here:
https://github.com/garden-of-delete/incubator-superset/tree/master/RELEASING/release-notes-0-38/";non_debt
done;non_debt
On hold - tests are failing, needs investigation and explore best case solution;non_debt
206444-935 description-0;non_debt
java.source.base: Fix for Java 11, update args, swap import;non_debt
@godfreyhe  comments addressed.;non_debt
function needs to return something.;non_debt
pom.xml: upgrade to dataflow version v1b3-rev30-1.22.0;non_debt
@hvanhovell  Since the optimization rule change the result (or sematics), I'd like to fix it, not to introduce more complexcity in physical layer. The saved few cycles may not worth, because people usually not use that in practice.;non_debt
 Merged build triggered.;non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3615/
Test PASSed (JDK 8 and Scala 2.11).";non_debt
feature: multiple certificates support for single domain;non_debt
Could you also modify the title to include java tests and python API?;non_debt
@felixcheung yup! Thanks!;non_debt
Done;non_debt
This fixes a potential cause of GEODE-6297 by preventing locator/server status from writing to status file if the status contains only whitespace.;non_debt
@chinmaykolhatkar I've committed your change. I would like to make sure that we fixed the problem. What's the easiest way to run a test for this change?;non_debt
"Ah, results are different since the number of operations are different. It may be an issue like #20630.
I am curious why test are failure when seed is changed. Of course, I understand the sequence of rand must be reproducable with certain seed value in a package or implementation.";non_debt
31006158-229 description-0;non_debt
"Could you remove 
So it doesn't have any effect by default. I'll verify if it confuses users later";non_debt
oops. will fix. how does everything else look?;non_debt
It already logs the original error.  I am not sure how much value another log message adds...;non_debt
Good idea, thanks!;non_debt
Translation into English.;non_debt
"Environment: kvm-centos7 (x2), Advanced Networking with Mgmt server 7
Total time taken: 33180 seconds
Marvin logs: https://github.com/blueorangutan/acs-prs/releases/download/trillian/pr4430-t3096-kvm-centos7.zip
Intermittent failure detected: /marvin/tests/smoke/test_kubernetes_clusters.py
Smoke tests completed. 83 look OK, 0 have error(s)
Only failed tests results shown below:
Test | Result | Time (s) | Test File
--- | --- | --- | ---";non_debt
I removed that too.;non_debt
I don't think it works since the logic there is to use the global field number if the field already exists in another segment. In the logic above we try to re-assign the global field number locally since it clashes with a local one.;non_debt
There isn't a currently a distinction between streaming and batch in the places where this interface is called (except in the experimental continuous processing streaming mode). The streaming engine executes a sequence of WriteToDataSourceV2Exec plans, in the same way that a sequence of unrelated batch queries would be executed. The only thing distinguishing streaming queries is that they have a custom DataSourceWriter implementation, which forwards each individual epoch to the StreamWriter.;non_debt
minor, but I'm not a huge fan of the variable name `d`;non_debt
"Sorry, the above is not a real issue. the latest code ""storm-buildtools/maven-shade-clojure-transformer"" have solved this problem.";non_debt
comment can be removed;non_debt
Build Success with Spark 2.2.1, Please check CI http://95.216.28.178:8080/job/ApacheCarbonPRBuilder1/420/;non_debt
[CARBONDATA-584]added validation for table is not empty;non_debt
Thank you, @iwasakims , @sekikn;non_debt
"The only way `fieldScale` can make it into the dialect is by the field metadata.
It was always added prior to the previous commit (which I agree with on a fundamental level)
https://github.com/skestle/spark/commit/0b647fe69cf201b4dcbc0f4dfc0eb504a523571d#diff-c3859e97335ead4b131263565c987d877bea0af3adbd6c5bf2d3716768d2e083";non_debt
Fix #1316: Exclude component-libaries.xml file when package collector jar;non_debt
20089857-2099 comment-660626678;non_debt
done;non_debt
Ref #2275..;non_debt
ok;non_debt
From what I could find, deletedAt and deleteDelay is only set using System.nanoTime/1000000, so there should not be any mismatch from comparing times that used nanoTime and times that used currentTimeMillis;non_debt
let's add `private` to all fields;non_debt
`newConfigKeyWithPrefixRemoved` ?;non_debt
MiMa failure is clearly unrelated, jenkins retest this please.;non_debt
Build Success with Spark 2.1.0, Please check CI http://136.243.101.176:8080/job/ApacheCarbonPRBuilder1/6515/;non_debt
retest it please;non_debt
@tristaZero I will optimize it too.;non_debt
/pulsar-bot rerun-failure-checks;non_debt
[approve ci autest];non_debt
Maybe up this a bit? A proper Java stacktrace can be bigger than 100 lines.;non_debt
"I have manually checked that all files are the same which obtained in the result of `mvn clean install -DskipTests -Papache-release`.
Looks like there are no differences, except those mentioned in Laurent's comment in the PR #449";non_debt
"*fix single yarn get application status failed*
deploy dolphin scheduler in single yarn hadoop cluster, 
exec a yarn job will lead npe when get the yarn app status 
*dolphinscheduler/common/utils/HadoopUtilsTest.java*";non_debt
retest this please;non_debt
[Dubbo-2798]fix apporiate NotWritablePropertyException;non_debt
If we match the old ProducerRecord behavior and don't include the actual record data in the toString (which I think we should change), then your proposal ends up essentially matching what I said.;non_debt
[SPARK-16499][ML][MLLib] improve ApplyInPlace function in ANN code;non_debt
Run Seed Job;non_debt
So, the throws comments from all the javadoc in the tests, right ?;non_debt
Altering theme for more subtle alerts / labels / buttons;non_debt
@willbarrett it would be a large refactor of existing functionality and all permissions model was based on using model triggers. It is definitely possible, however we should get buy-in from project maintainers & probably do it outside of this PR.;non_debt
remove since we added a fix in the code.;non_debt
Looks good to me.;non_debt
Similar to the previous placeholder issue.  This is what appears when I tested the course in both IntelliJ and GoLand:;non_debt
cc @hvanhovell;non_debt
@DaanHoogland a Trillian-Jenkins test job (centos7 mgmt + kvm-centos7) has been kicked to run smoke tests;non_debt
@keith-turner Looks good.;non_debt
"Unless we copy the plugin information when creating the proxy
transaction, the %<pitag> logging field always emits ""*"".";non_debt
Yep, I changed the title.;non_debt
Update compare strings to use 64 bit sequence Id.;non_debt
Added a new test for line chart to check clearing validator errors.;non_debt
"@davies Using a build from the latest master, I still got the above error.
`net.razorvine.pickle.PickleException: invalid pickle data for datetime expected 1 or 7 args, got 2`";non_debt
Thanks for posting the test case that reproduces the error, @hzfanxinxin! I'll take a look and work on fixing it.;non_debt
nice fix.;non_debt
[BEAM-10961] enable strict dependency checking for sdks/java/io/kafka;non_debt
"It looks like the error is:
Does it compile and run locally?";non_debt
Can one of the admins verify this patch?;non_debt
i don't think asserts are checked in druid test runs, if you are really uncerstain about timeout and interval then you would use Preconditions. However, given that this method is private , I wouldn't worry about the checks.;non_debt
This guy is on the way out - it should be replaced by CountingSource, so I'm not going to go to town here.;non_debt
@srowen Thanks! I will make a quick pass.;non_debt
include the table name;non_debt
"Thank you for submitting a contribution to Apache NiFi.
_Enables X functionality fixes bug NIFI-YYYY._
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
I think the following examples are self descriptive. I can emphasize it in the description too.;non_debt
    fix sonarcloud bug;non_debt
LGTM, with the changes for decorate. @fjy Can you also review ?;non_debt
NOTE 2: The source path should not be used from multiple **sources or** queries when enabling this option, because source files will be moved or deleted which behavior may impact the other **sources and** queries.;non_debt
Sorry, I did this wrong, should have tested it better.  I'll fix it.;non_debt
"The regular expression for a pattern of topic names to read from. All topics with names that match the specified regular expression will be subscribed by the consumer when the job starts running. Note, only one of ""topic-pattern"" and ""topic"" can be specified for sources.";non_debt
I made some suggestions about passing a KeyExtent to InitVolChooserEnvImpl.  If that works out, I don't think there would be any methods throwing UnsupportedOperationException;non_debt
Jenkins test this please;non_debt
[BEAM-11271] getDataset instead of getTable for query location inferâ€¦;non_debt
Thanks. Will fix.;non_debt
"Hi @AHeise ,
Do you think this plan is OK? If ok, I will develop the code of the current PR when I have time.";non_debt
@godfreyhe , is it only enabled in CBO? I mean, users shouldn't care about what optimizer is used in Flink.;non_debt
@viirya Do we need to fix this in Spark 2.0? UDTs are private APIs and the only intended use case is Vector/Matrix UDTs for MLlib, which doesn't put vectors or matrices inside an array inside a pipeline. In Spark 2.1, we probably need a formal discussion on merging UDT into Encoder, which could completely change its implementation.;non_debt
Fix : Searching for groups while creating center is displaying groups which are already member of other center.;non_debt
@anuragaw please put a Fixes \#xyz ID in the description.;non_debt
ARROW-3798: [GLib] Add support for column type CSV read option;non_debt
For what its worth I +1. Not sure why travis is failing though.;non_debt
"Remove .md5 files from release artifacts
N/A";non_debt
[FLINK-18710] Make ResourceProfileInfo serializable;non_debt
@blueorangutan test centos7 xenserver-65sp1;non_debt
Can u break down the executorCommand(...) by invoking this method executorCommandArgs(...)? You can just copy-and-parse the snippet of code in executorCommand(...) and put it here.;non_debt
Will do that in a separate PR, right?;non_debt
[Relay] WIP - *Do not merge* - Minor guard for Conv2D for Float32.;non_debt
67198520-626 description-0;non_debt
Back to your question,  we need to set the job state as FAILED as well. This is handled in BaseJob.run by throwing an exception.;non_debt
Filed https://issues.apache.org/jira/browse/BEAM-8414 to reenable missing checks, I don't think we need another issue.;non_debt
https://issues.apache.org/jira/browse/STORM-1412;non_debt
Add random int to table name in big_query_query_to_table_it_test.py.;non_debt
then can we set it to false? `DROP PARTITION ... PURGE` is not supported in hive 0.13, setting it to false can make the partition related functions still work in older hive versions.;non_debt
The same config values here are repeated in the following 3 tests.  Maybe extract to a method `getConfig()`?;non_debt
356066-2874 description-0;non_debt
"This seems like a ""progress bar"" instead of a metrics. Most likely people don't care about it because it's usually `num output rows of left` * `num output rows of right`.
Have we measured the perf overhead?";non_debt
Sure, new machine...;non_debt
@koushik-das You sure this is needed for this PR to work? it seems to me this can be future work. Nothing will be broken more then it is now.;non_debt
helijia join;non_debt
I understand that checking on both versions is necessary and I'm supportive of it, but I've seen some machines images that came with no python at all and people may just randomly install only one of the python versions. To ensure an out-of-box experience for our users we should either clearly specify installation of both python versions as a prerequisite or we do it for them within the script.;non_debt
You can use `akka.http.scaladsl.model.headers.BasicHttpCredentials` instead of creating this class.;non_debt
Merged build finished.;non_debt
Packaging result: :heavy_check_mark: centos7 :heavy_check_mark: centos8 :heavy_check_mark: debian. SL-JID 241;non_debt
Rebased with master and only kept relevant changes.;non_debt
I am :+1:;non_debt
remove extra line;non_debt
PARQUET-352: Add object model property to file footers.;non_debt
"Does this logic need to be in the runtime unparsers? We can only know if a choice is hidden if it is directly referenced by a hidden group ref. But if a choice is a child of a hidden group ref then this can't know if it's hidden or not.
Do we need to just always determine a defaultable branch and pass that into the choice combiantor, and then only use that if we determine we are hidden at runtime?";non_debt
50904245-4136 review-153940130;non_debt
Hmm.. if the node itself satisfies `isKeyChangingOperation`, then we will return it directly right? Maybe we should fix it in findParentNodeMatching (see below)?;non_debt
SPARK-3461. Support external groupByKey using repartitionAndSortWithinPa...;non_debt
Sorry, I may not have made it clear, but what I meant was to stop manually adding headers to ProducerRecord in the kafka-v1 plugin;non_debt
cc @cloud-fan @viirya;non_debt
Thanks for the remind, @zentol and @StephanEwen , I should be too hurry to open this PR. I tried to fix the exception in bloom filter in this PR and verify other potential issues in hash table behind negative count number separately, obviously, there is no need to do in that way. So let's wait for Greg's response now.;non_debt
Does this really need to be `info`?;non_debt
[broker] Prevent redirection of lookup requests from looping;non_debt
Fix done;non_debt
user is doing simple click from UI, nothing selected so why should they see nasty error?;non_debt
Check Test Started: https://jenkins.esgyn.com/job/Check-PR-master/1121/;non_debt
"This looks good to me. Simple getters added.
VOTE +1.
@pietermartin --- you are really introspecting deep into the traversal. Note that we have yet to publish the virtual machine step library and thus, things like `LoopTraversal` may not be in the specification. Thus, we can't guarantee you stability of the API in this area. Just a heads up.";non_debt
206424-447 description-0;non_debt
yes.there is no concept of async driver status poll for other modes , read https://spark.apache.org/docs/latest/running-on-yarn.html ! in other modes the submit to launch is synchronous . i think u can cancel this @albertusk95;non_debt
"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
Run Dataflow PostRelease;non_debt
"I have similar concerns. I think that the proposed, simple solution may suffice for a single-user or short-lived spark session. On the other hand, an unbounded cache here will probable lead to trouble for some users in a multi-user or long-running Spark app such as the thriftserver.
Right now I'm thinking the `InMemoryCache` is okay as-is because this feature is behind a configuration flag, but I think we'll need a resource-sensitive solution in order to support this cache in the thriftserver.";non_debt
 Merged build triggered.;non_debt
Thanks!;non_debt
Cloudstack-9285 exception log addition;non_debt
Run Python PostCommit;non_debt
Thanks @yidawang;non_debt
Missed that there is one test class extending the StreamProcessor and override this method. Changing to VisibleForTesting as suggested.;non_debt
Packaging result: :heavy_multiplication_x: centos7 :heavy_multiplication_x: centos8 :heavy_multiplication_x: debian. SL-JID 174;non_debt
this is a very good suggestion. It doesn't have to be distinct. But I will try to make it more unique.;non_debt
KAFKA-7799: Use httpcomponents-client in RestServerTest.;non_debt
Why not make this a `VersionedName`?;non_debt
okay, let me check.;non_debt
why remove this?;non_debt
"Add Geon-Woo Kim to committers
JIRA:
[REEF-784](https://issues.apache.org/jira/browse/REEF-784)";non_debt
"Choose one
A function in `superset/utils/core.py` had a circular import fix applied to it (required in utils/core due to it being depended on by the model layer) but it was only referenced in one place, in another `utils` file. Moving it to that file removes the need for the circular import fix.
@dpgaspar @craig-rueda";non_debt
I might just write `if (buffer == null) null else buffer.buf`;non_debt
@rabbah, sure thing.;non_debt
"Was just about to retest these changes but out of curiosity:
What is the effect of including a semicolon within the attribute of an AngularJS directive like this? Does Angular (intentionally) support multiple statements within expressions?";non_debt
"In realtime consuming segment, we first add value to dictionary then update inverted index.
If we query a column with inverted index, it is possible that the inverted index has not been generated.
The fix is filter out all null inverted index.
This will not affect the accuracy of the results because we just ignore a single under-indexing record.";non_debt
51905353-7233 description-0;non_debt
I took out 17645 per https://github.com/apache/spark/pull/17645#issuecomment-306907150;non_debt
"This PR adds Text Sentiment Classification examples using Gluon fit() API. This PR depends on its parent PR([14346](https://github.com/apache/incubator-mxnet/pull/14346)).
JIRA epic can be found here: <https://issues.apache.org/jira/projects/MXNET/issues/MXNET-1335>
The examples has been modified from D2L Gluon [book](http://d2l.ai/chapter_natural-language-processing/index.html). This examples shows that the fit() API helps in reducing the training script by approx. 25 lines, and also the target users doesn't need to write their own training loop.
Note: We have covered some of the models, more to be added soon.  
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here";non_debt
Well, sounds great.;non_debt
Fixed;non_debt
fix the GPU flavor problem for Scala;non_debt
"@wzhfy I've looked at the new CBO join reordering. The star schema detection can be used as follows: 
Assume a four-way join: A, B, C, D.
Star schema join detection is called before CBO. It returns **{A, B, D}** as a star join. This info can be used by the the dynamic programming as follows:
* level 0: p({A}), p({B}), p({C}), p({D})
 * level 1: p({A, B}), ~~p({A, C})~~, p({A, D}), ~~p({B, C})~~, p({B, D}), ~~p({C, D})~~
 * level 2: ~~p({A, B, C})~~, p({A, B, D}), ~~p({A, C, D})~~, ~~p({B, C, D})~~
 * level 3: {{A, B, D}, C}
At level 2 we only generate a plan for {A, B, D}. The winner plan should be **{{A, B, D}, C}**";non_debt
We probably want to make a CMake list of libraries to bundle, then iterate over that with `foreach`;non_debt
â€¦haning from meta replica to non-meta-replica at the server side.;non_debt
AU check *successful*! https://ci.trafficserver.apache.org/job/autest-github/521/;non_debt
CLOUDSTACK-9555 when a template is deleted and then copied over againâ€¦;non_debt
"- Why submit this pull request?
- Related issues";non_debt
Not one, two, or three flakes, but four!;non_debt
"Is this while loop still needed after changing the verification approach or `HistoryServer` will not monitor job files created after its start?
Also a side note, would it be a more incapsulated approach to poll `MultipleJobsDetails` similar way in this loop until the expected, non-failed state reached instead of instrumenting `HistoryServer` internals for testing?";non_debt
You should add tajo-storage-kafk module to tajo-storage and add module copy-script in prepare-package;non_debt
Fixed test #464 made [build green](https://travis-ci.org/yahoo/pulsar/builds/241298883) so, I think now we can merge this one or can trigger the build after rebasing.;non_debt
Fix for invalid numpy float indexing;non_debt
updated this with unit test showing saving records with bloom filters off, closing RocksDB then open again with bloom filters enabled, no errors and can retrieve previous records successfully.;non_debt
Adding JDK10 to Windows Jenkins build nodes.;non_debt
oops, missed the update. sorry for delay. LGTM and merging.;non_debt
"What's the difference you see between LimitedPrivate(""management tools"") and Public? When things are LimitedPrivate(""HBase"") there's a specific community that we know to coordinate with. With ""management tools"" it's arbitrarily open, and I don't know what the compatibility guarantees we need to hold to are. I know with Hadoop 3's compatibility we technically violated compatibility guidelines with the original metrics framework being removed before it had been deprecated for a full release, because metrics were also seen as a similar ""administrative"" or ""auxiliary"" interface and it didn't need to really be ""public"". If we're doing that again, I think we can still set expectations more clearly.";non_debt
"This PR enhances `ColumnVector` to keep `UnsafeArrayData` for array to use `ColumnVector` for table cache (e.g. CACHE table, DataFrame.cache). Other complex types such as Map and struct will be addressed by another PR if it is OK.
Current `ColumnVector` accepts only primitive-type Java array as an input for array. It is good to keep data from Parquet.
This PR changed or added the following APIs:
`ColumnVector ColumnVector.allocate(int capacity, DataType type, MemoryMode mode, boolean useUnsafeArrayData)`
* When the last is true, the `ColumnVector` can keep `UnsafeArrayData`. If it is false, the `ColumnVector` cannot keep `UnsafeArrayData`. 
`int ColumnVector.putArray(int rowId, ArrayData array)`
* When this `ColumnVector` was generated with `useUnsafeArrayData=true`, this method stores `UnsafeArrayData` into `ColumnVector`. Otherwise, throw an exception.
`ArrayData ColumnVector.getArray(int rowId)`
* When this `ColumnVector` was generated with `useUnsafeArrayData=true`, this method returns  `UnsafeArrayData`.
Update existing testsuite";non_debt
Hmm...This may can be a problem...;non_debt
"It's not clear for me: cost of what was saved?
I can only see new cost of creating bunch of new, unrelated `Random`s every time `process` or `attempt` are called.
There is no performance evidence or analysis provided in [CAMEL-13499](https://issues.apache.org/jira/browse/CAMEL-13499) neither.
I'd like to learn more about this issue.
From jira:
And if it isn't? Such control, IMO shall be introduced when needed, as it might never come.";non_debt
@cheddar I believe the latest revision addresses your comments;non_debt
sounds good @FrozenGene  i agree with all your points;non_debt
Thanks - merging this.;non_debt
@mcgilman good catch. Going to push another commit to address this.;non_debt
[SPARK-29574][K8S][2.4] Add SPARK_DIST_CLASSPATH to the executor class path;non_debt
@maropu - cool, thanks.;non_debt
Yeah. I removed it. We have fully backwards-compatible behaviour now.;non_debt
33884891-8962 review-441010609;non_debt
We still need the tests for HashAggregateExec here?;non_debt
maybe mention both date formats under `minDate`, and then just say this can take the same formats as `minDate`?  It should be pretty obvious that both versions work for either option, but just to be a little more clear.;non_debt
Ditto.;non_debt
@ewencp Rebased and updated based on comments;non_debt
Oh, code conflict, actually we'd better let this PR merge asap. as some of my PR will depends on this.;non_debt
?;non_debt
pull this into a helper like `HoodieSparkEngineContext.getSparkContext(engineCtx)` ?;non_debt
"# [Codecov](https://codecov.io/gh/apache/kylin/pull/1237?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/kylin/pull/1237?src=pr&el=continue).";non_debt
Jenkins, retest this please.;non_debt
"[patch.txt](https://github.com/apache/lucene-solr/files/4053932/patch.txt)
I reworked a few things, Mike. Now... rat hangs for me (?). I wonder if we could bypass ant and use rat classes directly - the conversion between gradle and ant filesets is a bit clumsy (it's fine as a first draft though!).
Inclusion/ exclusion patterns have to be reviewed and consolidated with ant: some projects override these, gradle should exclude project-local build/**, etc.";non_debt
How do these changes preclude you from later adding in your proposed split generation function?;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-superset/pull/4620?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-superset/pull/4620?src=pr&el=continue).";non_debt
You miss `segment.trim()` and avoid empty string.;non_debt
Should we mock the other new method disconnect() too?;non_debt
"Writing it as `data += (i, j + startCol, v)` would yield compilation errors:
thus here it's written as `data.+=((i, j + startCol, v))`";non_debt
@csantanapr Yes, I changed that line.;non_debt
Build Failed  with Spark 2.3.4, Please check CI http://121.244.95.60:12602/job/ApacheCarbonPRBuilder2.3/5068/;non_debt
@ijuma;non_debt
31006158-4519 description-0;non_debt
Extract that.;non_debt
"Instead of looking for parent in this filter why don't apply the filter on first listStatus done on loadPath?
Also, should skip the files even if the name matches the prefix.";non_debt
nit: how about `localSubpartitions.forEach(ResultSubpartition::flush)`?;non_debt
As the implementation of `subBlock` returns something like `new OffHeapMemoryBlock(this.offset + offset, size)`, doesn't it be relative offset to original offset? `new absolute offset` looks incorrect.;non_debt
Merging to 1.6;non_debt
Thank you for looking at it, Svatopluk!;non_debt
@ijuma Have we explicitly stated in the code or documentation that this class is internal and should not be used by users? Becket is not aware of this as well. It seems that user can already construct Scala AdminClient directly and that is how I expect this API to be used.;non_debt
[SPARK-19909][SS] Batches will fail in case that temporary checkpoint dir is on local file system while metadata dir is on HDFS;non_debt
[SPARK-6661] Python type errors should print type, not object;non_debt
+1;non_debt
"Join on output threads to make sure any lingering output from process reaches stdout, stderr before exiting
CC @andrewor14 since I believe he created this section of code";non_debt
Create new test folder with kamel cli specific tests;non_debt
I see, in that case, maybe you can remember a Boolean the first time you saw your tasks to see whether some of them had hostnames specified but not executors. Those are the ones we're worried about.;non_debt
36057260-1231 comment-297753130;non_debt
Based on the comments it looks like more work is needed and this needs to be rebased.  Please complete this.;non_debt
thanks, merging to master!;non_debt
Seems to be correct given `mvn dependency:tree -Phive-3.1.1`.;non_debt
ping;non_debt
@dongjoon-hyun can you take a look?;non_debt
a/and/is ?;non_debt
why should we order it according to schema order.;non_debt
@hvanhovell Yes. #16785 only does a limited improvement. Both #16785 and this are non-parallel approach.;non_debt
@anirudh2290 can you please review;non_debt
This diff should not be displayed. Can you try rebasing?;non_debt
Should we check the root cause?;non_debt
@mozga-intel let me know once the comments are addressed and I can take another look then.;non_debt
"[cloudstack-pull-analysis #288](https://builds.apache.org/job/cloudstack-pull-analysis/288/) SUCCESS
This pull request looks good";non_debt
hi @aljoscha, do you have any other concern about this ?;non_debt
"@jorgebay I've modified the Sasl Authenticator so that a mechanism can now be passed in along with any mechanism options. I've changed the Sasl Plain Authenticator to mimic the Java handler code as discussed, however I had to do some fudging as I couldn't get the flow to work right. 
From my understanding of the Java code the Driver/Handler/GremlinSaslAuthenticationHandler class receives a 407 from the Gremlin Server and then sends a message containing { saslMechanism: 'PLAIN', sasl: (A base64 encoded null byte string) }, the server then returns an initial response and the client then sends the actual sasl authentication message... I tried that and I was getting a message that ""Authentication ID must not be null"". Having looked at the Gremlin Server code and the Handler/SaslAuthenticationHandler class and the Auth/SimpleAuthentication class it doesn't appear that saslMechanism token is actually parsed and the the server expects the actual sasl message right away. So, currently, in the JS SaslAuthenticator class I've passed the full Sasl message with the initial SaslMechanism argument. 
I hope that makes sense and if it needs changing I'm happy to do so.";non_debt
Yes,  we do delete and insert. Definitely, would be helpful to have someone with more experience review this.;non_debt
"MustFollow can be used to force processing to wait for another stage to complete, when a direct PCollection dependency does not exist.
The unit test only checks pass-through correctness.  I have tested the actual sequencing on DirectRunner and FlumeRunner, and believe the mechanism should be portable to other runners.
This could also be added as a call on ParDo, but providing a standalone PTransform seemed cleaner.
The series of operations adds more to the execution graph than I'd like a dotted-line dependency would be better.  Not sure if this is easy to fix though.
@robertwb 
Post-Commit Tests Status (on master branch)
------------------------------------------------------------------------------------------------
Lang | SDK | Apex | Dataflow | Flink | Gearpump | Samza | Spark
--- | --- | --- | --- | --- | --- | --- | ---
Go | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Go_GradleBuild/lastCompletedBuild/) | --- | --- | --- | --- | --- | ---
Java | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_GradleBuild/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Apex_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Dataflow_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Flink_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Gearpump_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Samza_Gradle/lastCompletedBuild/) | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Java_ValidatesRunner_Spark_Gradle/lastCompletedBuild/)
Python | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Python_Verify/lastCompletedBuild/) | --- | [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_VR_Dataflow/lastCompletedBuild/) </br> [![Build Status](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/badge/icon)](https://builds.apache.org/job/beam_PostCommit_Py_ValCont/lastCompletedBuild/) | --- | --- | --- | ---";non_debt
"I wonder if picking a fixed channel size might be a good idea. Specifically, given there is s one  That way the producers can't get too far ahead. I think a small fixed number (`number_of_cores`? 10?) might be reasonable to begin with
Given that the actual work of `MergeStream` is pretty light (simply passing the record batches on) this may not be a problem in practice, but it probably depends on what order the tasks are run.";non_debt
Retest this please.;non_debt
"This PR fixes the problem of SliceChannel operator only accepting data type float32. It should support any data types supported in MXNet.
https://github.com/dmlc/mxnet/issues/5504
Test script:";non_debt
Will it work for operators that inherit from `DummyOperator`?;non_debt
"Apache Mesos 0.27.1 resolves the problems in 0.27.0.
Now, REEF integration test passes on Apache Mesos 0.27.
JIRA:
  [REEF-1213](https://issues.apache.org/jira/browse/REEF-1213)
Pull request:
  This closes #";non_debt
"[incubator-brooklyn-pull-requests #827](https://builds.apache.org/job/incubator-brooklyn-pull-requests/827/) SUCCESS
This pull request looks good";non_debt
"Hi, @zero323 . Thank you always for your contribution.
Since you are active contributor, I want to give two recommendation.
1. You had better use `[MINOR]` if you don't have a JIRA issue. This is a convention and irrelevant to `JIRA Priority`.
2. For documentation PR, `[DOCS]` is recommended.
In short, you had better revise your title to `[MINOR][SPARKR][DOCS] Remove ...` . I didn't change this PR because I want you to learn for both this and your future PRs.";non_debt
Can resource_request also be added to ndarray.cc? Others may use ElementwiseSum as a black box. Also see https://github.com/apache/incubator-mxnet/blob/master/src/ndarray/ndarray.cc#L667 which request temp resource;non_debt
"https://github.com/apache/kafka/pull/4356 added `batch.size` config property to `FileStreamSourceConnector` but the property was added as required without a default in config definition (`ConfigDef`). This results in validation error during connector startup. 
Unit tests were added for both `FileStreamSourceConnector` and `FileStreamSinkConnector` to avoid such issues in the future.";non_debt
I will merge this later today if there are no objections...;non_debt
I think this is safe for a point release. In the worst case the exception is exchanged for wrong syntax highlighting, but the user should then still be able to work with his/her file.;non_debt
"I think you are right about the argument that we do not expect users to ""handle"" the exceptions in any way, so they may as well be unchecked exceptions.
Should we then throw `FlinkRuntimeException` or define a specific `StateException extends FlinkRuntimeException` that we throw there?";non_debt
45721011-5103 description-0;non_debt
retest this please;non_debt
@bprakg is this a mistake? can you close it?;non_debt
,;non_debt
this also is something we should note down to remove down the line.. for now, makes sense..;non_debt
"This PR is based on #3166 , and added following changes:
1. Refactor `RexProgramExpressionExtractor` and `RexProgramExpressionExtractor` to `RexProgramExtractor` and `RexProgramRewriter`. `RexProgramExtractor` is responsible for retract either projection expressions or filter expression.
2. Make sure we don't fail during extracting and converting filter RexNodes to expressions. The expressions which successfully translated and unconverted RexNodes will both be returned.
3. Add some tests for `RexProgramExtractor`.
4. Provide unified `PushFilterIntoTableSourceScanRuleBase` to support filter push down in both batch and stream mode.
5. Add some logical tests for filter push down in different situations, like fully push down and partial push down.
6. Slight change of testing class `TestFilterableTableSource` to make it less specialized.
Another thing to notice is that UDF expression can not be translated from RexCall to Expression yet, since the name is not consistent with the name in FunctionCatalog due to the change of #3330. We can revisit this issue later since it's not very practicable for a TableSource can understand and execute UDF registered in Flink.";non_debt
"@mpjlu This is the behavior I get:
So, it throws an exception when nothing is set, as intended it seems.";non_debt
"@JoshRosen I don't think the situation is quite a dire as you suggest (every line of test code?).  We can add logic to `QueryTest` and the other base test classes that creates a `SQLContext` per suite with whatever `SparkContext` you want.  We can then turn the data objects into traits that are mixed into the test cases they need.  As long some SQLContext is in scope, and the required tables are added to that context during the constructor I don't anticipate any major problems.
Hive is going to be another story.  The whole reason for this singleton context pattern is that we have problems initializing more than one HiveContext in a single JVM.  If you try to do that all DDL operations fail with a mysterious `Database default does not exist` error.  We have never been able to figure out what sort of global state Hive relies on (though admittedly it has not been a very high priority since a global context with a robust `.reset()` has worked pretty well so far).";non_debt
@shulmanb I've done a coding style-level review.;non_debt
Instead of adding a new method, we can probably just add one more parameter here. Then we don't need to worry about the execution order of `processCommitDuration` and `processStats`.;non_debt
Run Python PreCommit;non_debt
"# [Codecov](https://codecov.io/gh/apache/incubator-openwhisk/pull/3592?src=pr&el=h1) Report
------
[Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-openwhisk/pull/3592?src=pr&el=continue).";non_debt
@XD-DENG @zhongjiajie - Please let me know if you have more comments/recommendations and should I resolve your comments.  Thanks;non_debt
How can we combine two columns with different values?;non_debt
"Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project flink-test-utils-junit: ExecutionException
flink 1.7 not  find 
import org.apache.flink.table.api.Table
import org.apache.flink.table.api.TableEnvironment
import org.apache.flink.table.api.java.StreamTableEnvironment";non_debt
It would be good to add some summary info about JIRA in the same line as the [TRAFODION-1986]. Otherwise it just shows an empty subject.;non_debt
Packaging result: âœ–centos6 âœ”centos7 âœ”debian. JID-1086;non_debt
Sounds good to.check. I will be back after investigating. BTW, I guess the original state does not handle that case too.;non_debt
CUDA has __popc and __popcll http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__INTRINSIC__INT.html#group__CUDA__MATH__INTRINSIC__INT_1g43c9c7d2b9ebf202ff1ef5769989be46;non_debt
All nodes in the cluster have access to the global connector configuration and can respond to REST apis, but the `Worker` class only tracks the connectors being executed on that particular node. To find the type of a connector which is not executing on that node, we probably have to pull the classname out of the config using `ConnectorConfig.CONNECTOR_CLASS_CONFIG`.;non_debt
Mention `SPARK-12058` in the message.;non_debt
Ping @eric-haibin-lin regarding the tracking issue;non_debt
"The file source has a schema validation feature, which validates 2 schemas:
1. the user-specified schema when reading.
2. the schema of input data when writing.
If a file source doesn't support the schema, we can fail the query earlier.
This PR is to implement the same feature  in the `FileDataSourceV2` framework. Comparing to `FileFormat`, `FileDataSourceV2` has multiple layers. The API is added in two places:
1. Read path: the table schema is determined in `TableProvider.getTable`. The actual read schema can be a subset of the table schema.  This PR proposes to validate the actual read schema in  `FileScan`.
2.  Write path: validate the actual output schema in `FileWriteBuilder`.
Unit test";non_debt
ok to test;non_debt
A message doesn't get automatically ACKed if there is the output topic is not set;non_debt
https://issues.apache.org/jira/projects/CALCITE/issues/CALCITE-4251?filter=recentlyviewed;non_debt
ditto;non_debt
"cab you precise what is expected as ""data type""? is it a classname? is a value taken from somewhere elese?";non_debt
@blueorangutan package;non_debt
This buffer is too short.;non_debt
Fixed.;non_debt
@bkietz why remove those DCHECKs? I think it's a good idea to have them, they caught a legitimate issue here (which could be memory corruption, but...);non_debt
+1 Thanks for adding the documentation @revans2 .;non_debt
retest this please;non_debt
should we log the exception here?;non_debt
Retest this please;non_debt
@comaniac @mbrookhart good to go?;non_debt
"Hi @wuchong  Thanks for the update.
+1 to merge.";non_debt
thanks for merging;non_debt
[MXNET-320] Support elemwise_add/sub between dense and csr tensors;non_debt
Are we guaranteed that this string of 3 get().remove() calls will never NPE after the get() and before the remove()?;non_debt
I am afraid currently we can not do this. Because some unit test in zeppelin-zengine & zeppelin-server depends on spark interpreter (ZeppelinSparkClusterTest).;non_debt
@tianchen92 would you mind starting a thread on the ML, it seems that @jacques-n might not have bandwidth.;non_debt
@adrian555, yes, attach() is useful in R. What I mean is that supporting attach() for DataFrame by allowing direct use of column name is so useful? User can simply use <df>$<colname>?;non_debt
I think this mean we now have 2 tests named `TupleTypeTest`.;non_debt
looks like it's calling `super().get_serialized_fields()`, so we should be good for ExternalTaskSensor;non_debt
exclude org.glassfish.web:javax.servlet.jsp from storm-autocreds hbase-server dependency;non_debt
"I guessed it was because the metadata changed. By that, I mean the `org.apache.druid.segment.Metadata` object stored in the segment, which contains the TimestampSpec.
It adds up, I think, since -1 character is the difference between the old default `timestamp` + `auto` (13 chars) and the new default `__time` + `millis` (12 chars).";non_debt
is this relevant to this PR?;non_debt
This is the first part - creating a generic Q2 unblock handler.  Follow up commits will use this to enable Q2 backpressure on HTTP/1.x messages.;non_debt
to remove this if;non_debt
"`executeProcedures()` is taken care of internally using `preExecuteProcedures()` I think, but feel free to include `clearSlowLogsResponses()`.
Thanks @lujiefsi";non_debt
"I am not sure this is not a WARN at all... it happens all the time...
-1000
this is supposed to happen that way... the TransactionManager will commit the Transaction from its own session in a lot of instances.";non_debt
jenkins, retest;non_debt
deps: update kotlin to v1.3.72;non_debt
I just pushed commit without it.;non_debt
[TE][subscription] Fix duplicate anomaly report issue and clean up subscription pipelines;non_debt
"NATableDB is caching a pointer to a HiveClient_JNI object
client disconnects.  Fixing this by keeping the HiveClient_JNI around
across sessions.";non_debt
"Fixed Binary data type in beeline rows to encode to Base64
https://issues.apache.org/jira/browse/HIVE-23856";non_debt
You may be able to check expected parameters with easymock.;non_debt
This is a temporary PR -- opened to verify if tests are passing.;non_debt
I still think we should pass null if HelixParticipantProperty is not there. Having an object but no value does not really benefit us.;non_debt
"Since current additivity setting of `kafkaAppender` is `true` (default value), This causes duplicated logs in the `kafkaAppender` and the `root` appender.  
It would be better default log4j configuration for **production env** if the additivity value of `kafkaAppender` is `false`";non_debt
"Added a check to the configuration sanity checker to ensure that, if a crypto module other than NullCryptoModule was selected, a
SecretKeyEncryptionStrategy other than NullSecretKeyEncryptionStrategy must also be selected (and vice versa).";non_debt
no switch is necessary, you can use pf->CallPacked;non_debt
[AIRFLOW-XXX] Add section to Updating.md regarding timezones;non_debt
+1 tested the change by swapping the 1.5 mongo storage plugin jar with the 1.4 one and it worked.;non_debt
Moved it back. We should address de-unittest-ing the tests in a different PR if we want to do that;non_debt
Travis already came back clean, skipping Travis for just the file name change (non-functional) to address comments.;non_debt
[FLINK-4631] Prevent some possible NPEs.;non_debt
Implement Thrift.Protocol.prototype.skip() for JavaScript library;non_debt
"Sorry, I forgot a `groupBy()` in my example.
It should be";non_debt
Fyi, https://issues.apache.org/jira/browse/ZOOKEEPER-2211;non_debt
Upgrade flask-appbuilder to latest.;non_debt
FreeBSD11 build *successful*! https://ci.trafficserver.apache.org/job/freebsd-github/1966/;non_debt
If this is the actual translation, it can stay, but this looks like an untranslated string?;non_debt
"That would violate our definition of begin well-defined from the javadocs:
`no 2 options exist for the same key with different descriptions/default values`
The organization into separate classes that we have is only for developer convenience and readability only it does not (and must not) have any semantics attached to it. Hence the containing class doesn't matter.
The reason for this is simplicity we don't have to worry about
* options clashing in their default value, which is difficult to document in a good way and can lead to subtle issues when de-duplicating options
* options clashing in their description, which is also difficult to document and usually leads to stale documentation at one place
* options clashing in their type, which may result in a component being unusable when used in conjunction with another
* users not being able to configure distinct values per option";non_debt
"Thanks @bgaborg for testing and providing update.
I can draft a release notes later, but to answer the question of what should we change to enable this:
I'll address Steve's other comments in a new commit, and post testing results with different config settings in auth-keys.xml including default SSE config (AWS owned CMK), AWS managed CMK and customer managed AWS key.";non_debt
K. Just getting a sense of urgency. We might have a 1.10.1 anyway, and if we do I'll pull this in to it.;non_debt
The Travis-CI failure is due to a regression in a Cython 0.28: https://github.com/cython/cython/issues/2148;non_debt
yeah if it doesn't mean adding a dependency on HttpClient into core -- it may happen to come in transitively already;non_debt
"Fix [TS-3752](https://issues.apache.org/jira/browse/TS-3752).
Approach: Collect all Header Block Fragments before decode with HPACK.";non_debt
"maybe use doc_value_mode is more suitableï¼Ÿ
in future, we can use different parser for _source or doc_value mode";non_debt
The CI result shows green CI now, :).;non_debt
Interesting. We have an internal lib/ts/ink_base64.cc, I wonder if it'd be useful to have generic support for all the various Base\<nn\> flavors in the core, and expose those as TS APIs? Not saying this has to be done here, as part of this PR, but maybe it'd be generally useful to have long-term? Similar to how we have;non_debt
LGTM, +1;non_debt
Ditto.;non_debt
alrighty!  things looking good.  the builds i broke (spark-master-test-sbt-*) are good, and i spot-checked one (https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-3.2/324/) and the spark unit tests just started successfully:;non_debt
[FLINK-15548] [table-runtime-blink] Make KeyedCoProcessOperatorWithWatermarkDelay extends KeyedCoProcessOperator instead of LegacyKeyedCoProcessOperator;non_debt
"This PR adds a stored procedure to expire snapshots.
The main author of this change is @liukun4515. I took commits from #1819, rebased, and added more changes to match the current state of procedures.
Resolves #1597.";non_debt
GroovyShell parse script use given context;non_debt
"Manage New scheduler's invoker healthy management
Design document: https://cwiki.apache.org/confluence/display/OPENWHISK/InvokerHealthyManager";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/Ambari-Github-PullRequest-Builder/5605/
Test PASSed.";non_debt
"Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/incubator-trafficcontrol-PR-trafficops-test/639/";non_debt
â€¦ided by the camel-zipkin component;non_debt
"Tumble.over(lit(1).hour.on(orders.rowtime).alias(""hourly_window"")) -> Tumble.over(lit(1).hour).on(orders.rowtime).alias(""hourly_window"")";non_debt
Ambari-24331. Hide not restartable components from Service Auto Start;non_debt
19961085-3969 description-0;non_debt
Merged to trunk and 1.1.;non_debt
I see. Thanks for clarification.;non_debt
refresh;non_debt
Can one of the admins verify this patch?;non_debt
Is this test useful? We don't set the header value in the test.;non_debt
"Thank you for submitting a contribution to Apache NiFi.
In order to streamline the review of the contribution we ask you
to ensure the following steps have been taken:
     in the commit message?";non_debt
But we shouldn't be updating DSL_SQL branch -- that requires force push, and may lose data.;non_debt
ditto;non_debt
@drcrallen @jihoonson Travis is good enough. Now close this PR. Thanks for your comments. :+1:;non_debt
@vinothchandar it worked without changes to /etc/hosts;non_debt
This is PR for https://issues.apache.org/jira/browse/HADOOP-15124;non_debt
this may be a breaking change. @davsclaus , @ffang  could you have a look?;non_debt
@rmetzger / @zentol can you please advise on which `TravisGroup` annotation I should apply?;non_debt
Fixed. Thanks!;non_debt
Get it.;non_debt
let's go option 1 then;non_debt
Can one of the admins verify this patch?;non_debt
@chamikaramj, I think we're ready for a review;non_debt
/pulsarbot run-failure-checks;non_debt
@MikeThomsen ok, sorry, I am too impatient :D;non_debt
Generalization of the WIP API gateway controller route for supporting system packages as API handlers;non_debt
OK, closing.;non_debt
should also check have the following checks;non_debt
STORM-2652: fix error in open method of JmsSpout;non_debt
Agree! This has an external change. Just let me know if we can do it. Thanks!;non_debt
Hi, I think it's correct now, here is the code view form my patch.;non_debt
@guozhangwang;non_debt
Trying that right now;non_debt
oh jira already closed. excellent :);non_debt
Ping @mboehm7;non_debt
"What is the relationship between ""dataflow counters"" and ""metric updates""?  This comment refers to counters, while the code below refers to metric update protos.  Should this comment instead mention translating accumulators for metric updates so that this class should be renamed accordingly?
Also, the naming of the methods below is confusing: two arguments are passed, e.g. set_boolean(accumulator, metric_update_proto).  Just reading this signature suggests that the accumulator will somehow have a boolean set from metric_update_proto, which is the opposite of what is intended.  Can you instead rename these methods translate_*, e.g. for translate_boolean(accumulator, metric_update_proto)?  This would make it clear that we are translating accumulator into metric_update_proto.  If you do this, can you rename set_scalar and set_mean in this file as well?";code_debt
"Thanks for your contribution @zentol. I've gone over the code and made some inline comments. My main concern/question is actually the representation of metric's type and hierarchy information. I think that encoding it in a string and then re-parsing it on the receiver side to reconstruct the information is rather fragile and error-prone especially wrt maintainability. Maybe you can give me some background why you decided to do it so.
Apart from that, I think the code contains many tests, which I really like :-)";design_debt
nit: use local variable if possible;code_debt
TINKERPOP3-957: Improve speed of addV();code_debt
"Agreed.
You may have gotten that impression because I'm against bundling. But, I'm also against using Hadoop's bundled libs for same reason I'm against bundling our own. I'm in favor of intentional and thoughtful dependency convergence, as a downstream activity. I'd actually prefer we not ship any binary tarball packaging... but since we do, we might as well do it in a way that works well for most users. In any case, I agree with you, this can be improved once we get the basics in. :smile_cat:";design_debt
Made the changes to improve logging.;code_debt
"I think both ways have some pros and cons. I personally think that using a single endpoint is easier for users?
* The main purpose of online AD endpoint is to provide a convenient way to run AD tasks so the endpoint should be as simple as possible. Using a single endpoint is more user-friendly. Using a separate CRUD endpoint does allow more flexibility but it will ask users to firstly register their data before using this service.
* Secondly, online service will not allow much too large size of data so in my sense, sending data in the request every time is not a bottleneck?
I think we could provide both ways. This is phase 1 for this feature. In phase 2, we probably could support another two endpoints to support what you suggested. 
Regarding the cleanup, I think currently in phase 1, it is just a one-call request so I mentioned this as stateless because users will not have a separate endpoint to retrieve anomalies afterwards and hence we could clean up them. In phase 2, another two endpoints will be provided and for those endpoints, we do not need to do the cleanup.
Thanks for your suggestions!";design_debt
Maybe you also need to add null checks in process... up to you.;code_debt
ARROW-7473: [C++][Gandiva] Improve error message for locate function;code_debt
yes, I think earlier code was not formatted with formatter that caused this reformatting change.;code_debt
Use log message formatting `%d`;code_debt
Adding .rat-excludes, readme for rat and remove unneeded files lacking copyrights.Also hive/TEST009 fix.;code_debt
Doesn't look like columnReference is used after this. Is this a memory leak?;design_debt
"I'm a little confused about why this needs to be a config. Isn't this just metadata written to the state store? Why would a user ever need to set this?
I have to go over the rest of the code with more care, since I'm really not familiar with it. But it looks like an insane amount of code just to add one single field to some state object...";design_debt
yes, let's worry about this later.;design_debt
Also add tests and a few clean-ups.;code_debt
I finally made it through all of the code.  It looks good for the most part.  Just a few minor comments.  I also am wiling to maintain/support this code. I have a umber of customers who I know would be very interested in using this, so I would be on the hook for supporting it anyways :);code_debt
"This is so specific to the way YARN runs things that I don't think it would be useful anywhere else. If at some point it becomes useful, the code can be moved.
I think the tests I added are better than just unit testing this function, since that way the code is actually being run through YARN and bash.";code_debt
docs: Updates to Superset Site for 1.0;code_debt
There are _12_ new overloads of `createStream` on top of the existing 4. This seems like big overkill. There should be one version in Java/Scala that takes all arguments, one each that takes minimal arguments, and any others needed to retain binary compatibility. The rest seem superfluous.;design_debt
"Should we take a step back and reconsider whether updating the current API in HttpOp is the right thing to do.
Maybe we ought to
- introduce a different style more fluid
- reconsider a design where the caller is responsible for setting up more of the `HttpClient` and `HttpOp` provides operations for nothing special (no auth) + ops that use a redefined `HttpClient`. This is to reduce method bloat.
On (1): something like (quick sketch)
where there are implicit builder objects from, `.get(url)`, `.post(url)`, `.put(url)`, `'delete(url)`.";design_debt
@Nonnegative int keyGroupId, for consistency;code_debt
"It seems possible to collapse at least some of this class hierarchy, but I was told that there's future plans for making subclasses of the PrinterIface class. There just seems like a lot of class hierarchy (up to 4) for such a small amount of code, and I really don't think we should make provisioning now, for something that *might* be done later. If it becomes an issue later, it's easy to refactor (don't do premature ""abstractions"" :-).";code_debt
"Instead of using this separate class and do the `instanceof` check on each call (which maybe expensive), maybe we could just have a `WrappedBatchingStateRestoreCallback` which only takes the non-batching `StateRestoreCallback` in constructor and then in `restoreAll` always do the for-loop, and in places that we need it (seems we only have two callers) we can do sth. like
Just once.";code_debt
Hmm... you'll also need to change `taskEndReasonFromJson`, otherwise the history server won't see the new information. Also, when adding new properties to serialized classes, we generally use `Option[Foo]` (see calls to `Utils.jsonOption` in this class).;code_debt
so we should not change this value without better knowledge about it.;code_debt
nit: commented out code bothers me. It's in a storybook, though, so I won't strongly object.;code_debt
You probably *really* don't want to do this. Converting the response to a string causes the client to download the **entire** response to memory, then convert it to a Java string. Doing so completely negates the disk-based buffering that seems to be implemented below.;code_debt
nit: indent two spaces;code_debt
nit: `clone()`;code_debt
consider converting the following conditions to methods, this would assign a name to the condition, and possibly would make it a bit easier to test.;code_debt
Pretty sure that such a long duration isn't really necessary, but I don't think it hurts to make it longer just in case.;code_debt
The second parameter is better to be a complete sentence.;code_debt
"Impala is the only reference I can find https://www.cloudera.com/documentation/enterprise/5-10-x/topics/impala_show.html
To be consistent with the other SHOW function, we can make LIKE optional?";code_debt
@mridulm yes I'm aware of that. But before, SparkContext was using reflection to instantiate two different classes in the yarn package, and then connect them manually. I removed one of those (see that there's still reflection code to load `YarnClusterScheduler`) because it seemed unnecessary.;code_debt
"In very rare cases, it might. I want to change the `Execution` a bit on the `master` to make this unnecessary.
However, that is too much surgery in a critical part for a bugfix release, so I decided to be conservative in the runtime code and rather pay this price in the tests.";code_debt
i think this is already defined in #6317 so it should not be here again ?;code_debt
Yea this is right now used by a lot of test code. Somewhat annoying to update those.;design_debt
I tried and don't see a better way to organize the code. We need to have slightly different logic for each primitive type;design_debt
"I'd say it is not sufficient.
As a workaround for most cases, it works, and personally I'm OK with it as is. However, I think the behavior of `TSStringPercentEncode` shouldn't be changed because it's just a workaround for logging issue.
So, if the API keeps current behavior, then I'm fine with landing this change.";code_debt
instead of string formatter - can we use string builder so that we don't have to count the number of formatting arguments - often it takes a couple of iterations to get this right?  String Builder now will be good especially with the CLI options, thoughts?;code_debt
I'm thinking of taking over this PR and am considering dropping this logic since it adds complexity and might not be a huge performance issue in practice. Let me know if you disagree.;code_debt
This is not necessary. See discussion at https://issues.apache.org/jira/browse/SPARK-11337. You can include all imports in a single block or move unused imports to a separate group. We only need to keep imports ordered in each group. Use empty lines to separate import groups.;code_debt
"`scalar` in linear algebra, relative to `vector`. refer to SQLAlachemy and Pandas variable naming. 
`isArray` more straightforward, I changed this variable name.";code_debt
Updated PR to improve readability (https://github.com/apache/hive/pull/1780/commits/37e707705c6cc7c025bbe7fb1b67682062dc84c7) and at the same time address HIVE-24646.;code_debt
OK, in the name of keeping it simple I might not touch this this time. Since this occurs 2 places only, it doesn't save much.;code_debt
We have unmodifiable map alternatives in the JDK. If there is not a good reason why we need guava here, I would suggest to solve this without this import. Even in that case, I would use import and not fully qualified classname.;code_debt
"As noted my biggest concern is that message refs need to be as light as humanly possible as theyâ€™re all in memory and affects greatly the scaling.
I would personally prefer the refactor if needed, than take this hit. 
Especially as this is only needed by someone wanting to use this in a plugin. Which means everyone else has to suffer";design_debt
LGTM. Merging to trunk and 0.10.1. Thanks @edoardocomar and @mimaison for the hard work (and patience) on this patch! I have some minor cleanups/improvements on the client and in testing, which I will submit in a follow-up PR.;code_debt
Spacing here. Is this simpler with .forall?;code_debt
"@VenuReddy2103
 I checked all places, but 14 places still keep the same with previous.
1) in 9 places, it uses only one event, fireEvent is ok. 
2) in 4 places, preEvent and postEvent are in the different code blocks.
3) in 1 place, there is a large code block between preEvent and postEvent.";code_debt
"This will require to update this file for each release which is not ideal. We could get the list of versions from `versions.json` though the maven install might take a while to complete.
Another option would be to just build the latest version and make sure we add the version tag, so that we just build 1 version (since the older docs will not change).";design_debt
indentation;code_debt
Is this the implementation class? If so can we use `class` instead of type to be more explicit?;code_debt
"I see you added a test, but you still haven't addressed my question about synchronized on .class.
Why is that needed?";code_debt
"Rewrite the tuple construct and consume work flow to improve the read and write performance in pluggable storage framework. To be specific, it uses virtual tuple in tuple table slot instead of heap tuple between storage and executor to avoid tuple data copy during query processing.
@huor @jiny2";code_debt
Could you add `@deprecated` to give a proper warning?;code_debt
"What about using `rules(RelTrait in, JdbcConvention out, RelBuilderFactory relBuilderFactory)` here, but with null `in` value and adding if statement in that other overloaded method?
It allows slightly avoid duplication of code";code_debt
"What a mess...
On Mar 16, 2018 5:12 AM, ""Carlos Santana"" <notifications@github.com> wrote:";code_debt
"This is pretty hacky. It makes assumptions about how things are formatted in the event log.
The previous assert should be enough (ignoring my previous comment about changing this test).";code_debt
"minor, but `org::apache::nifi::minifi::` is not needed
in a few other places, too, `org::apache::nifi::minifi::utils::StringUtils::toBool(...)` could be shortened to `utils::StringUtils::toBool(...)`";code_debt
"Okay I think the issue is pretty tough. Unfortunately hive is directly using the shaded objenesis classes. However, Spark needs Kryo 2.21 which depends on the original objenesis classes.
Here is the hive code that uses it:
https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L186
So we can't just remove kryo that hive uses. This is pretty ugly. One solution might be to update chill in Spark so that Spark is using the same Kryo version as Hive.";design_debt
"I'm wondering if it would make sense to insert a layer between AbstractFeature and the actual features like:  
and then each of the subclasses would look something like:
and a lot of the duplicate code in each of the subclasses goes away. 
Thoughts?";code_debt
I think its better to use org.wso2.carbon.utils.CarbonUtils.getCarbonHome() here.;code_debt
"I tend to agree that the terminology used here is a little confusing, and doesn't seem to match up with the ""general"" terminology (I use that term loosely however).
In my dealings with LSH, I too have tended to come across the version that @sethah mentions (and @karlhigley's package, and others such as https://github.com/marufaytekin/lsh-spark, implement). that is, each input vector is hashed into `L` ""tables"" of hash signatures of ""length"" or ""dimension"" `d`. Each hash signature is created by concatenating the result of applying `d` ""hash functions"".
I agree what's effectively implemented here is `L = outputDim` and `d=1`. What I find a bit troubling is that it is done ""implicitly"", as part of the `hashDistance` function. Without knowing that is what is happening, it is not clear to a new user - coming from other common LSH implementations - that `outputDim` is not the ""number of hash functions"" or ""length of the hash signatures"" but actually the ""number of hash tables"".
In terms of `transform` - I disagree somewhat that the main use case is ""dimensionality reduction"". Perhaps there are common examples of using the hash signatures as a lower-dim representation as a feature in some model (e.g. in a similar way to say a PCA transform), but I haven't seen that. In my view, the real use case is the approximate nearest neighbour search.
I'll give a concrete example for the `transform` output. Let's say I want to export recommendation model factor vectors (from ALS), or Word2Vec vectors, etc, to a real-time scoring system. I have many items, so I'd like to use LSH to make my scoring feasible. I do this by effectively doing a real-time version of OR-amplification. I store the hash tables (`L` tables of `d` hash signatures) with my vectors. When doing ""similar items"" for a given item, I retrieve the hash sigs of the query item, and use these to filter down the candidate item set for my scoring. This is in fact something I'm working on in a demo project currently. So if we will support the OR/AND combo, then it will be very important to output the full `L x d` set of hash sigs in `transform`.
My recommendation is: 
1. future proof the API by returning `Array[Vector]` in `transform` (as mentioned above by others)
2. we need to update the docs / user guide to make it really clear what the implementation is doing
3. I think we need to make it clear that the implied `d` value here is `1` - we can mention that AND amplification will be implemented later and perhaps even link to a JIRA.
4. rename `outputDim` to something like `numHashTables`.
5. when we add AND-amp, we can add the parameter `hashSignatureLength` or `numHashFunctions`.
6. make as much private as possible to avoid being stuck with any implementation detail in future releases (e.g. I also don't see why `randUnitVectors` or `randCoefficients` needs to be public).
One issue I have is that currently we would output a `1 x L` set of hash values. But it actually should be `L x 1` i.e. a set of signatures of length `1`. I guess we can leave it as is, but document what the output actually is.
I believe we should support OR/AND in future. If so, then to me many things need to change - `hashFunction`, `hashDistance` etc will need to be refactored. Most of the implementation is private/protected so I think it will be ok. Let's just ensure we're not left with an API that we can't change in future. Setting `L` and `d=1` must then yield the same result as current impl to avoid a behavior change (I guess this will be ok since current default for `L` is `1`, and we can make the default for `d` when added also `1`).
Finally, my understanding was results from some performance testing would be posted. I don't believe we've seen this yet.";design_debt
Perhaps renaming this variable to something like `DEFAULT_STORE_TYPE` would clarify the reason for declaring it here as opposed to just using the enum value where necessary.;code_debt
IGNITE-11654: [ML] Memory leak in KNNClassificationModel;design_debt
"I think the SparkSession.close() behavior is on purpose, and that's a coherent behavior (i.e. just don't shut anything down until you're done, and then everything shuts down). What's not consistent with that is maintaining some state in the session that can't be cleared. 
I think the ways forward are probably:
- A new lifecycle method like `clear()`? more user burden but at least provides _some_ means of doing cleanup without changing `close()`
- Figure out how to automatically dispose of those resources or not hold them
- Just change the behavior of session's `close()` to not shut down the context. Behavior change, yes, but perhaps less surprising than anything.
Eh, do people like @cloud-fan or @gatorsmile or @HyukjinKwon or @dongjoon-hyun have thoughts on this? I feel like reference counting is going to end in tears here eventually, but, it's not crazy";design_debt
"@shangxinli,
If we will agree on the extending of the schema with metadata is a good idea and as you said the serialization/deserialization is also required we need to change the format first. The schema objects in parquet-mr are only exist in the parquet-mr runtime. To have them serialized we need to convert this object structure to the thrift object structure defined in the format. If we don't have the new metatdata fields in the format we cannot serialize/deserialize them. So it is a much bigger topic. Also, I'd like to see this feature separated from the encryption as it would be general approach for storing metadata in the schema. Meanwhile, I am not convinced that we need to have such extension.
About the namespace prefix etc. I don't agree this is not user friendly. That's why I've suggested to implement a helper API so the user doesn't need to deal with the conf keys (and values) directly. 
@ggershinsky,
I don't agree we cannot have a meeting about this topic in terms of transparency. What we have to do is to document here about what we have discussed and what are the conclusions. Meanwhile, I am not sure if a meeting would help but I am happy to participate if anyone thinks otherwise.
Also, if we think we are getting stuck with this issue I would suggest involving other members of the community. Maybe draw their attention on the dev list about this PR or bring up the topic on the next parquet sync.";design_debt
"Design doc writes could fail on the target when replicating if with non-admin
credentials. Typically the replicator will skip over them and bump the
`doc_write_failures` counter. However, that relies on the POST request
returning a `200 OK` response. If the authentication scheme is implemented such
that the whole request fails if some docs don't have enough permission to be
written, then the replication job ends up crashing with an ugly exception and
gets stuck retrying forever. In order to accomodate that scanario write _design
docs in their separate requests just like we write attachments.
Fixes: #2415";code_debt
At this pointm it may be worth wrapping the getXOrThrow calls to actually return a legible error message since the generated code does not print the input ids.;code_debt
nit: `left_anti`;code_debt
"Would it be acceptable to reformat the code when such a demand appears, or after I'm finished with these improvements? These ""some people"" might never need to look into Hunspell code.";code_debt
nit: this line is long enough its time to split;code_debt
I think this method is not really necessary. We already have logic for accessing composite types (see `visitFieldAccess()`). Maybe we just have to make the methods there a bit more generic.;code_debt
minor nit:mark return type as nullable;code_debt
"this warning seems can be a bit frequent, instead of this, do
Do CHECK_GE(sorted_order_.size(), threads_.size())";code_debt
We only needed it here because the we were trying to ensure we had a chance to send the Abort before going into poll. After thinking about it, it seemed a little simpler to move the `hasAbortableError` check to `maybeSendAndPollTransactionalRequest`. Then we no longer need this change.;code_debt
Mmm.... infra will complain if we use jenkins builds as an update center. Also note this is extremely slow...;code_debt
pypi can use categories for better description and version number was out of sync;code_debt
Unused default constructor.;code_debt
"- Remove `lodash.throttle` from dependency since there is now `lodash`.
- Add `babel-plugin-lodash` that helps optimize bundle output by taking only necessary part from lodash instead of the entire bundle.
https://github.com/lodash/babel-plugin-lodash
- Replace `underscore` calls with `lodash` where applicable. 
@williaster @xtinec @conglei";code_debt
Do you mean I should remove the `addPKColumns` variable and change the if statement to something like the following:;code_debt
nit: stick at the top (line 20) with the gab above the org.apache stuff. We can at least try to not make import ordering worse;code_debt
Is it possible to reverse this (`SqlTypeFamily.BINARY.equals(family)`)? That is the preferred style and also eliminates the need for a null check.;code_debt
"Fixes #5589 
It seems that there is a memory leak in the pulsar-function-go library.
I implemented a simple pulsar function worker that just write logs using pulsar-function-go/logutil for sending logs to log topic. I tried to long-term test by sending request messages consecutively to the input topic to check the feasibility.
During the test, I faced `ProducerQueueIsFull` error with `--log-topic` option. And I observed indefinitely grown memory usage of the pulsar function worker process.
Clear the `StrEntry` variable after finish addLogTopicHandler() function regardless of the log messages are appended to logger or not. If it is not cleared, it causes memory leak because StrEntry has grown indefinitely. Moreover, if the function set --log-topic, then the topic could get accumulated huge messages that cause ProducerQueueIsFull error.
Verified it by reproducing step described in the issue #5589.";design_debt
OK, yeah I see that in the docs, though it's not set up that way in CDH (at least, maybe my installation never needed to configure that file a certain way, dunno). It seems bad to silently ignore unreadable files, so at least log it maybe? then... should it be a warning? because it sounds like there's one file that could reasonably be expected to be unreadable. Do we special case it and warn on anything else? fail on anything else?  i'd rather tighten this up in some way from doing this silently.;code_debt
We should remove Config.UI_ACTIONS_ENABLED, or at least deprecate it.;code_debt
"1. Not sure about it but we can introduce additional constraints like the total size of scanned bytes as you suggested to further improve this feature later. 
2. This is correct. By itself, it does not eliminate. However, the client can wait for all the page operation to complete or fail before returning to the application, as an additional improvement. This will further reduce the race conditions. I think we have to enforce the client side timestamp to make the race almost impossible.
3. I expect this feature will improve the overall performance and availability since paging limits the memory usage and the time to hold server resources. My experience with paging on a real cluster is very positive.  I have not seen any negative impact yet as long as the page size is not very small (e.g., less than 1000).";design_debt
"Default value of number of delta commits required to do inline compaction (hoodie.compact.inline.max.delta.commits) is currently 1. Because of this by default every delta commit to MERGE ON READ table is doing inline compaction as well automatically.
I think default value of 1 is little overkill and also it will make MERGE ON READ work like COPY ON WRITE with compaction on every run. I am propsing to increaset the value to old default of 10.
This pull request is a trivial rework / code cleanup without any test coverage.";design_debt
"As noted in personal discussion:
I found it hard to grasp the code because we use `Unit` returning functions which resolve a `Promise` at some point. This reads like it blocks the code until the result is there (which makes no sense at this point).
I'ld prefer to use `Future` composition if possible so it becomes more apparent that this is in fact returning a Future which is resolved as soon as the ack-message is there.";code_debt
"The ""data:"" parser you wrote would be a duplicate code because it's already implemented in the in-house URL class I wrote.  I'm not going to make a change request because I don't want to delay this feature, but either of the parsers should be removed eventually to eliminate duplicate code.
Also, because environment variables are general enough as data sources, I think ""env:"" parser should be added as a `URLStreamHandler` so that other places can use it too via the in-house URL class.
As for ""token:"", I'm ok with having it in this plugin as a special case. But if you renamed it to ""raw:"", it could be used on other places.";code_debt
"This PR fell in a crack. Let's revive it. Seems like the long term solution is not using WTForms at all as we rip out more of the MVC and FAB scaffolding over time.
So I get the `NULL` vs `''` problem with unique constraints not applying properly, but other than that, does it affect users directly? I'm guessing it may affect the filtering functionality in the CRUD list view UI?";design_debt
Personally think that this place is worth discussing. Most of the exceptions have been handled in connect and disconnect, and there is a log. I think the log here can be removed.;code_debt
"That should not be a problem as in most cases, processing a single `element` in this case means emitting quite many elements downstream. The cost of the structuralValue should be pretty much amortized I would say. We could do something to calculate it only once per element?
I don't think we can use a reference either, because the UnboundedSource is Serializable and any runner is free to clone it (which is what DirectRunner does, afaik). The best solution would seem to be to mark each initially split (via @SplitRestriction) restriction with unique ID and then transfer this ID to all residual restrictions. There should be always be at most one ""active"" (either currently being processed or having non-null residual) restriction, so we could use that for identifying the reader without referencing Source (which might be problematic, as implementing hashCode and/or equals for UnboundedSource will not be a common practice, I'm afraid).";code_debt
"- section title is Interoperability of Language Classes and Types
- corrected namespaces (packages)
- removed duplicate table captions
@davebarnes97 @joeymcallister @dgkimura @mmartell @echobravopapa @PivotalSarge Can you please review these (mostly namespace) changes in the docs?";code_debt
"If we can cleanup the variables names above I think it would help a lot, the test is confusing.  I know you just copy and pasted but would be nice to clean up.
Also can we have 3 tests or 3 asserts,  
- one for same file in --files
- one for same file in --archives
- one for same file in --files and --archives";code_debt
"Very minor - but I'm guessing the first error condition will happen way more often than the second - and getting a message that says it's ""invalid"" is a lot less clear than just saying it doesn't exist. What about breaking these out and including more specific error messages in either case? Also it might be nice to print the value of `fileSystem` in the error message, so it knows what filesystem we inferred the path to be from.";code_debt
"This method doesn't always create a new DeployRequest - can we name it appropriately? Say ""addOrModifyDeployRequest"" or ""getDeployRequestForContainerId"". By the way, would it be an error in some cases to find an existing DeployRequest for a given containerId? For example when you are calling this from StreamingContainerManager.scheduleContainerRestart(), wouldn't it be an error to find an existing request? If true, we should add code to detect such errors. The caller passes a parameter to indicate that only new request is expected to be created and so on...";code_debt
We shouldn't remove this... This is intended to be a general purpose class for logging that handles compression and buffering and other logic. I notice that you replaced this with PrintWriter elsewhere. Even with the new naming scheme you can still use this class for event logging, and it will simplify `EventLoggingListener` a bunch.;code_debt
"@rdhabalia Maybe the name `NonDurableCursor` doesn't fully convey the intended semantic for the new class. The context here is just to have a way to read through a topic (eg: support `TopicReader`) and reuse as much code as possible from the regular cursor implementation. Basically all the cache code and the logic for how to switch to ""next valid position"", plus the asyncReadOrWait stuff.
About non impeding messages to be deleted, consider that in case of non-durable topic, during a disconnection the cursor will go away and data will get potentially deleted anyway. So I prefer it to be explicitly ingrained into the API. Same thing about naming the cursor. It will go away in any case after a restart, so I don't see the advantage of naming it.
When data gets deleted, the cursor will just skip over it. The intended usage for the non-durable cursor and topic reader is in conjunction with the message retention, to make sure data sticks around for the intended amount of time";code_debt
"Well, my point is, that the check can be simplified. I don't think that `record.headers() == null` can be true it's guaranteed that there is a headers object.
Not sure if we can simplify the second check. It iterators over the headers map and does String comparison to find a header with key `v` -- seems to be rather heavy.";code_debt
I would suggest to change the name `PRETTY_JSONISE` to `JSON_PRETTY`, also the string value and test method names below. Using a unified name convention will benefit new developers when they do a code searching.;code_debt
ZEPPELIN-3138. checkstyle for zeppelin-interpreter;code_debt
"I think you can create the callback once in the open method and then pass the instance to all async calls.
This way, you save a lot of instance creations";code_debt
"This works, but it asks for a lot more columns and rows than we need.
We could try changing the return inside this function from `return tis.all()` to just `return tis`, and this line could become:
https://docs.sqlalchemy.org/en/13/orm/loading_columns.html#load-only-and-wildcard-options
Do you think this is worth it or not worth it?";code_debt
yes, if max input is sys.maxint, then we will get 57. I increase bucket size by 1 just for safe purpose, maybe unnecessary. But I don't think one more bucket per counter will cost too many memory.;code_debt
"so we should definitely fix the /api/v1/applications/<app-id>/logs to go through the acls.  It looks like it should be protected in ApiRootResource.java. You have the app id so it needs to do something like the withSparkUI to get the acls included in that application.
Like I mentioned the listing (/api/v1/applications) and /api/v1/applications/<app-id> (which is same info I believe as listing) were intentionally left open.  I don't really see a reason to change that but if other people have a use case for it then perhaps we should make which pages are protected by acls configurable.  
on the history server I would expect spark.acls.enable=false and spark.history.ui.acls.enable=true, I can see where that could be confusing, perhaps we should document this better. spark.acls.enable on the history UI really is protecting the root UI, not the app level ui's.  We could explicitly turn this off.";code_debt
"Hm, this should have been caught by Scala linter because we follow Java style comment. See ""Code documentation style"" in http://spark.apache.org/contributing.html";code_debt
As above: `stores` should never be null, and thus we don't need this change? Also the check for `isEmpty` does give us much, we can still call `addAll` even it `stores` is empty?;code_debt
"@hzxa21 Thanks for the code review.
1)	This consistently happens on Windows platform. I have seen few people complaining similar issues for some Linux flavors, but consensus is this is not an issue for most of the popular Linux flavors. This is a classic windows problem where if a handle is open for a file, NTFS does not allow it to be renamed / moved / deleted. All the file handles open on the file are required to be closed before renaming them. This is not much of an issue with Linux because of the way Linux file system is implemented. The way files are implemented in Linux, it allows to rename / move / delete files even if there are open handles to them.
The way I think about this patch is, before renaming any files / directories, we should always close the file channels. This is applicable to Linux as well. There are 2 fundamental scenarios. 
a)	Deletion of directories since the topic is deleted and deletion of segment files by cleaner thread. We believe, we should close the log file in case of log deletion and close the segment in case of segment file cleanup. Since no one should be reading these files, it is safe to close.
b)	 During swapping flow, where â€˜.cleanedâ€™ files are renamed to â€˜.swapâ€™ files and then they are renamed to actual log files. Here also we believe segment should be closed during renaming and re-opened after the swap.
Overall gist is, files should be closed before renaming them and should be opened back if required. This should be applicable to Linux as well
2)	Yes, initially we implemented the change in rename files method. But that change turned out to be 
ugly. Here is the abandoned pull request done such way.
https://github.com/Microsoft/kafka/pull/17/files
Problem was, there was no need to re-open â€˜.deletedâ€™ files. So we had put an if check to not to re-open â€˜.deletedâ€™ file. Also, it resulted in changes at many places.
Also, I felt like it was not a cleaner fix where we are not solving the root problem. Which is to close the segment before they are renamed / deleted. Also, we wanted this to be a generic fix, rather than specific to Windows.
3)	Yes, I can add some test cases. We wanted to get a initial feedback on the idea before adding tests.";code_debt
"* Do not write new buffers if connection is CLOSED_WRITE
* Do not call connection_wake if CLOSED_READ or CLOSED_WRITE
This fixes crashes but there is still work left with leaking messages and buffers when server connections close before client connections.";design_debt
Format the code to be consistent with old functions;code_debt
Makes sense. I removed these assertions completely, as they are unrelated to the fix. Also added comments to the tests.;code_debt
I just feel increasing timeouts won't cut it. Doesn't sound like a real solution. Unfortunately, I don't understand much about schedular jobs to have a proper say in this but I think we have some performance issues. Need to figure out how to deal with that.;design_debt
We may not add a class only for convenience, may need more design or inline into original class;design_debt
AFAICT, this is never used, so I removed it.;code_debt
Why `rethrow` as `RuntimeException`?;code_debt
Is this commented out code no longer needed?;code_debt
Why are deprecated methods being used?;code_debt
I just removed this detail since we are saying these libraries are deprecated and we no longer want to use them.;code_debt
"Our existing withColumn for adding metadata can simply use the existing public withColumn API.
The existing test cases cover it.";code_debt
I think we can back that out if there's any question to keep this limited. @JoshRosen Yes that's where we ended up again, now that it's clear that there are several ways and several places this can happen. It's easiest just to ignore the exception.;code_debt
HBASE-24791 Improve HFileOutputFormat2 to avoid always call getTableRelativePath method;code_debt
"Jira: https://issues.apache.org/jira/browse/SPARK-18035
In HiveInspectors, I saw that converting Java map to Spark's `ArrayBasedMapData` spent quite sometime in buffer copying : https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala#L658
The reason being `map.toSeq` allocates a new buffer and copies the map entries to it: https://github.com/scala/scala/blob/2.11.x/src/library/scala/collection/MapLike.scala#L323
This copy is not needed as we get rid of it once we extract the key and value arrays.
Here is the call trace:
Also, earlier code was populating keys and values arrays separately by iterating twice. The PR avoids double iteration of the map and does it in one iteration.
EDIT: During code review, there were several more places in the code which were found to do similar thing. The PR dedupes those instances and introduces convenient APIs which are performant and memory efficient
The number is subjective and depends on how many map columns are accessed in the query and average entries per map. For one the queries that I tried out, I saw 3% CPU savings (end-to-end) for the query.
This does not change the end result produced so relying on existing tests.";code_debt
HDFS-15731. Reduce threadCount for unit tests to reduce the memory usage;design_debt
I thought about it before, but it is a bit tricky to do since it could be dynamic based on which class the `toString` function is triggered first.  Let me think about it more and see if I can come with a clean solution.;code_debt
Seems it'd be worth factoring this out into a context (including the expansion service jar test above).;code_debt
We should still get rid of setup_requires, but this just might not be the _complete_ solution.;design_debt
This is redundant.;code_debt
Nit: Redundant conversion;code_debt
Per our coding standards, please wrap all `if` blocks in curly braces.;code_debt
"this seems a little more complicated than is really necessary for the what you're doing here.  couldn't you achieve the same thing by leaving the original code and changing the one line above the original to:
not exactly the same -- it also allows whitespace around the scheduling mode, but maybe a good thing?";code_debt
nit: add this blank line back in.;code_debt
Is this TaskSet index really needed? We should avoid rely on the ordering of TaskSets to verify the results.;code_debt
"Also maybe it is completely unnecessary to automatically attach a source timestamp if we don't have any windowing operators. 
One other thing that came into my mind: in order to keep ""deterministic"" results after failure we should persist data with the timestamp attached at the sources. Are we planning to do this? I guess this question goes hand in hand with the automatic source level backup even without kafka. I just wanted to bring it up.";code_debt
"need space before ""+""";code_debt
`null != sortScope` can be removed, it is checked inside `CarbonUtil.isValidSortOption`;code_debt
"I'd think we explicitly do not want to have the prewarm pool survive, if other actions would benefit from using that space. After all, it's a performance optimization, not a guarantee.
Wouldn't this also be plumbed into the controller for it to not send requests down a path where they might not get executed?
I think this warrants a dev-list discussion ðŸ¤”";code_debt
`np.uint64` -> `np.int32` (to be consistent with Scala implementation);code_debt
"maybe just add a ""finally"" instead of a ""then""?";code_debt
"While working on other issues, I found this empty class. I am proposing its removal as it is not used and can only cause confusion.
Locally
Testing";code_debt
The scala code is almost the same as Java, so no need to create a separate tab for scala , we have also the case that Java/Scala share the same code demo, such as  Filesystem connectors..;code_debt
"Doing just one operation for fluent seems inconsistent. `Context` manipulation isn't (shouldn't) be that common an operation.
See also the quite recent `Context.mergeCopy`.";code_debt
"Yea default values can be hardcoded.
And also agree that showing a precise message for the used config will be better";code_debt
"Really good work @r-pogalz. I had only some minor comments concerning style and test cases. 
I like your approach to split the implementation of FLINK-687 into multiple parts. This makes it far easier to review. Concerning the description of FLINK-2106, you haven't integrated the outer sort merge join into the optimizer and the API, yet. I guess this will happen as a next step. Maybe you can update the description of FLINK-2106 accordingly.
Other than that, the PR looks good to me :-)";code_debt
Same with these other methods. Should these be primitives?;code_debt
`TwoInputStreamOperator` and `OneInputStreamOperator` are internal classes. We should not mention them here but use a generic `operator with one input` term.;code_debt
That should do it - the batching option has been removed from MapState, in favor of parallel processing with existing opaque and transactional logic to handle consistency. Cassandra batch statements are more trouble than they're worth.;design_debt
Would this change be needed if we touch the package name in T2 source to include apache ? Should we change the package name (org.apache.trafodion...) in the source and the file layout  to make it consistent ?;code_debt
It might be easier to read if we use multiple lines:;code_debt
let's put `10` as a parameter, to make this method a bit more general.;code_debt
`seed` is not necessary since we use `approxQuantile`. It was added after Spark 1.6, so removing it will not involve breaking change.;code_debt
"I think so. In some cases, unnecessary executor-side reduce might invoke an additional map task although it just returns the single element. So this is just a minor concern for me.
The other thing is, `takeOrdered` is introduced into RDD API earlier than `treeReduce`. So I guess we may not consider to use `treeReduce` in this place before. For the nature of `takeOrdered` that sequences of elements are collected to the driver and reduced. It sounds a good fit for `treeReduce` as we can partially reduce before collecting to the driver. There is an overhead but sounds like a trade-off. Driver side usually a bottleneck for such case. The driver-side reduce is not parallel and also bound to local memory for all data.
I'm totally okay to close this if it doesn't sound good direction to go.";design_debt
super nit: order. swap with import above.;code_debt
This is reasonable, but it doesn't handle `Error`. I don't think you need a new 'inner' method? it also duplicates the job cleanup code.;code_debt
[Relay][Test] remove redundant test cases in test_op_level4.py;code_debt
"This is ""bad"", according to the tidyverse style guide, which I believed we were trying to follow: https://style.tidyverse.org/functions.html#long-lines-1
I can get used to whatever style conventions we decide, just want to make sure we're in agreement.";code_debt
 Also, there's no need for the String.format(), preconditions will do %s interpolation for you (I realize the String.format()was there before, but let's take this chance to get rid of it).;code_debt
in fact, the table qualified name may be long. in addition, we may need to add catalog name and database name to distinguish between tables with the same name.;code_debt
"hmm, not sure. Maybe this happens because the JVM writes that to the STDERR. That's annoying :/
This is probably fine then I was initially worried we might get so long filenames that you wouldn't be able to extract the logs on Windows.";code_debt
AFAIK we don't have a style guide for Java code, but I think we should put spaces after the casts.;code_debt
Feels like this collection is redundant. You can get the name from `InternalTopicConfig` perhaps?;code_debt
"~nit. If possible, can we have additional test statements for `minute` and `second` together below line 743 to make it sure?~
Never mind. I missed the other PR which landed already on `branch-2.4`.";code_debt
Rather than using `!muted` I think it would be much clearer to alias this to a boolean that is named: `guaranteeExpirationOrder` similar to the `guaranteeMessageOrder` boolean in `Sender`;code_debt
"I have the same concern as @dbtsai in his comment. Most consumers of this API will already be caching their dataset before the learning phase. Without user care, this will introduce effectively double caching (in terms of data size of cached RDDs) and will cause many jobs to fail after upgrading by exceeding available heap for RDD cache. Furthermore, we are making assumptions about how to cache -- in-memory only in this case. Should we parameterise this? Perhaps that will help send the message in the API that there is caching also done before learning. (FWIW, in-memory is definitely the right default choice here.)
See email thread on dev for my specific encountering of this bug: 
http://mail-archives.apache.org/mod_mbox/spark-dev/201502.mbox/%3CCAH5MZvMBjqOST-9Nr9k1z1rUODfSiczr_fV9kwqDFqAMNLC2Zw%40mail.gmail.com%3E";design_debt
We need more thinking about reuse the UTF8String object, it's not a trivial decision, so I'd like to leave this out of this PR.;design_debt
[FLINK-21609][tests] Remove usage of LocalCollectionOutpuFormat from SimpleRecoveryITCaseBase;code_debt
I still think we need a better name for `pendingInSyncReplicaIds` since it is misleading in this case. Maybe we could call it `overrideInSyncReplicaIds` or something like that?;code_debt
Remove this sentence. It is not needed anymore.;code_debt
Actually no. I think it's safe to have it fixed. We only need to adjust the values for tests. What is the easiest way to only allow internal configuration? The problem is that for integration tests it's hard to set configuration values for runtime components otherwise.;design_debt
"There is a `MIN()` macro you can use for this.
Also, I would use `sizeof(buffer)` instead of magical numbers.";code_debt
[SPARK-16008][ML] Remove unnecessary serialization in logistic regression;code_debt
nit: we can use multiline string, e.g.;code_debt
"looks good. I still have another thought(no binding for the following, you can ignore it):
- Since the root cause is a narrow windows between `queuePacket` and `cleanup`, so synchronized `objectLock` is also an alternative way? which one is better? Since `outgoingQueue` is a critical Queue for client to talk with server, synchronized `outgoingQueue` will have performance and future program extensibility issue?
Haha, I also test for the `global` and `inner` wording by the following way:
- new two different zookeeper clients, create some znodes, printing the `hashcode` of `outgoingQueue` and `state`. They really hold different `outgoingQueue`, but the same hashcode of `state`. it really confuses me.
- I believe different clients will have different `state` instance, otherwise when one client calls `close()`(set `state` to `CLOSE`), it will affect another client. However, using following ways cannot reason about it.
`javap` to see the bytecode, the value set to `state` is `public static final`, so it's really global-shared by multi-clients.
- synchronized `Enum` is also not thread-safe. Look at my demo attached in JIRA.
- In a word, `Enum` is a heresy:)";design_debt
You can simplify all this by doing:;code_debt
I've just renamed this as it made me confused - I imagined DB as LevelDB but there's separate suite for LevelDB. KVStore sounds better to me.;code_debt
This message looks a little bit misleading, or am I mistaken?;code_debt
"Hi @sirpkt ,
+1
The patch looks good to me. I have one suggestion. Each test method name should have the prefix 'test'. For example, `lastValue1` should be `testLastValue1`. It's trivial, so you can immediately commit the patch after fixing them.";code_debt
Remove useless shortopt;code_debt
Quick question: can we avoid catching `NullPointerException`? It's a bit odd that we catch `NullPointerException`. We could just switch to if-else I guess.;code_debt
"# What is the purpose of the change
Currently the error thrown from `runAsync()` method will be swallowed because Flink didn't handle all throwables with `AkkaRpcActor`. Here is a temporary fix for such cases in `YarnResourceManager`.
* Use try-catch to wrap the runnable that was invoked in `runAsync()` method, and reuse the `FatalErrorHandler` to handle the error.
* Add a new unit test";design_debt
Unnecessary whitespace;code_debt
"@sborya I agree it is pretty convoluted how we are doing this. However, @prateekm and I have discussed this option extensively in the past when we are working on [SAMZA-1212](https://issues.apache.org/jira/browse/SAMZA-1212). 
As I explained before , it is not fully clear how to wire-in the source of the stop and cause of the stop of a streamprocessor across the components. I don't believe it is as straightforward as passing in a `Throwable`. It could work. It becomes particularly tricky for bounded jobs or jobs that decide to stop themselves using TaskCoordinator. The approach you are suggesting is equivalent to threading a needle , where we will end up passing around the state/source of the `stop` across the components' api and callbacks. 
The alternative and more straightforward solution, imo, is to clearly define a state model for each components and persist/manage the status in the streamprocessor. That way, it will make it easier for streamprocessor to take remediation steps.
We can discuss more on this. However, this PR is simply a bug-fix. We should scrutinize the refactoring/behavior of the APIs in a separate thread/JIRA. What do you think?";design_debt
"Could you add a check to verify that the returned iterator is empty. Something along the lines of `assertThat(iterator.hasNext(), is(false))`?
Could you also add a test for a range query where the start key is equal to the end key? Such a unit test ensures correct behaviour for this special case.    
nit: I would rename the test to `shouldReturnEmptyIteratorForRangeQueryWithInvalidKeyRange`. Correct me, if I am wrong, but I think the empty iterator and the invalid key range are the points here, not the negative starting key. I would even change the range from (-1, 1) to (5, 3). It took me a bit to understand why (-1, 1) is an invalid range. 
These comments apply also to the unit tests below.";code_debt
hard code is not a good way;code_debt
[USERGRID-415] Pushing changes to make duplicate properties more readabl...;code_debt
Minor: unnecessary space after `put(`;code_debt
I think it makes sense to reuse the definition.;code_debt
Could you please factor out this logic in a separate method?;code_debt
"I added a `null` map testcase and remove redundant implementation.
For the removal from `sql/functions.scala`, there is no problem to remove that. But, could you check that again?";code_debt
The code seems fine, however, it seems like it would be good to at least log the transid with the exceptions so we can better track down/correlate issues after the fact.;code_debt
Am I missing something or you're using raw types in several places now? Raw types only exist for migration compatibility purposes and should never be used in new code IMO.;code_debt
Ah. The hive-hook shouldn't be there. I think I must have included it from someone elses commit when doing a rebase. Will tidy it up and do a force push later today.;code_debt
"Well, I'd say it differently. A Python person may not know what a JVM stack trace means. Taking it away doesn't itself do much except shorten a big dump of output, which doesn't really simplify much. Taking away important information that perhaps someone _else_ can make sense of isn't making usage (debugging) harder. if this is only removing ""unhelpful"" stack traces from the console, I can see it.";code_debt
Might be able to create a utility method in the abstract class like `byte[] getRow(String row, String encoding)` since it looks PutHBaseCell and PutHBaseJson both need the same logic;code_debt
CSourceModule with an empty string looks to me as well. @kumasento could you do that instead of creating a dummy llvm module? Thanks.;code_debt
"Jira : https://issues.apache.org/jira/browse/SPARK-17698
`ExtractEquiJoinKeys` is incorrectly using filter predicates as the join condition for joins. `canEvaluate` [0] tries to see if the an `Expression` can be evaluated using output of a given `Plan`. In case of filter predicates (eg. `a.id='1'`), the `Expression` passed for the right hand side (ie. '1' ) is a `Literal` which does not have any attribute references. Thus `expr.references` is an empty set which theoretically is a subset of any set. This leads to `canEvaluate` returning `true` and `a.id='1'` is treated as a join predicate. While this does not lead to incorrect results but in case of bucketed + sorted tables, we might miss out on avoiding un-necessary shuffle + sort. See example below:
[0] : https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/predicates.scala#L91
eg.
BEFORE: This is doing shuffle + sort over table scan outputs which is not needed as both tables are bucketed and sorted on the same columns and have same number of buckets. This should be a single stage job.
AFTER :
- Added a new test case for this scenario : `SPARK-17698 Join predicates should not contain filter clauses`
- Ran all the tests in `BucketedReadSuite`";code_debt
Fair. Added that. Still not perfect, but at least it will be perfect in pipelines without bundle failures, which is, hopefully, most pipelines.;design_debt
"The current `KafkaRecordDesrializer` has the following problems:
- Missing an `open()` method with context for serialization and deserialization.
The purpose of the change is to fix the above issues. 
- Renamed `KafkaRecordDeserializer` to `KafkaRecordDeserializationSchema` to follow the naming convention.
- Added the method `getUserCodeClassLoader()` to `SourceReaderContext` so the `SourceReader` implementation can construct the `SerializationDeserializationContext`.
- Added methods `valueOnly(...)` and `open(..)` in the `KafkaRecordDeserializationSchema` interface to enable the reuse of the `DeserializationSchema` and `KafkaDeserializationSchema`.
- Added the method `setValueOnlyDeserializer(...)` in `KafkaSourceBuilder` class to make it easy to set value-only deserializer.
- Added unit tests in `TestingDeserializationContext` and `KafkaRecordDeserializationSchemaTest`.
- Added tests in `KafkaRecordDeserializationSchemaTest` to verify the changes made in KafkaRecordDeserializationSchema";code_debt
formatting, new lines would be easier to read;code_debt
Can we introduce a `TableSinkFactoryContextImpl` class to reduce so many anonymous classes?;code_debt
[Issue 9725][Transaction] - Fix deleteTransactionMarker memory leak;design_debt
indent.;code_debt
"There are a couple of CDN-in-a-Box services that take much longer to build than they ought to because of how long it takes to send the build context to the daemon - but don't actually need a context of that size. This reduces the services' contexts to only what is necessary for them to build.
- CDN in a Box
Build CDN-in-a-Box";code_debt
nit: move to previous line;code_debt
missing space **...setup (data ...**;code_debt
@objmagic - Thanks for reviewing. I'm agree with you. At first, I think putting `ByteAmountUnit` inside of `ByteAmount` is a better approach and using enum to instead static `MB` and `GB` inside of `ByteAmount` seems nice too. But It might trigger a large refactor in `ByteAmount`. So, I decided to put `ByteAmountUnit` outside of `ByteAmount` temporarily.;design_debt
The configuration options are not symmetric here, which can lead to confusion and does not allow all override possibilities. The code is ignoring the config store whitelist tag when a job-level whitelist is specified, but it is adding the job-level blacklist to the tag-based blacklist.;design_debt
"We assume that, for a given version, the notice and license files will not change, which is a safe assumption to make.
I'm dubious as to whether it will be possible to pull in NOTICE files automatically. We would have to pull in every NOTICE file, which isn't really necessary. And then someone would have to check the contents of the pulled in NOTICE file to ensure everything is ok.
If we make the pulling of licenses automatic, then they will only ever be checked at release time. At release time, all dependencies need to be checked, and when there's so much to check, people are likely to just give it a quick glance, and +1 it, without actually checking each dependency.
I would prefer that the work in manually checking dependencies occurs as part of the development process, each time we update a dependency. At this time, there will be a smaller subset of the dependencies changing, so it can be reviewed more carefully. The submitter will be able to take their time with it, and the reviewer will be able to give each dependency their full attention. Once a license/notice has been updated for a version of the dependency, it shouldn't need to be looked at again (as licenses/notices don't change within a single version).";design_debt
Fix indentations if you want to remove try-catch, also add a test?;code_debt
Using `Logger` would be more standard. See [Logging in NetBeans](http://bits.netbeans.org/dev/javadoc/org-openide-util/org/openide/util/doc-files/logging.html) document.;code_debt
Bad formatting.;code_debt
"Remove unnecessary include-elasticsearch5 profile since its activation condition (usage of jdk8) is always true.
Run any mvn command in `flink` or `flink-connectors` and check that the reactor includes ES5.";code_debt
"TLDR: I agree this is fine. Below are some notes from digging around in the synchronization code.
Looking at the other synchronization blocks, `emitInternal` will synchronize on `EntityManagementSupport` instance (when it calls `getSubscriptionContext()`. It will then call `publish` which will synchronize on `LocalSubscriptionManager` instance (in `getSubscriptionsForEntitySensor`).
However, I think we can trust both the `EntityManagementSupport` and the `LocalSubscriptionManager` to not call out to alien code while holding a lock on itself. Therefore we should be safe in that respect.
The code in `AbstractEntity` and `AbstractGroupImpl` looks a bit scary, where it first gets the lock on this `values` object and then on either `abstractGroup.members` or `abstractEntity.children` (but it's kind-of understandable why it does that and how that pattern avoids it it would just be nicer if instead we didn't call out to alient code while holding the lock on `values` - but that's a bigger discussion than for this PR):";code_debt
Yeah, you are going to end up getting the same thing.  I'd say we drop this one and leave the other.  Right now it probably doesn't matter, but the other one is lazy and gives the optimizer a chance to possibly improve things before actually executing.;code_debt
maybe more descriptive name than `get`.;code_debt
This function seems not used?;code_debt
"ad 1. - that's why I have used `StateAssignmentOperation.operatorSubtaskStateFrom`, to share this logic. I didn't want to use all of the `StateAssignmentOperation`, because it's constructor is really annoying to fulfil.
ad 2. - that's a valid concern, however writing ITCases for this might be also an overkill. And wouldn't it be to necessary to test it against every state backend, to make sure that there are no introduced quirks during (de)serialisation?";code_debt
"nit: remove ""of bytes"".";code_debt
@ANSHUMAN87 Hi, thanks for trying this! Could you be more specific about your setting? The adaptive evaluator works in the occasion when the repeat/number of the measure_option is a big number (like 500), and according to our experiment results in the paper, the search efficiency outperforms than the base AutoTVM in the Transformer encoder tuning case.;code_debt
"This overall feels like it makes the code cleaner, but I do wonder if it's useful to keep this config setting (so that people can tweak the behaviour of the Airflow without _having_ to go in to the UI and change behaviours.
If we do decide to delete these two config values we should add a note to UPDATING.md about it.";code_debt
use `new MatrixBlock(((ScalarObject) newOutput).getDoubleValue())` instead.;code_debt
[CARBONDATA-3877] Reduce read tablestatus overhead during inserting into partition table;code_debt
Get rid of dead code or un-comment to add coverage.;code_debt
No point in duplicating IfFunctionBuilderApplying's code just for one differing line. Better instantiate IfFunctionBuilderApplying with Null object for the input argument (i.e. empty IfFunction).;code_debt
"Not part of your changes but... variable `devIds` should have type `List<Long>` instead of `List<String>`.
All that conversion from `int` to `String` and then converting from `String` to `long` seems unnecessary.
Should simply be able to do:
Note: my code above includes fixes to two other comments I made further down in the code.";code_debt
"This PR aims to clean up package name mismatches.
Pass the Jenkins.";code_debt
"Note that this change is not Python 3 compatible since `long` was removed in Python 3.  Can you add something similar to the following?
https://github.com/apache/beam/blob/8d854d4ce0365a8e201b388618d7732f000c65b9/sdks/python/apache_beam/transforms/combiners.py#L39";code_debt
"The issue I had with RAT is it wouldn't deal with py and js/jsx files in a way that our linters liked, and there was lots of files to touchup... rodent is a very small app (haven't pushed it to pypi yet), but could become an alternative to RAT.
Also it wasn't aware of my .gitignore, but I see you took care of this here with a rat-ignore...";code_debt
@dbtsai I did batching for artificial neural networks and the performance improved ~5x https://github.com/apache/spark/pull/1290#issuecomment-70313952;code_debt
Origin code ignore the `newName`. Is this intended?;code_debt
Sounds correct. The subsequent tries do try in parallel. So, I suppose that's pretty good evidence it's parallelized. Unless anyone else speaks up I think this sentence can be removed.;code_debt
"@michaelandrepearce this is just code cleanup. Nothing that would bring any value to a release notes...
The commit itself would be enough record of the change here.
If someone is creating a JIRA, it would be a task, as the is not an improvement, not a feature, not a bug fx.. no value on the release notes.";code_debt
Can we name this `transactionGenerator` for simplicity?;code_debt
"Is this interface really necessary? Especially with `@PublicEvolving` annotation? How are users supposed to use it? If I understand it correctly you need it for internal operations. Moreover you need it because the `WrapperTypeInfo` is in `blink-runtime`, right?
Can't we move the `WrapperTypeInfo` to the `table-common` instead?  The class itself has no runtime dependencies. Only the factory methods need some runtime classes.";code_debt
"Adressed issues in comments and added 3 more commits. They should be squashed to their respective ""parent"" commit.
1. ""Adding withClue statements to action tests, dropping unneeded tests"" -> ""Rewrite CLIActionTests in Scala, refactor withActivation helper""
2. ""Formatting nit in TestUtils"" -> ""Refactor runCmd, remove obsolete helperclasses""
3. ""Renaming some ruletests, adding comments"" -> ""Rewriting CLIRuleTests in Scala, adding a new testhelper""";code_debt
how is this less code? It is just a bad idea to create unnecessary state. Just move both tables into two fields in Hex object.;code_debt
"@apovzner Personally I think the timestamp should be accurate. Modifying the timestamp sounds very hacky and creates extra complexity. Please also notice that the timestamp index built by the followers will be purely depending on the timestamp in outer message of compressed messages. The followers will not even decompress the messages. If we play the trick here, the time index on follower will also be affected. 
If we want to make things right, then producer should be able to get the necessary topic configuration info from broker, either from TopicMetadataRequest or some other requests. So the producer can set the timestamp correctly to avoid server side recompression. But like you said this is a bigger change and it is unnecessary to block on that change.
I think the current solution is reasonably clean as of the moment.
Once the producer is able to get the topic configuration from broker, we can simply migrate to use that. Since everything is purely internal, the migration is very simple and transparent to users.";design_debt
I actually don't need the .valueOf(). Removing it.;code_debt
"nit: i think line 68-70 can be removed (to avoid confusion), looks like abandoned legacy code:
thanks for fixing this!";code_debt
letlist is everywhere. can you put this in some common file?;code_debt
"Ok - if the behavior we get from `dapply(repartition(df, cols))` is the same as `groupByKey().flatMap` then I'm fine with going with the simpler implementation. 
But I think we should have a high level `gapply(df, cols, function(group))` API in SparkR that is clearly specified. The internal implementation we should go with whatever is simpler.";code_debt
This is a single class name, variable name should reflect that.;code_debt
nit: one step further would be to store `CheckpointBrief` in `PendingCheckpoint` instead of collections. But that's probably out of scope of this PR;code_debt
As per our offline discussion, let's just change this to `unused/class/path` or something.;code_debt
yeah...this seems to be one place where this tablename git confusing ... :+1:;code_debt
I prefer addressing non SEP-1 related suggestions in a separate PR. This is an awfully large change and should not be bloated with other important changes.;design_debt
"nit: better to do `node1.nodetoolResult(""disableautocompaction"", ""netstats_test"").asserts().success()` rather than exec into the instance.";code_debt
Recommend inline these 2 methods for readability (as the existing code). Current way is not as readable;code_debt
"Most of the executors run local task jobs by running `airflow tasks run ...`. This is achieved by passing  `['airflow', 'tasks', 'run', ...]` object to subprocess.check_call. 
This is very limiting when creating new executors that do not necessarily want to start a new process when starting a local task job, e.g. fork a process instead of create.
We could achieve a similar effect if we process back the argument list, but this is an ugly and hack solution, so I did refactor the code and now the executor passes the LocalTaskJobDeferredRun object that contains all the detailed information. A particular executor could create a command if it needs it.
This will facilitate the development of other executors:
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-29%3A+AWS+Fargate+Executor (@aelzeiny)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-28%3A+Add+AsyncExecutor+option (@dazza-codes)
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-25+The+Knative+Executor (@dimberman 
https://github.com/apache/airflow/pull/6750 (@nuclearpinguin )
This also made the DebugExecutor code simpler. (@nuclearpinguin )
---
Issue link: [AIRFLOW-6334](https://issues.apache.org/jira/browse/AIRFLOW-6334)
---
Read the [Pull Request Guidelines](https://github.com/apache/airflow/blob/master/CONTRIBUTING.rst#pull-request-guidelines) for more information.";design_debt
Nit: it would be nice to keep the expression on one line, wrapping after `checkArgument(`;code_debt
but why did you add the space between `undefined` and `)`;code_debt
"nit: `val tmpIsNull = ctx.freshName(""coalesceTmpIsNull"")`, to be consistent with https://github.com/apache/spark/pull/19901/files#diff-a966ee88604a834221e82916ec051d7dR190";code_debt
"@abhishekagarwal87 @knusbaum 
Not at all. It shouldn't hurt much so let's apply this as it is. We can address broader considerations from another issues.
One thing I'd like to see is the status of backpressure for each queue. We can show the percentage, or just show whether this queue meets condition to trigger backpressure or not.";design_debt
Regarding the operator as a whole and as discussed in the past, constraints are mostly inherited and would need to be addressed by refactoring the base. I'm not sure about the timing for this though, if it should wait until legacy runner goes away (there is still significant work for portable runner to become adequate replacement for Java pipelines).;design_debt
"The suggested change is only for making this test suite cleaner, right? In that case I'd +1 with the suggestion of being able to clearly check we're catching the exception we know we're throwing.
Would you like to submit a PR for it?
The intent is never to actually reach the null-return, but always cause an exception to be thrown at `CodeGenerator.compile()` and abruptly return to the caller with the exception. To make the compiler happy you'll have to have some definite-returning statement to end the function, so a useless null-return would probably have to be there anyway (since the compiler can't tell you'll always be throwing an exception unless you do a throw inline)";code_debt
"Yes, here:
It would be nice if we could get rid of this case somehow, because by making this optional we have to deal with the possibility of `pvalue.pipeline` being `None` throughout the code base.  I went back and forth on whether to make the arg optional or simply ignore the error in the method above, but I think I decided that the method above was a common case and thus we needed the protection against None-values throughout the code.";code_debt
That's great point. At the moment, I think that we are not consistent about this. Some are package private and some are not. The advantage of keeping it public is that it allows to use the class in unit tests which resides in other packages.;code_debt
This sync is not necessary.;code_debt
keep old-style maybe better;code_debt
I think it's good to take opportunities like this to do more general code cleanup. I'm on vacation this week, I will look at splitting it in to two commits when I get  back.;code_debt
"Even if we wrap column names in backticks like ``a#$b.c``,  we still handle the ""."" inside column name specially. I think it's fragile to use a special char to split name parts, why not put name parts in `UnresolvedAttribute` directly?";code_debt
I see, can that thread be a daemon? If System.exit is viable (i.e. immediately stopping daemon threads) then it should be. But if not, then yeah such a thread needs to be shut down cleanly somehow during the shutdown process. This could be a shutdown hook.;design_debt
"nit: ""an none"" seems ungrammatical";code_debt
"try to make comparisons more specific. Since you know there are only 2 parts and parts cannot be negative, avoid the ""less than"" comparison and instead use `parts.length != 2`";code_debt
Can we reuse an existed method OzoneFSUtils#addTrailingSlashIfNeeded(keyName) instead of?;code_debt
Yes. Talking with @dbtsai he wanted to add a lock on the blocks inside of `doCleanupShuffle`, but given that the only price is duplicated messages to the executors I'm not sure its worth the overhead of keeping track of that many locks.;code_debt
"Ideally you should get the token by calling a python method directly -- by calling this endpoint you are ""retesting"" the login endpoint, which it would be better to avoid.";code_debt
TS-4038: Redundant `isdigit(b)` in `LogFormat::parse_escape_string;code_debt
it is a bad idea to have two *isSetter* methods, pls. consider to combine both into one single isSetter method;code_debt
"It is not correct javadoc ""list"", it should be written in HTML style like
So, it is a test though... who cares.";code_debt
 I thought having all metadata constants in one place would make it simpler. This is used in reading archived commit. I can move the constant to ArchivedTimeline if you think thats a better place.;code_debt
"This PR is ready to be merged, approved by @kaisun2000 
There is connection leakage in CustomRestClientImpl and causes timeout waiting for connection.
Fix this issue by consuming entity and releasing the content stream and connection.";design_debt
This method is only used in the test currently. Please add it to `shutdown()` methods of `ZooKeeperServer` and `QuorumPeer` classes.;code_debt
"Can you use something like this instead?
Otherwise it throws an uncaught exception on CPU only machines.";code_debt
"New commands:
  heron examples list
  heron examples run <cluster> <example-id>
This patch is to provide easier commands to get the heron examples up and running.";design_debt
it's for correctness.  to allow users to manipulate the ExecutorResourceRequests multiple times.  The original intent was all of these classes would be immutable, but that made things less user friendly so in the original PR this class got created and users are allowed to modify multiple times and they could do it from multiple threads.  It's something I missed in the original pr rework.;code_debt
Thanks for the pull request. We can't just change a private to public like this, because it can make the API more difficult to maintain in the long run. Can you justify more why this is needed, and why you can't work around it from the user side?;design_debt
"@dongjoon-hyun to be clear, I think there are 2 problems:
1. The PostgresDialect indicates that `CASCADE` is enabled by default for Postgres. This isn't the case as the Postgres docs show. 
2. As you correctly mention (this is what in my previous comment), Spark doesn't use `CASCADE` at all, which, especially considering the method this PR edits, is a bit odd I think. I plan to open a different JIRA ticket for this, and add it. This will be more work, and is outside the scope of the current JIRA.";code_debt
"I did -- so setting SkipMessage=null has the same effect as the SkipTestException since our version of XUnit does not support it. But I think the comment of ""// Use null to run tests"" could be a bit more explicit. This is more what I was thinking:";code_debt
"LGTM,
thanks for writing these benchmarks. 
I think moving forward, I agree that ColumnVector is a natural data structure to decode into, but we should probably not add this logic directly into those classes just from a code maintenance point of view. I think exploring the parquet encodings makes sense but let's start by benchmarking those and see if they have the right performance characteristics.";design_debt
"Travis-CI:
 - remove all environment variables from the build matrix, to use the same cache for all jobs
 - activate the mysql service only when this service is needed
 - activate the xvfb service, when is necessary and possible
 - removed Bower Caching to remove too many complicated lines in Travis-ci
 - Giving the test names
 - Installing R only once with conda, previously it was installed twice with 'testing/install_R.sh' and 'testing/install_external_dependencies.sh
 - remove the R-Cache, because the installation with conda is quite fast
 - Delete 'test/install_R.sh' because it is no longer used
Other:
 - Ignore the tests in 'HeliumApplicationFactoryTest.java' to get the JUnit tests running in the IDE + remove exclude in 'travis.yml
 - Remove deprecation warning in maven-surefire-plugin
 - Helium works better with an absolute path, because a relative path in PATH, is not a good idea for local testing
 - Remove JVM language dependent asserts
Improvement
* https://issues.apache.org/jira/browse/ZEPPELIN-5024
* Travis-CI: https://travis-ci.org/github/Reamer/zeppelin/builds/723116251";code_debt
LGTM - thanks for the cleanup @Shafaq-Siddiqi. The scenario with 10 components was still failing, but after some debugging it turned out this was due to Kmeans not converging. During the merge I fixed the hard-coded maximum iterations for Kmeans, some formatting issues, and vectorized part of the cholesky computation. With those changes it ran fine.;code_debt
"This sounds a little risky to me, `A process is invoked even for empty partitions`.
This may cause a hang situation if the command is expecting input.
For example, this PR's test case is using `cat`. And, `cat | wc -l` hangs.
If we are okay, could you add a test case of empty partition to make it sure that we handle those cases?";code_debt
There's some other cruft here that ought to be removed. I'll make some additional changes and then merge this;code_debt
TreeMap was used in the original GroupByEngine code being moved, my goal with this PR was to create the interface and move type-specific code but not change functionality generally/target performance optimizations;code_debt
"Instead of a ConcurrentHashMap, we should actually move it to a disk backed Map - the cleanup of this datastructure is painful - which it can become extremely large particularly for iterative algo's.
Fortunately, most cases, we just need the last few entries - and so LRU scheme by most disk backed map's work beautifully.
We have been using mapdb for this in MapOutputTrackerWorker  - and it has worked beautifully.
@rxin might be particularly interested since he is looking into reduce memory footprint of spark
CC @mateiz - this is what I had mentioned about earlier.";design_debt
"There was an intentional optimization to reduce one  `System.nanoTime()` call per tuple execution. In the new commit this optimization is removed to simplify the reasoning. 
Now it is much easier to understand.";code_debt
"Out of curiosity: why was it failing sometimes on Travis and not locally? And how did you discover this? From the program level logs?
Another thing that came to my mind: in the long run, do we need a more complex way of configuring the retry policy? In my understanding, the number of retries is fixed. I can see an issue for very long run programs, which fail once in a while, but operate normally most of the time -- then at some point they will fail because of the fixed number of retries.";design_debt
"I think it would be more clear to just do string matching. E.g.
The regex just adds extra complexity. Same with setProperty.";code_debt
"BTW, this code `this.getRoot().getCluster().getMetadataQuerySupplier().get()` confused me a lot, can you please explain why we need a fresh new `RelMetadataQuery` instance here ? Couldn't we use the instance already existing there with `RelOptCluster#getMetadataQuery` ? (I have checked that almost each `isValid` case is in the `RelOptRuleCall` circle).
Even we have to got a fresh new instance here, why we just add a new interface `RelOptCluster#getMetadataQuerySupplier` just for debugging ? Can you refactor that out ?";code_debt
Instead of doing this, just prevent users from creating a StorageLevel with offHeap = true and replication = 1. Add a check in the StorageLevel constructor and throw an exception if they make one. Otherwise nobody will understand why this code was added here.;code_debt
Since we don't need the old index, shall we remove the obsolete indexes?;code_debt
"i agree no matter gunicorn/nginx support very long header or not, carry 8k referrer request header is too much. I think add an upper limit is necessary.
About losing query state, @mistercrunch do you think use localStorage to save query state is a good idea? we can use js (already existed) generate an `impression` key, unique for every page load. With same `impression` key, we store query state, so we can support redo and undo like dashboard edit.";code_debt
"sorry for the late reply. @sijie 
I just notice this change and am wondering the necessity of exposing the whole admin client from functions to users. Maybe we can have some discussion when you are available";code_debt
"We are no longer referring to field names here. Honestly I am not sure if the point makes sense with the `KeySelector` only. The way I read this point is tells you can use field names, which we discourage nowadays.
I'd rather remove the point whatsoever.";code_debt
@rHermes yes general good practice is to separate PRs that deal with different subjects. But for cleaning PRs, it is the same subject. There a lot of leftovers in nexmark what I would like to avoid is tens of PRs that remove only a couple of fields.;code_debt
`en` is unused.;code_debt
"So, this feels a little racy, in that this thread might miss things added by the log checking thread. I'd suggest the following:
- Create a single-threaded executor for running the replay tasks
- Create a list of app infos to parse in the log checking thread, break it down into batches.
- Submit each batch to the executor
Basically, instead of having `logLazyReplay`, you'd have something like `replay(apps: Seq[LazyAppInfo])`. You don't need `lazyApplications` because that becomes part of the task being submitted to the executor, so you solve another source of contention in the code. And since it's a single-threaded executor, you know there's only a single thread touching `apps`, so it should all be thread-safe.
For testing, you can use Guava's `sameThreadExecutor()` as I mentioned, instead of the single-threaded executor.";code_debt
"this pull request is based on https://github.com/apache/incubator-mxnet/pull/10804
with the following further changes:
1. reduce ident changes
2. prefer cudnn depthwise convolution over mxnet implementation
still use the explicit #if #else #endif statement over
the new variable effective_num_group solution for backward code path compability
because the new variable effective_num_group may confuse readers with standard group convolution";design_debt
The implementation is different from the description.  It would be easier to call it docker instead of docker-build to make the command less characters to type.;design_debt
It is a `Singleton`. Should be `final` instead.;code_debt
Also, is the only purpose of `sqlListener` to prevent multiple listeners from being created at the same time? If so, I think it would be better to use an AtomicBoolean so that we don't create another strong reference to a SQLListener, which might have a lot of internal state that could lead to memory leaks.;design_debt
useless import?;code_debt
we can reuse some code in this file by pulling the common structure into a helper function ?;code_debt
If this is an upper limit, I would suggest something like `radius-max-retries` to make this more clear.;code_debt
[SPARK-11719] [ML] Remove duplicate DecisionTreeExample under examples/ml;code_debt
also useless;code_debt
When are we removing the entry upon task closure? If it never cleans up we could potentially have an ever-growing map.;design_debt
Nit: the second brace and its match are redundant;code_debt
nit: 2 space indentation;code_debt
+1 LGTM , @cyrus-jackson  please to take care of the checkstyle too;code_debt
"I'm sorta ambivalent on the package name -- I looked at crates.io and there are some other ASF projects with packages that just use the Foo in Apache Foo. If ""arrow"" is shorter and sweeter, that's no problem";code_debt
"What changes were proposed in this pull request?
Some of the columns of JDBC/ODBC tab  Session info in Web UI are hard to understand.
Add tool tip for Start time, finish time , Duration and Total Execution
Why are the changes needed?
To improve the understanding of the WebUI
Does this PR introduce any user-facing change?
No
How was this patch tested?
manual test";design_debt
I think it should be computed on higher level, producing two different classes. The lambda below is capturing, i. e. it's an allocation on each iteration;code_debt
nit: indent;code_debt
Nit: should we include bytes like we did `ms` for the other case?;code_debt
"this is `Class.toString` which is harder to read for arrays, so did this so I could read the output without googling =D.
primitives and objects will normally have valid string names, but arrays will be like `J[` which require knowing what that means or googling it.";code_debt
"One ""short term workaround""â„¢ for getting the `pex_pytest` to work was to change it so the pex binaries+tests defaulted to be non-zip safe so they are extracted. There were still failures later, which I think is from pex_library consumption. I'll see if I can patch that too, then hopefully come up with a neater solution for all of the issues";design_debt
Maybe this at debug. Too noisy.;code_debt
I find the else to be more readable, but happy to drop it and the indent...;code_debt
"todo before release: 
Document this cp as what's it's doing is a little bit obscure";code_debt
"It's very rare for us to mark a class as final, especially a private one. Is there a reason for that?
It makes other things (like mocking the class in tests) more complicated, for example.";code_debt
There's https://github.com/apache/arrow/blob/master/cpp/cmake_modules/FindPythonLibsNew.cmake, is one or the other made redundant by the other?;code_debt
I would prefer doing this change differently. Maybe by allowing allocator to return an empty buffer even if closed. This is because it makes bugs/issues much easier to understand than getting an NPE.;code_debt
local compile and package speed is so slow,because  need to download so much dependency jar file from maven depository;code_debt
"@abhishekagarwal87: Ok, didn't spot that. Was only looking for ""guava"" relocations.
So, the dependency could be additionally included relocated into storm-redis. Or is it possible to use the already relocated package which is provided via storm-core? This would save some resources. I'm not very experienced with the `maven-shade-plugin` yet, unfortunately.
What is preferred?";design_debt
This looks 100% the same as in the local runtime. Can we reuse that code instead of duplicating it?;code_debt
"In the PR, I propose to optimise the `DateTimeUtils`.`rebaseJulianToGregorianMicros()` and `rebaseGregorianToJulianMicros()` functions, and make them faster by using pre-calculated rebasing tables. This approach allows to avoid expensive conversions via local timestamps. For example, the `America/Los_Angeles` time zone has just a few time points when difference between Proleptic Gregorian calendar and the hybrid calendar (Julian + Gregorian since 1582-10-15) is changed in the time interval 0001-01-01 .. 2100-01-01:
The difference in microseconds between Proleptic and hybrid calendars for any local timestamp in time intervals `[local timestamp(i), local timestamp(i+1))`, and for any microseconds in the time interval `[Gregorian micros(i), Gregorian micros(i+1))` is the same. In this way, we can rebase an input micros by following the steps:
1. Look at the table, and find the time interval where the micros falls to
2. Take the difference between 2 calendars for this time interval
3. Add the difference to the input micros. The result is rebased microseconds that has the same local timestamp representation.
Here are details of the implementation:
- Pre-calculated tables are stored to JSON files `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json` in the resource folder of `sql/catalyst`. The diffs and switch time points are stored as seconds, for example:
  The JSON files are generated by 2 tests in `RebaseDateTimeSuite` - `generate 'gregorian-julian-rebase-micros.json'` and `generate 'julian-gregorian-rebase-micros.json'`. Both tests are disabled by default. 
  The `switches` time points are ordered from old to recent timestamps. This condition is checked by the test `validate rebase records in JSON files` in `RebaseDateTimeSuite`. Also sizes of the `switches` and `diffs` arrays are the same (this is checked by the same test).
The hash maps store the switch time points and diffs in microseconds precision to avoid conversions from microseconds to seconds in the runtime.
- I moved the code related to days and microseconds rebasing to the separate object `RebaseDateTime` to do not pollute `DateTimeUtils`. Tests related to date-time rebasing are moved to `RebaseDateTimeSuite` for the same reason.
- I placed rebasing via local timestamp to separate methods that require zone id as the first parameter assuming that the caller has zone id already. This allows to void unnecessary retrieving the default time zone. The methods are marked as `private[sql]` because they are used in `RebaseDateTimeSuite` as reference implementation.
- Modified the `rebaseGregorianToJulianMicros()` and `rebaseJulianToGregorianMicros()` methods in `RebaseDateTime` to look up the rebase tables first of all. If hash maps don't contain rebasing info for the given time zone id, the methods falls back to the implementation via local timestamps. This allows to support time zones specified as zone offsets like '-08:00'.
To make timestamps rebasing faster:
- Saving timestamps to parquet files is ~ **x3.8 faster**
- Loading timestamps from parquet files is ~**x2.8 faster**.
- Loading timestamps by Vectorized reader ~**x4.6 faster**.
No
- Added the test `validate rebase records in JSON files` to `RebaseDateTimeSuite`. The test validates 2 json files from the resource folder - `gregorian-julian-rebase-micros.json` and `julian-gregorian-rebase-micros.json`, and it checks per each time zone records that
  - the number of switch points is equal to the number of diffs between calendars. If the numbers are different, this will violate the assumption made in `RebaseDateTime.rebaseMicros`.
  - swith points are ordered from old to recent timestamps. This pre-condition is required for linear search in the `rebaseMicros` function.
- Added the test `optimization of micros rebasing - Gregorian to Julian` to `RebaseDateTimeSuite` which iterates over timestamps from 0001-01-01 to 2100-01-01 with the steps 1 Â± 0.5 months, and checks that optimised function `RebaseDateTime`.`rebaseGregorianToJulianMicros()` returns the same result as non-optimised one. The check is performed for the UTC, PST, CET, Africa/Dakar, America/Los_Angeles, Antarctica/Vostok, Asia/Hong_Kong, Europe/Amsterdam time zones.
- Added the test `optimization of micros rebasing - Julian to Gregorian` to `RebaseDateTimeSuite` which does similar checks as the test above but for rebasing from the hybrid calendar (Julian + Gregorian) to Proleptic Gregorian calendar.
- Re-run `DateTimeRebaseBenchmark` at the America/Los_Angeles time zone (it is set explicitly in the PR #28127):";code_debt
"These ``_fileOS == null`` checks are kindof ugly. Thoughts on making this so this class just extends OutputStream instead of ByteArrayOutputStream, and then make it a wrapper for an output stream, which might change from ByteArrayOutputStream to FileOutputStream? I'm thinking something like:
Makes it so all the overrirde functions are basically just stream.whatever(), except for write which just calls the switch thing. Another benefit is once the switch happens, the old ByteArrayOutputStream can be garbage collected, whereas before it couldn't.";code_debt
"@bhaisaab I don't like maven but we are using it! cherry-picking is really not an argument and backporting is difficult for worse reasons then this one.
Using maven we better adhere to the conventions in the maven world as keeping our diversions from it correct will become increasingly difficult over time. I will meet you half way so we can abandon 4.5 first and continue to prove our fwd-merge schedule over several versions. We will face issues in this respect as well, btw, if at the time of 4.11 we will be fixing things in 4.6 )";design_debt
"Can you provide some test data, say before/after applying this patch, how many duplications are found respectively?
IMHO, we should make the API as concise as possible.";code_debt
This automatic fix isn't my favorite though I see that it is necessary to not have a braking change in the api for  ParquetDataset (with the filters argument). Perhaps though it would be better to throw an error here and have this fix in that specific case instead of allowing a wrong nesting level in all cases.;code_debt
nit: since it's just an accessor, maybe we could drop the parenthesis?;code_debt
.toString() is unnecessary;code_debt
"I think it's better to keep this internal, it's a tradeoff between 1.0 and 1.1, most of the users do need to touch this.
We could document it later if user really need it.";design_debt
"@nsuke, I switched it so we now consider a `long` like two `int`s (the high bits and low bits) and we combine their values like we combine `hashCodes` (just with a different multiplicative factor). For `double`s we get the `long` represented by their bytes and treat as before. Since we're changing the values, I also took the chance to pick arbitrary constants to combine with.
For what it's worth, though, the previous `hashCode` helper functions are pretty much what are described in Effective Java. Anyway, I'll rebase and squash after you have a look.
@jsirois That's a good idea. Unfortunately, I don't have the time at the moment to do that :(";design_debt
I get declaring a constant though it doesn't just pertain to byte arrays. I couldn't find a good place for it. I don't know if its reused so much that it needs this;code_debt
"Feels slightly odd to pass in the set of topics to load here, but I can't think of a good way to avoid it. Perhaps we could pass MetadataCache into LogManager and let startup call MetadataCache#getAllTopics? That might be more risky though since it changes the startup order in KafkaServer, maybe we can look into this as a follow-up.
Besides that, the name here seems strange. Maybe something like ""topicsToLoad""?";code_debt
"if using an Optional, this becomes:
Looks cleaner to me.";code_debt
oh, I just realized that when we reuse the Decimal value, we do not really need to use the returned value. But, we have another place that needs the returned value. Can we add a comment at here?;code_debt
Instead of `3.7` we should use `PYTHON_VERSION` variable;code_debt
`RDD[(Set[Double], Set[Double])]` may be hard for Java users. We can ask users to input `RDD[(Array[Double], Array[Double])]`, requiring that the labels are ordered. It is easier for Java users and faster to compute intersection and other set operations.;design_debt
@sxjscience Yeah this will cut out a lot of code in our framework that were just there to redundantly track tensor dims for the purposes of feeding them to Reshape layer. I'm very excited about this. A small increase in (optional) complexity in Reshape layer pays off with a large decrease in complexity in our framework code.;design_debt
using `unwrap` in libraries is generally a bad idea if it's possible to panic. please use `?` since you're already returning a `Result`;code_debt
"@emkornfield I created a new benchmark to evaluate the performance of consumer directly. The improvement is not significant:
after: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  77326.747 Â± 218.829  ns/op
before: JdbcAdapterBenchmarks.consumeBenchmark  avgt    5  79007.087 Â± 63.994  ns/op
I think there are two reasons for this:
1. in our benchmark, the jdbc implementation is based on h2, and for this library the wasNull method implementation was simple: 
    @Override
    public boolean wasNull() throws SQLException {
        try {
            debugCodeCall(""wasNull"")
            checkClosed()
            return wasNull
        } catch (Exception e) {
            throw logAndConvert(e)
        }
    }
It can be seen that the implementation is based on a simple flag, plus some simple checks. For other implementations with other RDBs, the implementation can be more heavy. For example, the following code gives the implementation of MySQL JDBC, which may involve megamorphic virtual calls:
  public boolean wasNull() throws SQLException {
    try {
      return this.thisRow.wasNull()
    } catch (CJException var2) {
      throw SQLExceptionsMapping.translateException(var2, this.getExceptionInterceptor())
    }
  }
2. The time for wasNull method was insignificant compared with the Arrow set methods.";code_debt
For posterity, I came across this discussion around the original hard-coding of the rpath: https://github.com/apache/arrow/pull/2489#discussion_r215664651.;code_debt
Is there any reason to `catch Throwable`, instead of specific exceptions?;code_debt
"couple of quick thoughts.
- we'll need to get all the version numbers aligned with whatever nifi version this would be committed into.  Currently that would be 1.3.0-SNAPSHOT.
- It would probably be a good idea to have the notion of 'nifi-leaderelection-api' which is not about zookeeper but rather just generic election/tracking of a leader for a given thing (a partition?) Then there would be zookeeper based implementations of those.  Processors then can leverage the api for their code but users can select whichever types of services exist (zookeeper being the obvious initial example).  The structure appears already in place for this other than the current naming and perhaps the API referencing zookeeper.  Thoughts?
- It would be good to have a processor which leverages this or some docs that go along with it to show suggested usage.";design_debt
Hmm... We will avoid boilerplate, but seems like it will make code more error prone. Developer can forget to override this method for trainer which potentially supports updating and get an error while trying to update this model in the future whereas keeping it abstract forces developer to think if this trainer supports update and insert `NotImplementedException` more cautiously.;design_debt
it will not be JITted it also should not not be too small, otherwise there will be many function calls.;design_debt
nit: Can we unify this with `createYarnResourceForResourceProfile` in YarnAllocator ?;code_debt
"Can we drop ""Adaptor"" from this class name?  When I first read this I was assuming it was a set of tests that pertained only to the HTTP1 Adaptor.
Also should this class be a subclass of ""object""?";code_debt
what is this? is it doing a union? For a union, it would be better to use BasicOperations.union on Automaton objects rather than mess around with regexes as strings;code_debt
"Here, to avoid creation of new ByteBuf we modify same DoubleByteBuf of the message with newly computed checksum.
However, while message creation if we see memory-leak then we create `SimpleLeakAwareByteBuf` or `AdvancedLeakAwareByteBuf` (based on `ResourceLeak Level`) instead   `DoubleByteBuf`. So, should we keep this check.";design_debt
"This is actually somewhat problematic, since two arrays might be unequal but their unequal parts are ""masked"" by the parent bitmap. For example
so the data is technically equal, even though the children are different when you examine them without the ""mask"" of the parent bitmap
this suggests that the `Equals` method should accept an inclusion bitmap, which adds a lot of complexity. @emkornfield what do you think?";code_debt
Sharing is fine for children of the same parent. It makes the 'remove' redundant, but saves a lot of garbage.;code_debt
I tried breaking up the new test method in `WorkerSinkTaskTest` into multiple methods, but it actually just made it harder to read.;code_debt
+1 for reducing the redundancy.;code_debt
Typing here seems not needed. If source is optional, you can also give it a default value, otherwise maybe make it non-optional. There is actually an [eslint rule](https://github.com/yannickcr/eslint-plugin-react/blob/master/docs/rules/require-default-props.md) for this in the PropTypes world.;code_debt
@ravening I don't see the probably with requiring an internal DNS entry.  The operator can always enter the external DNS IP for it if they need to.  I can only see this causing pain elsewhere in the code.;design_debt
"ParquetFileFormat leaks opened files in some cases. This PR prevents that by registering task completion listers first before initialization.
Manual. The following test case generates the same leakage.";design_debt
Thanks for the much better solution. That works just fine.;design_debt
Would prefer `List<KeyValueMap>` here, it's generally easier to work with.;code_debt
Does this mean that conversion could give the wrong results (in addition to being leaky)? If so, can you add a test showcasing that? (I believe you need the different chunks to be unequal...).;design_debt
Why is `name` needed? The `MessageType` has a name that can be used instead of passing it in separately.;code_debt
Unnecessary semicolon at the end of the line.;code_debt
"This PR makes various changes to improve how throttling server uses config library.
* Add a policies endpoint to throttling server to query the policy for a particular resource.
* Add expiring resources to broker so policies can be reloaded (to always get the newest one from config library).
* Refactor config library client to make loading of few configs more efficient.
* Refactor Hadoop fs config store to make it less confusing.";code_debt
"Let's pass the map only, Map<GlobalRebalancePreferenceKey, Integer>.
ClusterConfig is too much for the rebalancer.";code_debt
Why is nullCounts checked for being null but minValues and maxValues being used without a similar check?;code_debt
nitpick - there should be blank line above the comment like before;code_debt
Removed `reportParseExceptions` here, it should always be true (always thrown, handling depends on config);code_debt
I don't think we really need this -- with multiple webserver worker processes any refresh would lead to confusing state (some on new list, some on old, and no way to tell which is which) unless we have some way to make _all_ workers perform it.;code_debt
The curly braces are not needed. The same for the following `case` statement.;code_debt
This call is effectively just cloning the properties, but we're already doing the clone inside of `.jdbc()` itself, so we don't need this.;code_debt
"It's not okay catch and ignore all throwables. E.g. OOMs should NEVER be ignored as it leads absolutely unexpected situations.
At best, you can catch `NonFatal(ex)` and ignore those (only after logging as a warning). For other throwables, log as a warning, and rethrow.";code_debt
LockedDriverState is a nested class within Driver, while it is used outside of it as well, and it is complex enough to be a class on it's own. DriverState should be it's nested class, and transitions / locking should be facilitated by functions within it.;design_debt
For consistency, `Zookeeper` should be replaced with `ZooKeeper` to match the official naming.;code_debt
"I see, so the example above passes in codegen off and fails with codegen on with this fix, while using `Map(3 -> 7, 6 -> -1)` passes codegen on and fails codegen off, am I right?
What I am thinking about (but I have not yet found a working implementation) is: since the problem arise when we say we expect `null` in a non-nullable datatype, can we add such a check? I mean, instead of pretending the expected value to be nullable, can't we add a check in case it is not nullable for being sure that it does not contain `null`? I think it would be better, because we would be able to distinguish a failure caused by a bad test, ie. a test written wrongly, from a UT failure caused by a bug in what we are testing. What do you think?";design_debt
"You should be able to have a single run with ""-Pkinesis-asl -Pyarn -Phive -Phive-thriftserver"" - I even think ""-Phive"" is unnecessary, I think it only affects packaging right now.
""-Phadoop2.2"" is unnecessary, that's the default.";code_debt
"@rdblue yea it's possible. In this PR, I try to adopt your suggestion to make it clear that `TableProvider.getTable` should take all the table metadata, so the method signature becomes
`TableProvider` has another `getTable` method which needs to infer schema/partitioning, and previously the method signature was
To make it consistent, I change it to use `properties: Map[String, String]`, also rename it to `loadTable` since we need to touch many files anyway.
We can still keep the old method signature with a TODO to change it later, so that this PR can be much smaller.";design_debt
"*This work is a preparation for FLINK-11726.*
*In `SingleInputGate#create`, we could remove unused parameter `ExecutionAttemptID`.
And for the constructor of `SingleInputGate`, we could remove unused parameter `TaskIOMetricGroup`.
Then we introduce `createSingleInputGate` for reusing the process of creating `SingleInputGate` in related tests.*
This change is a trivial rework / code cleanup without any test coverage.";code_debt
Do we actually want this to be `Long.MAX_VALUE`? Seems error prone since the normal thing to do with the TTL is add it to the current time and that will cause overflow. Should we have some sentinel value for infinite TTL?;code_debt
nit: We can have just `exception when duplicate fields in case-insensitive mode` as test title. Original one is too verbose.;code_debt
"In #2237, `administration-auth` is removed. However it is not removed from sidebard. It is causing failures on building website.
Remove `administration-auth` from the sidebar. Also remove codebase page, which doesn't make sense to be there because code changes are happening very frequently. The page quickly becomes out-of-dated.
Additionally cleanup a few things in the sidebar.";code_debt
nit: use the api using `jsonFormatSchema`;code_debt
This is getting a bit confusing -- this seems the same as `currentWorkerAssignment`. What mutates that causes this to be different?;code_debt
whitespace:tabs in line;code_debt
This exception message looks a bit confusing. We can say the given type is not supported and we only support the certain type (`java.util.List` and `java.util.Map`).;code_debt
I also think, it's difficult to apply new style checker only to new codes. I cleaned up codes in origin/master for the style checker suggested in this PR.  So, if this PR is merged, then we can enforce the new style to developers and all developers have to do is to check the style of the code changed by them.;code_debt
indentation is off the hook;code_debt
"This test should probably be named as TestZkClientAsyncFailureMetric since your new code is only adding metrics. The aync function should be already covered in existing tests. 
With that said, have you checked whether it's possible to only add metrics testing to existing tests by mocking the results? That would save us one additional test file. If there is no available code path or too many efforts involved, I'm fine with having a separate test too. But please check the zkclient and monitor test.";code_debt
"This PR improves the implementation of GPU `argwhere` added in https://github.com/apache/tvm/pull/6868, using exclusive scan (see https://github.com/apache/tvm/pull/7303). 
The current implementation of `argwhere` is very inefficient, because it uses atomic to update the write location. Since all threads compete for the single location, this effectively makes it a sequential kernel. Moreover, since the output indices need to be lexicographically sorted, the current implementation involves sorting along each axis.
Since `argwhere` is literally an instance of stream compaction, this is a perfect application of exclusive scan. Now, `argwhere` simply consists of 
* A single call to exclusive scan on a boolean flag array to compute the write indices.
* Compaction using the write indices (just copying elements with nonzero condition).
both of which are highly parallel operation. Thus, both atomic and sort are gone, vastly simplifying the implementation. Moreover, it also brings huge speed up, as shown below.
All numbers in milli sec
please review @zhiics @Laurawly @mbrookhart @tkonolige @anijain2305 @trevor-m";code_debt
"It will be cleaner to simply pass the metrics registry associated with this component and register more granular group of metrics under `ZkUtilsMetrics`. Overloading it with `ZkJobCoordinatorMetrics` is confusing as this component - ZkUtils is also accessed from CoordinationService. 
HTH. Thanks!";design_debt
No need to use ```LinkedHashMultimap``` because the key should be unique.;code_debt
After rethinking about this, I think it is better to indicate this threshold also determines the number of threads in parallelism. So it should not be set to zero or negative number.;code_debt
"Fixed bug whereby calling `registerError` with the following data set `[""A"", ""B"", ""C"", ""D"", ""E"", ""F""]` would return `[[""A"", 1], [null, null], [null, null], [null, null], [null, null]]` instead of `[[""A"", 1], [""B"", 1], [""C"", 1], [""D"", 1], [""E"", 1]]`.
Improved JavaDoc for `registerError`
Also removed JavaDoc which did not add anything to the method names.
Made the code more readable and at the same time fixed a subtle error.
On my spock branch:
- Bug fix (non-breaking change which fixes an issue)
[style-guide]: https://wiki.apache.org/jmeter/CodeStyleGuidelines";code_debt
Is this really needed? Isn't BULK_LOAD_HFILES_BY_FAMILY loaded as false by default if not set on LoadIncrementalHFiles#195? Or perhaps you can move this for a separate method that could be overridden by  TestLoadIncrementalHFilesByFamily to avoid duplicating whole setUpBeforeClass in the child class?;code_debt
I want to use this decorator also to prevent regression in scheduler performance. Some methods are critical and I have optimized it to use very few queries, but it can be easily broken.  This context manager will allow us to detect a regression regarding it.;design_debt
Unused, please remove;code_debt
"Regarding this naming convention, should these be `*.bind.address` or `*.address.bind`? What do you think? `master.bind.address` reads more naturally, but `master.address.bind` supports logically grouping address-related configs in a configuration hierarchy, which can be better for parsing, and more easily incorporate future changes.
An example of a future change which could benefit from the `*.address.bind` form could include work that address an explicit public advertisement address, in the case of the bind address not being publicly reachable (in the case of more complicated networking setups, such as those on some cloud services' infrastructure):
Example:
I'm leaning towards the `<service>.address.<addressType>` naming convention, rather than the `<service>.<addressType>.address` convention that you currently have here, but am open to discussion... because naming is hard. What do you think?";code_debt
nit: also `equals` here;code_debt
"Suppose to be && instead of || ?
line 123: 
boolean usePartitionInfoForDataMapPruning = table.isHivePartitionTable() && filter != null && !filter.isEmpty() && partitions != null
and use the same flag to reset in line 138, it is confusing now";code_debt
"No. That's a side improvement which can be dropped, not a major goal.
As I commented, fixing the problems on DataStreamWriter isn't the purpose of introducing DataStreamWriterV2. This is rather providing symmetric user experience between batch and streaming, as with DataFrameWriterV2 end users can go through running batch query with **catalog table** on writer side, whereas streaming query doesn't have something to enable this.
The problems I described in previous comment are simply the problems on Structured Streaming - let me explain at the end of comment, as it might be going to be out of topic.
I see DataFrameWriterV2 has integrated lots of other benefits (more fluent, logical plan on write node, etc.) which should be great to have in DataStreamWriterV2, but I think they're not a key part of *WriterV2. Supporting catalog table is simply the major reason to have it.
Regarding the problems on Structured Streaming - 
I kicked the incomplete state support on continuous mode out from Structured Streaming, but I basically concerns about ""continuous mode"" itself, as it's rather applying hacks to workaround architectural limitation. (+ No one cares about it in community.) 
And as I had initiated discussion earlier (and has been commented in various PRs), I think complete mode should be kicked out as well. The mode addresses some limited cases but is treated as one of valid modes which adds much complexity - some operations which basically shouldn't be supported in streaming query are supported under complete mode, and vice versa. Because the mode doesn't fit naturally.
It's useful for now because Spark doesn't support true update mode on sink - and once Spark can support update mode on sink, content in external storage should be just equivalent to what the complete mode provides, without having to dump all of the outputs. (Or that's just because of missing feature - queryable state.) Probably we can simulate complete mode via having a special stateful operator which only works with update mode.
Specific to micro-batch, supporting DSv1 is also a major headache - lots of pattern matchings in MicroBatchExecution are to support DSv1, and even there're workarounds applied for DSv1 (e.g #29700). I remember the answer in discussion thread that DSv1 for streaming data source is not exposed to the public API which is great news, but I see no action/plan to get rid of it. Is there something DSv2 cannot cover the functionality which is possible in DSv1? If then why not prioritize to address the problem?";design_debt
I understand this is hardcoded now, but expected this naming convention would change. So are we going to continue with the NNN-SNAPSHOT convention for naming?;code_debt
"@yvsubhash you are right... My problem is not related to this one here. The snapshots in XenServer is working as you said. However, sometimes if exceptions happen during copy of the snapshot to the secondary storage, snapshots in the primary storage are not cleaned. I am still investigating and trying to find a way to solve this problem.
BTW thanks for the reply!";design_debt
nit. `//  Need` -> `// Need`. (reducing one space between `//` and `Need`.).;code_debt
nit: please use the upper case where possible, `LIST ARCHIVES`.;code_debt
"Same here  - there are possibly few hundreds of places all over our code where "" no quote on the left side inside [[ ]] "" is  followed. I'd prefer to keep it - it makes the code a bit more readable.";code_debt
Two lines looks duplicated. We could remove duplicated two lines via following:;code_debt
Looks good to me, this is definitely cleaner;code_debt
Converted bang to ask to avoid scary warning when a block is removed;code_debt
Can we get rid of this?;code_debt
Total nit, but isn't it clearer to write `isSupportedComparison` and avoid inverting the logic everywhere?;code_debt
i want to return the same so that it can be reused in next trigger.;code_debt
This will slow down this performance critical code.;code_debt
Keep four spaces indentation for these parameters. Same for method `createProjectionRexProgram`;code_debt
clean up transport map & docs;code_debt
"Actually , looking at com.alibaba.rocketmq.common.ServiceThread from http://grepcode.com/file/repo1.maven.org/maven2/com.alibaba.rocketmq/rocketmq-common/3.2.6/com/alibaba/rocketmq/common/ServiceThread.java/ it looks like ServiceThread does a lot of synchronization and notifying on itself. That makes this impl much harder to review without an intimate understanding of ServiceThread.
Regarding testing, The general recommendation if the framework does not have a unit-test friendly tool, is to use EasyMock to force the behaviors you are testing for.";design_debt
This cast is unnecessary since the `Sum` method already returns a `double`.;code_debt
Moving to C++ and better error message;code_debt
It seems like this method is no longer needed.;code_debt
I think these error message would be more useful if they included the full path of the client props file that was used.;code_debt
nit: spaces around `{`;code_debt
"Even though this class is internal, it's widely
used by other projects and it's better to avoid
breaking them until we have a publicly supported
test library.";design_debt
"nit: this is kinda hard to read with the double negatives. consider:
?";code_debt
Added a test for the builders for Read, and cleaned up the code a bit for Write (since topic should never be null there).;code_debt
Is this a debugging println? Should this line be removed?;code_debt
No sure -- we also have upgrade info for 0.9. and 0.10 -- maybe this need a general clean-up (so not part of this PR)?;code_debt
my fault,I should remove irrelevant code,actually,test_base.py has nothing to do with this PR;code_debt
Breeze was slightly too chatty when there was no dirs created;code_debt
"-->
Currently, there are some transformation names are not set in blink planner. For example, LookupJoin transformation uses ""LookupJoin"" directly which loses a lot of informatoion.
1. Introduces a RelWriter `RelDisplayNameWriterImpl` to reuse code of ""explainTerms"" to generate operator names
2. Fix some operator names are not set in blink planner
This change is a trivial rework / code cleanup without any test coverage.";code_debt
"* Fixed bug in Struct.equals where we returned prematurely and added tests
* Update RequestResponseTest to check that `equals` and `hashCode` of
the struct is the same after serialization/deserialization only when possible.
* Use `Objects.equals` and `Long.hashCode` to simplify code
* Removed deprecated usages of `JUnitTestSuite`";code_debt
~I think it is better to explicitly declare the data instead of manipulating strings, that way it is very clear what the input data is for the example.~ On second thought, never mind this comment - it's pretty clear the way it is;code_debt
"Some messages get excessively noisy under high traffic conditions if
something about their mechanism goes wrong. The pipe logging feature,
for instance, will emit warning and error messages on every single log
event if the reader goes down or the pipe buffer fills up. This can
result in thousands of log messages being emitted per second, which
makes reading the logs difficult and causes disk space issues.
This commit addresses this issue by adding throttled versions of the
common logging messages so they only emit a message on some set
interval (60 seconds as a default). The following functions are added:
SiteThrottledStatus
SiteThrottledNote
SiteThrottledWarning
SiteThrottledError
SiteThrottledFatal
SiteThrottledAlert
SiteThrottledEmergency
As a bonus, these are implemented using a generic Throttler class which may also be
useful in other applications where throttling is desired.";design_debt
Note that the `namespace` and `action` would remain same for all test. Though we do a cleanup after each run it may be better to use different name for each test run;code_debt
LGTM, other than the naming issues (StandingQuery, etc. in the code);code_debt
okay, so I'll have to fix a few checkstyle issues :);code_debt
I think we can just get rid of it. I can't imagine both functions are specifically broken alone in `selectExpr`.;code_debt
do not need `ifPresent`;code_debt
optimize some code styles;code_debt
change(CLI): removed duplicated `include` option.;code_debt
"Descriptions of the changes in this PR:
This is cherry-pick from yahoo repo of branch yahoo-4.3.
original commit is:
https://github.com/yahoo/bookkeeper/commit/42bdc083
Release addEntry-Bytebuf on readOnlyBookie to prevent memory-leak";design_debt
I would suggest to simply use `mx.image.imread()` and then call `.asnumpy()` before plotting. There shouldn't be a need of a dependency on `cv2` that way.;code_debt
A few small logging improvements which help debugging replication issues.;code_debt
Not an incredibly big deal, but getSupportedPropertyDescriptors() is called very often (most often in validation during a UI Refresh) which causes lots of ArrayLists to be created.  I prefer the way DistributeLoad handles this, by creating one ArrayList and reusing it, while also using an AtomicBoolean to know when it should be recreated.;code_debt
"Added quotes to prevent syntax errors in weird situations.
Error seen in mgt server:
Cause:
Somehow a nic was missing.
After fix the script can handle this:
The other states are also reported fine:
While at it, I also removed the INTERFACES variable/constant as it was only used once and hardcoded the second time. Now both are hardcoded and easier to read.
This is the same as PR #1249 except it is against `4.7`.";code_debt
TAJO-1736: Remove and improvement a unnecessary method(getMountPath()).;code_debt
I knew that code should have been simpler :) updated it to use the readValueAsTree;code_debt
Extra horizontal lines;code_debt
I believe setting the compile scope is redundant;code_debt
nit: 0L;code_debt
Would you want to check if sc is a SparkContext instance to raise a more meaningful error?;code_debt
"It may have them from legacy someone put it there, but if you were designing a collections class, you'd design it in a fashion so it focussed just on the logic it needs to have. And any interaction needed is supplied generically, e.g. for your case, youd need generic method: remove(Predicate<SimpleString> predicate). This is a collection class in reality so i apply the same rules of engagement..
If anything that field, the flag and the method check hasInternalProperties really should move up to CoreMessage, as its only used there. Doing that would mean you can still get your benefit as then the check stays in CoreMessage. 
This would really clean up TypedProperties to just having fields it really should only care for, keeping it clean.";design_debt
PARQUET-318: Remove unnecessary object mapper;code_debt
"With code changes in https://github.com/apache/spark/pull/21847 , Spark can write out to Avro file as per user provided output schema.
To make it more robust and user friendly, we should validate the Avro schema before tasks launched.
Also we should support output logical decimal type as BYTES (By default we output as FIXED)
Unit test";design_debt
"This particular test (and probably others in this file) could be made runner-agnostic. That would make them much better.
However, this is an underlying problem here. Ok to ignore for now.";code_debt
"Hi Cheng Hao,  Thanks for implementing these functions.  I bet they will be much faster than using HiveUDFs as we do now!
Based on the number of comments on the test case refactoring, I think it would be easiest to try and commit just the LIKE and RLIKE with a few simple test cases, and then take on the rest in a separate PR.";code_debt
"No particularly strong feelings from me, but I'd lean towards including the no-arg constructor for two reasons:
1. It's consistent with the other enrichers (we could change them all, but having some of each pattern seems more confusing).
2. The contract for enrichers/policies/entities/locations to be instantiated through the `EnricherSpec` etc is that the class must have a no-arg constructor. We don't expect people to call this constructor directly.
The second point means we probably should include the constructor with a comment. We could maybe even change `InternalPolicyFactory` etc so that it can handle calling protected constructors, which would enforce that more.
Anyway, I'm happy to ignore it in this PR.";design_debt
Oops my bad, also removed the unnecessary sbin/../ from the other tachyon paths;code_debt
Nanoseconds is too much. Milliseconds will be enough.;design_debt
"This check makes sure the code always stays in sync. However, if we can make great code reviews (like yours), this static check should be unnecessary :)
I will remove this painful check (to me also) in next commit.";design_debt
nit: space after `if`;code_debt
@godfreyhe I think that this is a valid tradeoff for now. In the future we may have an exception that says that `CollectionEnviroment not allowed to be used with TableEnvironment. Please use XXXX instead`. For `DataSet` we do not have this problem because the execution logic is hardcoded in the environment.;code_debt
@benstopford @fpj This looks good to merge aside from the comment I left about the `use_authorizer` parameter to `KafkaService`. We only have one `Authorizer` implementation, so the parameter kind of makes sense in the scope of only Kafka's tests, but we should think of the `Service` classes as semi-public interfaces we want to be careful about changing -- they should be reusable by others to write system tests. If anyone wanted to test other authorizer implementations, I think the current `use_authorizer` parameters wouldn't be a great solution -- they'd have to add still another option (or make assumptions about the implementation by setting the `authorizer_class_name` field on `KafkaService` directly).;design_debt
This exception is too broad;code_debt
variable name could be better perhaps;code_debt
Got it, my reasoning is that it could be harder for someone looking at the code to figure out why this is not allowed, since we don't really mention about the rest server which is really the one requiring security to be turned off. Another reason it will be beneficial to have the check in the MesosRestServer is that the MesosClusterDispatcher framework could technically be decoupled from the MesosRestServer and allow another way to receive requests, so to increase flexibility and avoid someone forgetting about why we put this here, my suggestion is to move the check closer to where it's being required will help maintain this a bit better.;design_debt
"I ran a quick benchmark of just the javadoc task of `master` vs `LUCENE-9215` branch:
`./gradlew clean compileJava && time ./gradlew javadoc`
master: BUILD SUCCESSFUL in 3m 21s
LUCENE-9215: BUILD SUCCESSFUL in 3m 22s
So this check pass is effectively free and doesn't slow down javadoc processing at all (the hard part is already done), and I think it is a lot easier dealing with the errors directly from `gradlew javadoc`: things like filenames and line numbers really help. Plus we remove the previous python script which was recursively parsing HTML, and that thing was never instant.";code_debt
indentation;code_debt
This is unnecessary.;code_debt
"It is, but the less load we can place on the reviewers the better. If it's possible without lots of effort anyway.
Everything in `tests/utils` is badly named btw -- they are utils _for_ tests, not tests themselves.";code_debt
indentation here is not multiple of four;code_debt
"I am just starting to review, but I want to get some principles out beforehand:
1. The shaded path should absolutely never be used. It is derived from the module name just to make it unique. The reason we shade is as a way to isolate ""implementation detail"" dependencies. If this ends up on an API surface that is a bug. We had some tests for this, but they have rotted.
2. We work pretty hard to avoid Guava on the API surface, since the risk of diamond dependency conflicts is very high.
3. It is OK for an IO to have its own esoteric dependencies - including Guava - if the thing it is connecting to requires it. So if it is _Cassandra_ that requires Gauva on the API surface, then it can be included in the deps.
4. For those situations where Beam wants to use Guava internally, we are (slowly) moving to depend on `beam-vendored-guava-20_0`";design_debt
"-->
To complete finish the translation work of this page.
This page is about Flink's Table API & SQL. Mainly showing the usage of Table API & SQL, how Flink optimize SQL queries and the difference between Blink and Flink planner. Now it has been translate into Chinese.
This change is a trivial rework / code cleanup without any test coverage.";code_debt
"* Removed log4j config being done in Java but some remains
* Logging is now configured using standard log4j JVM property
  'log4.configuration' in accumulo-env.sh
* Tarball ships with less log4j config files (3 rather than 6)
  which are all log4j properties files.
* Log4j XML can still be used by editing accumulo-env.sh
* Moved AsyncSocketAppend to start module due to classpath issues
* Removed auditLog.xml and added audit log configuration to
  log4j-service & log4j-monitor properties files
* Accumulo conf/ directory no longer has an examples/ directory.
  Configuration files ship in conf/ and are used by default.
* Accumulo monitor by default will bind to 0.0.0.0 but will
  advertise hostname looked up in Java for log forwarding
* Shortened names of logging system properties
* Removed MonitorLoggingIT as it is now difficult to setup log
  forwarding using MiniAccumuloCluster.";code_debt
"Aren't 1, 2 and 4 the same as the 1, Shorts.BYTES and Ints.BYTES from the part above?
This code is really confusing me...";code_debt
Nit: seems a bit redundant. Can we not assign the size in the line below?;code_debt
"JIRA: MADLIB-1206
This commit adds support for mini-batch based gradient descent for MLP.
If the input table contains a 2D matrix for independent variable,
minibatch is automatically used as the solver. Two minibatch specific
optimizers are also introduced: batch_size and n_epochs.
- batch_size is defaulted to min(200, buffer_size), where buffer_size is
  equal to the number of original input rows packed into a single row in
  the matrix.
- n_epochs is the number of times all the batches in a buffer are
  iterated over (default 1).
Other changes include:
- dependent variable in the minibatch solver is also a matrix now. It
  was initially a vector.
- Randomize the order of processing a batch within an epoch.
- MLP minibatch currently doesn't support weights param, an error is
  thrown now.
- Delete an unused type named mlp_step_result.
- Add unit tests for newly added functions in python file.
Co-authored-by: Rahul Iyer <riyer@apache.org>
Co-authored-by: Nikhil Kak <nkak@pivotal.io>
Closes #243";code_debt
Btw, there are some really long lines in this PR. Our convention is that lines should not be longer than the GitHub review window.;code_debt
LUCENE-9069: Prevent memory leaks in PerFieldAnalyzerWrapper;design_debt
This is fine with me but FWIW I wouldn't even bother defining it.  It has no value set aside like this I doubt any user code would want to refer to it.  If we want to document what the default cost is, we should say so in cost()'s javadoc.  I know some devs like to make static constants for everything but IMO it's sometimes wasted ceremony.;code_debt
it might be slightly faster to take the minimum of length and only compare that in the loop check?;code_debt
Personally, I would get rid of this and use `extensionValue` and `extensionNames`. Otherwise, as @rondagostino said below, we should remove `extensionValue`.;code_debt
"This ` <exclude>javax/**</exclude>` exludes validation-api as well. Therefore it should be deleted. 
To avoid including unnecessary libraries I decided to add:
`               <exclude>javax/activation/**</exclude>`
`               <exclude>javax/annotation-api/**</exclude>`
`               <exclude>javax/inject/**</exclude>`
`               <exclude>javax/servlet-api/**</exclude>`
`               <exclude>javax/json/**</exclude>`
`               <exclude>javax/ws/**</exclude>`";code_debt
"Also, could you make the stats at the top of the page ""Waiting batches"" and ""Processed batches"" links to the corresponding sections? And the names should be consistent, so please rename them here, keep them as ""Active Batches"" and ""Completed Batches"".";code_debt
"This worries me.
I'm ok with having the tests run with `Parameterized`, but if we do so we should still be able to determine which case we're in and set the expected exception per-test within the test based on the value of the parameters, rather than inspecting the name of the test. For example, `testConflict_runnableOnServiceAnnotation_expectIllegalStateException` doesn't demonstrate by reading the test code directly that it expects an exception.";code_debt
I think we really need to consider option 1, eventually we may need to clean up a bit the camel's bom but I think that as long term we'll end up supporting most of the component camel supports and the chance to hit this issue again is in my opinion high;design_debt
"+1 lgtm
@liyafan82 , thanks for the contribution. I suggest you add a similar flag for isSet() to be optionally skipped in a separate jira to improve performance";code_debt
"Thanks for this - a few minor comments/nits:
1. Could you please rename tests that start with ""Test*"" to our more standard ""should*""
2. Have a look at where you might add more `final` declarations to match the code style.
3. I like the idea of integration tests with the `SimpleWebSocketServer` - very smart
4. I'm surprised you found as many places as you did where `Cluster.close()` wasn't called.
5. I don't see where the semantics of any of the `GremlinDriverIntegrateTest` tests changed so it seems you accomplished this fix without breaking behavioral changes in the driver...that's nice.
6. Maybe I missed it but was there a test in Gremlin Server to validate any of this change - perhaps it was already better tested by way of your `SimpleWebSocketServer` tests?
7. I assume you will polish up the commit history a bit on merge and squash things down to a few (or one) commits?";code_debt
"Hi, @pwendell , Thank you for your comments, here is my reply
First, ""whether accumulator value should be subject to change in the case of failure""...I think no, though during a long period, Spark runs in this way (this patch is actually resolving a very old TODO in DAGScheduler.scala)...I think accumulator is usually used to take the task which is more complicate than rdd.count/rdd.filter.count (Maybe I'm wrong), e.g. counting the sum of distance from the points to a potential center in K-means (see mllib), I think in this case, the health status of the cluster should be transparent to the user, i.e. the final result of K-means should be irrelevant to whether executor is lost, etc....
Second, Good point, I can understand what the use scenario is, but do you mind providing more details like how to implement this in Spark? I guess this can be solved by providing a approximateValue API in Accumulator or SparkContext....
Third, actually, this patch ensures that the value of the accumulator in a stage will only be available when this stage becomes independent (means that no job needs it any more, https://github.com/apache/spark/pull/228/files#diff-6a9ff7fb74fd490a50462d45db2d5e11L400), if a job finishes, and the other job still needs the certain stage, the accumulator value calculated in that stage will not be counted...";design_debt
"It's a more efficient way to get the last item from a generator - no need to iterate over every item from the generator just to get to the last one.
It might make a difference in case of lots of events.";code_debt
"In the case of a large amount of data, in order to ensure the performance of normal writing,
1. Should we use asynchronous Rewrite? (use AsyncWaitOperator)
2. Should you add a switch to control whether to  enable rewrite";code_debt
Could you add the code to avoide the infinite retry loop on error & also checking thread interrupted incase something else swallows the thread interruption exception in the future?;code_debt
better to add the value of max journal id and image id in response msg for easy debugging.;code_debt
"here are the results on my workstation:
the update path is somewhat slower (because the update must start by 1st copying the latest snapshot and then applying the new data to the copy), but starvation is gone.
also note that metadata codepaths in general do an awful lot of copying - seems to me that converting the entire class to java (or at least making it use 100% java collections) would avoid multiple copies going to/from KafkaApi (all those toJava() calls).";code_debt
I hope someone can figure out why I couldn't get that to work. It was *very* confusing.;code_debt
Nit: can the last two args simply be m, n for clarity?;code_debt
"nit: Always better to use parameterized logging for performance [1]. Here info is the default, so probably doesn't matter as much. 
[1] https://logging.apache.org/log4j/2.x/performance.html";code_debt
Not used anymore.;code_debt
[CARBONDATA-3107] Optimize error/exception coding for better debugging;code_debt
throw `IllegalArgumentException` or `UnsupportedOperatorException` makes more sense to me.;code_debt
As above. Simplify to:;code_debt
the default should be made a constant somewhere instead of being defined in multiple places;code_debt
nit: extra line;code_debt
"Much thanks for the in-depth review!
Is the scenario you are worrying about (two workers running the same task instance) already possible? For example if a worker's communication with the DB gets interrupted, then the scheduler assigns the task instance to a new worker, and then the communication between the initial worker and the DB resumes.
This makes sense. I misspoke in the PR description though, SLAs should still be sent, the difference would be the SLA email would now omit task instances in the dagrun that didn't succeed for reasons other than depends_on_past not being met (e.g. a task that couldn't run because it's pool was full won't get reported in the email). I think I'm going to just include all tasks that don't have a successful status in the SLA miss email, even those stuck on depends_on_past to align with your criteria (if the task caused core_data to not be delivered by 9AM the task caused the DAG to miss it's SLA regardless of it's depends_on_past_dependency), plus is stops treating depends_on_past differently from the other dependencies like the pool being full. LMK what you think.
Agreed about the efficiency, was going to look into caching if this causes perf issues.
The newfound power of the force flag could be used instead of ignore_depends_on_past, but making ""force"" the default for every backfill could potentially be a bit dangerous as users could e.g. unintentionally force run over a large range of already successful tasks in a backfill or violate a pool constraint. If you have any ideas let me know.
Agreed about not passing in a bunch of different flags. There is actually a TODO above that part of the code in the PR to use a context parameter instead (it will be addressed in this PR).
For the flag upstream_failed I would prefer to leave the fix for another PR since it was an existing hack and the cope of this PR is already a bit dangerously large.";code_debt
"Improve `KerberosDescriptorResourceProvider`:
 * Return HTTP 409 if trying to create duplicate `kerberos_descriptor` instead of HTTP 500 with ugly stack trace
 * Clarify message for incomplete request
 * Clean up the unit test
 * Minor clean-up in `KerberosDescriptorResourceProvider`
https://issues.apache.org/jira/browse/AMBARI-25025
Added test case in unit test.
Tested manually:";code_debt
@n3nash : Thanks. Added both Unused and Redundant Imports in checkstyle and corresponding code-style. If there are any other rules missing, we can add them in future PRs.;code_debt
"My understanding is that this allows pointing the Spark UI directly at the history server (old JHS or new ATS) instead of hardcoding the NM URL and relying on the NM redirecting you, since the NM may not exist later on.
That does open up some questions though. The code being modified is in the AM, which means that the user needs to opt into this when he submits the app, when perhaps if there was a way to hook this up on the Spark history server side only, that may be more useful.
I think someone tried that in the past but the SHS change was very YARN-specific, which made it kinda sub-optimal.";code_debt
"Let's drop this comment for two reasons:
1. First part of the comment is describing the same thing as the code in English, it is not adding additional context.
2. Second part is dataflow runner specific, and the example does not even run on dataflow runner yet.";documentation_debt
"""We should omit this comment for two reasons: Firstly, the initial part of the comment merely reiterates what the code already expresses in English without providing any additional context. Secondly, the latter part pertains specifically to the dataflow runner, but the given example has not been executed on the dataflow runner as of yet.""";documentation_debt
"""For two reasons, it would be best to exclude this comment: Firstly, the initial portion of the comment redundantly describes the same thing as the English code without adding any extra context. Secondly, the second part of the comment is specific to the dataflow runner and the provided example has not yet been executed on it.""";documentation_debt
"""We can remove this comment for two reasons: Firstly, the first part of the comment duplicates the information expressed in the code itself in English, without providing any additional context. Secondly, the second part of the comment is relevant only to the dataflow runner, and the given example has not yet been executed on it.""";documentation_debt
"""Let's disregard this comment for two reasons: Firstly, the initial part of the comment redundantly describes the same thing as the English code, without offering any additional context. Secondly, the second part is specific to the dataflow runner, and the provided example has not been run on it yet.""";documentation_debt
why call `res_fun` twice?;documentation_debt
"""What is the reason for calling 'res_fun' twice?""";documentation_debt
"""Could you explain the purpose behind calling 'res_fun' twice?""";documentation_debt
"""I'm curious why 'res_fun' is being called twice.""";documentation_debt
"""Can you clarify the motivation for invoking 'res_fun' twice?""";documentation_debt
Looks like doc was outdated, corrected it, batchSize controls elements used in batch INSERT SQL, i.e INSERT INTO t(a, b, c) VALUES(x1, y1, z1), ..., (xN, yN, yN) Batching can speed up loading data greatly, but not all SQL database may support it, so controlling batch size gives some lever for various SQL vendors.;documentation_debt
"""It appears that the documentation was outdated, and it has now been corrected. The 'batchSize' parameter controls the number of elements used in each batch for the INSERT SQL operation. For example, it determines how many sets of values, such as (x1, y1, z1), ..., (xN, yN, yN), are included in a single batch. Batching can significantly enhance the data loading speed, although it may not be supported by all SQL databases. By controlling the batch size, it provides flexibility for different SQL vendors.""";documentation_debt
"""Upon reviewing the documentation, it was found to be incorrect, but it has been rectified now. The 'batchSize' parameter is responsible for controlling the number of elements utilized in each batch for the INSERT SQL statement. For instance, it determines the number of value sets, like (x1, y1, z1), ..., (xN, yN, yN), included in a single batch. Batching can greatly improve the data loading performance however, it might not be supported by all SQL databases. By adjusting the batch size, it offers a means to accommodate different SQL vendors.""";documentation_debt
"""It seems that there was an error in the documentation, but it has been updated with the correct information. The 'batchSize' parameter determines the number of elements used in each batch for the INSERT SQL operation. For example, it controls the inclusion of multiple value sets, such as (x1, y1, z1), ..., (xN, yN, yN), in a single batch. Batching can significantly enhance the data loading speed, although its support may vary across different SQL databases. By adjusting the batch size, it provides flexibility to cater to various SQL vendors.""";documentation_debt
"""Upon closer examination, it was discovered that the documentation contained an error, but it has been rectified accordingly. The 'batchSize' parameter is utilized to control the number of elements included in each batch for the INSERT SQL statement. Specifically, it determines the inclusion of value sets, such as (x1, y1, z1), ..., (xN, yN, yN), in a batch. Batching can greatly expedite the data loading process however, it may not be universally supported across all SQL databases. By adjusting the batch size, it offers flexibility to accommodate different SQL vendors.""";documentation_debt
"This looks a little strange to my be retrospect, but the documentation says ""To reduce the amount of time admins spend on authorization management, policies are inherited from parent resource to child resource"", so I think in practice the case where you cannot access the process group but you can access a resource within should not happen.";documentation_debt
"""Upon further consideration, I understand your concern. However, according to the documentation, policies are designed to be inherited from parent resources to child resources in order to minimize the time spent on authorization management by admins. Therefore, in practice, it is unlikely that you would encounter a situation where you have access to a resource within a process group but not to the process group itself.""";documentation_debt
"""I can see why you find it unusual, but the documentation clearly states that policies are inherited from parent resources to child resources in order to streamline authorization management for admins. Consequently, in practical scenarios, it is highly unlikely that you would have access to a resource within a process group while being denied access to the process group itself.""";documentation_debt
"""I understand your observation about the situation appearing unusual in retrospect. However, as per the documentation, the purpose of policy inheritance is to minimize the time spent on authorization management for admins by propagating policies from parent resources to child resources. Therefore, it is unlikely to encounter a scenario where you have access to a resource within a process group but not to the process group itself.""";documentation_debt
"""I can appreciate your perspective on this, but the documentation clearly explains that policies are inherited from parent resources to child resources as a means to reduce the administrative effort required for authorization management. Hence, in practice, it is unlikely for a situation to arise where you have access to a resource within a process group while being unable to access the process group itself.""";documentation_debt
For checkstyle & fix typo;documentation_debt
"""To ensure checkstyle and fix typos""";documentation_debt
"""To perform checkstyle and address any typos""";documentation_debt
"""For checking the style and fixing typos""";documentation_debt
"""To validate the checkstyle and rectify any typos""";documentation_debt
Thanks, replaced as you proposed, but also left mentioning that we have metadata about segments, files, row groups, partitions since it wasn't described in this doc yet.;documentation_debt
"""Additionally, I included a mention of the metadata regarding segments, files, row groups, and partitions, as it was not yet described in this document.""";documentation_debt
"""Furthermore, I made sure to add a note about the existing metadata related to segments, files, row groups, and partitions, which had not been covered in this document.""";documentation_debt
"""Moreover, I incorporated a reference to the metadata pertaining to segments, files, row groups, and partitions, which had not been outlined in this document.""";documentation_debt
"""Moreover, I took the opportunity to mention the available metadata regarding segments, files, row groups, and partitions, which had not been previously explained in this document.""";documentation_debt
"My fault. I should open a JIRA issue before I start on this. I thought it is a quick change.
For the WordCount part, I like to move it to a top level class, and document why Spark runners need its own copy. (+@dhalperi for Read.from doesn't work for HDFS)
For the Tfidf part, I think I am close to run it with Spark runner in beam-examples module. #533 
I am having a spark dependency problem in the test (JavaSparkContext class not found):
https://builds.apache.org/job/beam_PreCommit_MavenVerify/1908/console
Could you help me to take a look? And, you can make a call on whether you want to include the Tfidf in this PR or left to me to handle in #533.";documentation_debt
"""Apologies, it was my mistake. I should have opened a JIRA issue before starting on this task. Initially, I thought it would be a quick change.
Regarding the WordCount part, I would like to move it to a top-level class and provide documentation explaining why Spark runners require their own copy. Additionally, I noticed that using '+@dhalperi' for 'Read.from' doesn't work for HDFS.
As for the Tfidf part, I believe I'm getting close to running it with the Spark runner in the beam-examples module. You can find the details in issue #533.
However, I'm currently facing a dependency problem with Spark in the test phase, specifically the 'JavaSparkContext' class not being found. You can refer to the following link for more information: [link to build logs]
Could you please assist me in investigating this issue? Also, I would appreciate your input on whether you would like to include the Tfidf in this pull request or leave it for me to handle in issue #533.""";documentation_debt
"""I admit it was my fault. I should have opened a JIRA issue before beginning this task, as I initially believed it would be a quick change.
Regarding the WordCount part, I intend to move it to a top-level class and provide documentation explaining why Spark runners require their own copy. Furthermore, I've encountered an issue where using '+@dhalperi' for 'Read.from' doesn't work for HDFS.
Concerning the Tfidf part, I'm making progress on running it with the Spark runner in the beam-examples module, as mentioned in issue #533.
However, I've encountered a problem with a dependency related to Spark in the test phase, specifically the 'JavaSparkContext' class is not being found. You can refer to the following link for the build logs and more details: [link to build logs]
Could you assist me in investigating this issue? Also, I would appreciate your decision on whether to include the Tfidf in this pull request or leave it for me to handle in issue #533.""";documentation_debt
"""My apologies, that was my mistake. I should have opened a JIRA issue before starting this task, as I initially thought it would be a quick change.
Regarding the WordCount part, I would like to move it to a top-level class and provide documentation explaining why Spark runners need their own copy. Additionally, I discovered that using '+@dhalperi' for 'Read.from' doesn't work for HDFS.
As for the Tfidf part, I believe I'm getting close to running it with the Spark runner in the beam-examples module, as mentioned in issue #533.
However, I'm currently facing a dependency problem in the test phase related to Spark, specifically the 'JavaSparkContext' class is not being found. You can refer to the following link for more information and the build logs: [link to build logs]
Could you please help me investigate this issue? Also, I would appreciate your decision on whether to include the Tfidf in this pull request or leave it for me to handle in issue #533.""";documentation_debt
"""I take full responsibility for that oversight. It was my mistake not to open a JIRA issue before starting on this task, as I initially believed it would be a quick change.
Regarding the WordCount part, I propose moving it to a top-level class and providing documentation to explain why Spark runners require their own copy. Additionally, I discovered that using '+@dhalperi' for 'Read.from' doesn't work for HDFS.
Regarding the Tfidf part, I'm making progress in running it with the Spark runner in the beam-examples module, as mentioned in issue #533.
However, I'm currently encountering a dependency problem related to Spark in the test phase, specifically the 'JavaSparkContext' class cannot be found. You can refer to the following link for more details and the build logs: [link to build logs]
Could you kindly assist me in investigating this issue? Also, I would appreciate your decision on whether to include the Tfidf in this pull request or leave it for me to handle in issue #533.""";documentation_debt
"Looks good.
I would expect a change in the documentation when you add support for a new feature )";documentation_debt
"""You're absolutely right. When adding support for a new feature, it's essential to make corresponding changes in the documentation to reflect the updated functionality.""";documentation_debt
"""Indeed, it's important to update the documentation when introducing support for a new feature. This ensures that users are aware of the added capability and can utilize it effectively.""";documentation_debt
"""I completely agree with you. Whenever a new feature is supported, it is crucial to make appropriate updates in the documentation to provide clear instructions and information about the added functionality.""";documentation_debt
"""You make a valid point. Adding support for a new feature should be accompanied by updates to the documentation to ensure users have accurate and up-to-date information on how to utilize the new functionality.""";documentation_debt
Since it's a new interface method, can you add some javadoc please?;documentation_debt
Could you please incorporate Javadoc into this new interface method, considering its novelty?;documentation_debt
Given that this is a recently introduced interface method, would it be possible for you to include Javadoc?;documentation_debt
It would be greatly appreciated if you could add Javadoc to this new interface method, considering its novelty.;documentation_debt
Considering that this is a fresh interface method, would it be possible to include Javadoc, please?;documentation_debt
Really detected one besides the mock. You need some fix on the doc this time. :);documentation_debt
I noticed one issue apart from the mock. There is a need for some improvements in the documentation this time.;documentation_debt
Apart from the mock, I have detected another issue. This time, there is a requirement for fixing the documentation.;documentation_debt
Besides the mock, there is another problem that needs attention. Specifically, the documentation requires some fixes this time.;documentation_debt
In addition to the mock, I have identified another issue. It is necessary to make some corrections in the documentation on this occasion.;documentation_debt
What does this function do? Favor SSD? Needs comment.;documentation_debt
Could you please provide a brief description of what this function does? It seems to favor SSD, so it would be beneficial to add a comment explaining its purpose.;documentation_debt
Could you kindly provide a concise explanation of the functionality of this function? It appears to prioritize SSD, so it would be advantageous to include a comment clarifying its intended behavior.;documentation_debt
It would be greatly appreciated if you could provide a brief description of the purpose of this function. It seems to have a preference for SSD, so it would be beneficial to add a comment to elucidate its intended functionality.;documentation_debt
Can you please provide a succinct explanation of what this function accomplishes? It seems to prioritize SSD, so it would be helpful to include a comment to clarify its purpose.;documentation_debt
Add comment for this in this commit otherwise, other people would be confused.;documentation_debt
It is crucial to include a comment for this code in the commit to avoid potential confusion among other developers.;documentation_debt
Adding a comment for this code in the commit is essential to prevent confusion among other users.;documentation_debt
To avoid confusion among other individuals, it is important to include a comment for this code in the commit.;documentation_debt
Ensuring that a comment is added for this code in the commit is crucial to prevent any confusion among other developers.;documentation_debt
"It's not super clear here what this does or why you'd want to use this config element. I had to read a bit of code to understand it.
Then add a proper example with type annotation";documentation_debt
The purpose and usage of this config element are not immediately evident. It requires some code reading to comprehend its functionality. To improve clarity, it would be beneficial to provide a thorough explanation of what it does and why one would want to utilize this config element. Additionally, including a well-defined example with type annotations would greatly enhance the understanding of its usage.;documentation_debt
The functionality and purpose of this config element are not adequately explained, making it unclear why it would be useful. To enhance clarity, it is recommended to provide a comprehensive explanation of what this element does and why it is beneficial to use. Additionally, including a properly annotated example would greatly facilitate understanding.;documentation_debt
The purpose and usage of this config element are not well elucidated, making it difficult to understand its significance. To provide better clarity, it is advised to offer a detailed explanation of the functionality and benefits of using this element. Furthermore, including a concrete example with type annotations would greatly assist in understanding its application.;documentation_debt
The intention and rationale behind this config element are not clearly conveyed, making it challenging to grasp its purpose. To enhance comprehension, it is suggested to provide a thorough explanation of what this element accomplishes and why it is advantageous to incorporate it. Additionally, including a comprehensive example with type annotations would greatly contribute to understanding its usage.;documentation_debt
Fix typo;documentation_debt
"""Fix the typo in the specified location."".";documentation_debt
"""Please correct the typographical error.""";documentation_debt
"""Fix the typo in the code.""";documentation_debt
"""Please fix the typo in the code.""";documentation_debt
Looks good to me. Thanks for fixing all those typos.;documentation_debt
"""I have reviewed the changes, and everything looks good to me. Thank you for addressing all those typos.""";documentation_debt
"""I have reviewed the changes and everything looks good to me. Thank you for addressing all those typos.""";documentation_debt
"""I reviewed the changes and everything looks good to me. Thank you for addressing all those typos.""";documentation_debt
"""I have reviewed the changes, and everything looks good to me. Thank you for addressing the typos.""";documentation_debt
"@geertjanw The XSDs/DTDs are used to generate code and my interpretation of the reply from apache legal is, that the license transcends to the generated code. So for weblogic we would distribute unlicensed code (i.e. we have no right to distribute) and for jboss/wildfly we would distribute LGPL code, which is against the ASF rules.
Getting the XSDs/DTDs at runtime would be perfectly doable, but the modules would need to be reworked to no rely on code generated at build-time.
@vikasprabhakar the DTD files are present here:
https://www.jboss.org/j2ee/dtd/
The files are missing an explicit license and as such I would assume they are covered by the projects LGPL license. That at least is reflected in some of the XSDs I saw. Asking redhat would most probably reveal the same. That still leaves us with the ""wrong"" license.";documentation_debt
"""@geertjanw, I believe that the XSDs/DTDs are utilized for code generation, and according to the response from Apache Legal, the license extends to the generated code. Consequently, distributing unlicensed code would occur in the case of WebLogic, as we lack the rights to distribute it. On the other hand, distributing LGPL code for JBoss/WildFly would be against ASF rules.
One possible solution is to obtain the XSDs/DTDs at runtime, which is technically feasible. However, this would necessitate a redesign of the modules to eliminate their reliance on code generated during the build process.
@vikasprabhakar, you can find the DTD files here: https://www.jboss.org/j2ee/dtd/. These files lack an explicit license, but I presume they fall under the project's LGPL license, as some of the XSDs indicate. Confirming this with Red Hat would likely yield the same result. Nonetheless, we are still faced with the issue of having the 'wrong' license.""";documentation_debt
"""@geertjanw, I believe that the XSDs/DTDs play a crucial role in code generation. According to the response I received from Apache legal, the license extends to the generated code as well. Consequently, distributing the unlicensed code for WebLogic would be a violation as we lack the distribution rights. On the other hand, distributing LGPL code for JBoss/WildFly would also go against ASF rules.
While it is technically feasible to acquire the XSDs/DTDs at runtime, it would necessitate significant modifications to the modules to eliminate their reliance on code generated during the build process.
@vikasprabhakar, you can find the DTD files at this location: https://www.jboss.org/j2ee/dtd/. However, these files do not have an explicit license mentioned. Based on my observation of some XSDs, I assume they fall under the project's LGPL license. Confirming with Red Hat would likely yield the same result. Nevertheless, this still leaves us with the challenge of having the 'incorrect' license.""";documentation_debt
"""@geertjanw, it seems that the XSDs/DTDs play a crucial role in generating code. Based on the response from Apache legal, it appears that the license extends to the generated code. Consequently, distributing unlicensed code for WebLogic would pose a concern as we lack the rights to distribute it. On the other hand, distributing LGPL code for JBoss/Wildfly would violate ASF rules. While it is feasible to retrieve the XSDs/DTDs at runtime, it would require restructuring the modules to remove the dependency on code generated during the build process.

@vikasprabhakar, the DTD files can be found here: [insert URL]. These files do not have an explicit license, but I assume they are covered by the project's LGPL license, as observed in some of the XSDs. Confirming this with Red Hat would likely yield the same conclusion. Nevertheless, this still leaves us with the challenge of having an ""incorrect"" license.""";documentation_debt
"""@geertjanw, my understanding is that the XSDs/DTDs are utilized for code generation, and according to the response from Apache legal, the license extends to the generated code. Therefore, distributing the code generated for WebLogic would involve distributing unlicensed code, which is not permissible. On the other hand, distributing the code generated for JBoss/WildFly would mean distributing LGPL-licensed code, which goes against ASF rules.
One possible solution could be retrieving the XSDs/DTDs at runtime instead of relying on code generated during the build process. However, this would require reworking the modules to remove the dependency on build-time generated code.
@vikasprabhakar, you can find the DTD files at this location: https://www.jboss.org/j2ee/dtd/. Although the files do not explicitly specify a license, I assume they are covered by the project's LGPL license, as reflected in some of the XSDs I have seen. Contacting Red Hat would likely confirm the same information. However, this still leaves us with the challenge of the ""wrong"" license.""";documentation_debt
@wu-sheng  thx for check the structure this time.  If this version looks good, I will modify the doc and add the unit test later.;documentation_debt
@wu-sheng, thank you for reviewing the structure this time. If this version appears satisfactory, I will proceed with modifying the documentation and adding the unit test at a later stage.;documentation_debt
@wu-sheng, I appreciate your thorough examination of the structure this time. If this version meets the requirements, I will proceed with making the necessary modifications to the documentation and incorporating the unit test in the near future.;documentation_debt
@wu-sheng, thank you for taking the time to review the structure in detail this time. If you find this version to be acceptable, I will proceed with updating the documentation and implementing the unit test at a later stage.;documentation_debt
@wu-sheng, I'm grateful for your diligent inspection of the structure on this occasion. If you believe this version to be satisfactory, I will make the appropriate modifications to the documentation and include the unit test in due course.;documentation_debt
"Doc needed!  Something like:
""A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer.  When [[fit()]] and [[transform()]] are called, the stages are executed in order, and each stage may modify the dataset before it is passed to the next stage.""
Maybe say something about what happens when there are no stages too.";documentation_debt
"Documentation is required! Consider adding the following information:

""A Pipeline is composed of a series of stages, where each stage is either an Estimator or a Transformer. During the execution of the [[fit()]] and [[transform()]] methods, the stages are sequentially processed, and each stage has the ability to modify the dataset before passing it to the subsequent stage.""

Additionally, it would be beneficial to include an explanation of what happens when there are no stages:

""If a Pipeline does not contain any stages, calling [[fit()]] or [[transform()]] will have no effect on the dataset. It will simply return the input data without any modifications.""";documentation_debt
"""We need to include some documentation here. Consider adding the following description: 'A Pipeline is comprised of a series of stages, each of which can be either an Estimator or a Transformer. When the [[fit()]] and [[transform()]] methods are invoked, the stages are executed sequentially, allowing each stage to modify the dataset before passing it to the subsequent stage.' Additionally, it would be helpful to mention what happens when a Pipeline has no stages, stating that calling [[fit()]] or [[transform()]] in such cases would result in no modifications being applied to the dataset.""";documentation_debt
"""It's important to provide some documentation at this point. Let's include the following explanation: 'A Pipeline is constructed by arranging a sequence of stages, where each stage can either be an Estimator or a Transformer. Upon calling the [[fit()]] and [[transform()]] methods, the stages are executed in order, enabling each stage to modify the dataset before passing it to the next stage.' Also, let's mention the behavior when a Pipeline has no stages, clarifying that invoking [[fit()]] or [[transform()]] would simply return the input data without any changes.""";documentation_debt
"""We should add some documentation here to provide clarity. Let's consider the following description: 'A Pipeline is made up of a series of stages, with each stage being either an Estimator or a Transformer. During the execution of the [[fit()]] and [[transform()]] methods, the stages are processed sequentially, allowing each stage to make modifications to the dataset before passing it to the next stage.' Additionally, it would be helpful to explain what happens when there are no stages in the Pipeline, emphasizing that calling [[fit()]] or [[transform()]] would result in no modifications being applied to the dataset.""";documentation_debt
It _may_ be doable, but is not documented/clear. I think when I added timezones I got it working for display side (rather i got it using my custom control to decorate the fields so they get converted);documentation_debt
The feasibility of incorporating timezones is a possibility, but currently, it lacks proper documentation and clarity. From my understanding, you were able to make it work for the display side by implementing a custom control to decorate the fields, resulting in the desired conversion. However, it would be beneficial to document this process and make it more explicit for future reference and understanding.;documentation_debt
"""The addition of timezones appears to be achievable, although the documentation and clarity around this feature are currently lacking. Based on your description, you were able to achieve the desired conversion for the display side by utilizing a custom control to decorate the fields. However, it would be advantageous to thoroughly document this process and provide clear instructions for future developers to understand and replicate.""";documentation_debt
"""While it is possible to incorporate timezones, there is currently a lack of documentation and clarity on this matter. According to your explanation, you were able to implement the desired conversion for the display side by leveraging a custom control to decorate the fields. To ensure future developers can understand and utilize this functionality, it is important to document the process in detail and provide clear instructions.""";documentation_debt
"""The inclusion of timezones seems to be technically feasible however, there is a lack of documentation and clarity surrounding this implementation. Based on your description, you managed to achieve the desired conversion for the display side by using a custom control to decorate the fields. To promote better understanding and facilitate future development, it is essential to thoroughly document this approach, providing clear instructions and guidelines.""";documentation_debt
"Typo. ""Unless the..."" Don't leave us hanging!";documentation_debt
"There appears to be a typographical error. ""Unless the..."" Please don't leave us in suspense!";documentation_debt
"Mistake in the text. ""Except if the..."" Let's not keep the suspense!";documentation_debt
"There seems to be an error. ""Except if the..."" Let's not keep the suspense!";documentation_debt
"There seems to be a mistake. ""Unless the..."" Please provide more information, so I can better understand your request and assist you accordingly.";documentation_debt
"@ambud 
The code looks good except what @vesense commented. 
Two things more to address:
1. It would be better to document new configurations. Without documentation, end-users have no idea about added feature. `external/storm-hbase/README.md` and `docs/storm-hbase.md`.
2. The code already uses JDK 8 API (Map.getOrDefault()), so can't get it as it is for 1.x. Could you provide a new PR for 1.x branch?
It would be also great if you can test it (with Caffeine) on JRE7 (expected to not work but we can document the precondition for JRE version) and JRE8 (expected to work).
cc. @ben-manes Is my expectation right?
Thanks in advance!";documentation_debt
"""@ambud, 
Overall, the code appears to be in good shape, with the exception of the comment made by @vesense. However, there are two additional points that need attention:

1. It would greatly benefit the end-users if we provide documentation for the new configurations. Currently, without proper documentation, users will have no knowledge about this added feature. I suggest documenting the changes in both the `external/storm-hbase/README.md` and `docs/storm-hbase.md` files.

2. Since the code already utilizes JDK 8 API (specifically, `Map.getOrDefault()`), it cannot be directly merged into the 1.x branch. I request you to create a new pull request specifically targeting the 1.x branch.

Additionally, it would be highly appreciated if you could conduct testing, preferably using Caffeine, on both JRE7 (where it is expected to not work) and JRE8 (where it is expected to work). By doing so, we can document the precondition of JRE version in our documentation.

cc. @ben-manes - Could you kindly confirm if my expectation is accurate?

Thank you in advance for your attention to these matters!""";documentation_debt
"@ambud,

The code appears to be in good shape, except for the issue mentioned by @vesense. There are two additional points that need to be addressed:

1. It is recommended to provide documentation for the new configurations. Without proper documentation, end-users will not be aware of the added feature. Please ensure that the `external/storm-hbase/README.md` and `docs/storm-hbase.md` files are updated accordingly.

2. Since the code already utilizes JDK 8 APIs (such as Map.getOrDefault()), it cannot be directly merged into the 1.x branch. Could you please create a separate pull request specifically for the 1.x branch? This will allow for the appropriate integration while maintaining compatibility.

Furthermore, it would be greatly appreciated if you can conduct testing with Caffeine on both JRE7 (expected to not work) and JRE8 (expected to work). This will help us document the preconditions for JRE versions. 

Please also make sure to include @ben-manes in the conversation to validate the expectations.

Thank you in advance for your cooperation and efforts!";documentation_debt
"@ambud, apart from the comment made by @vesense, the code appears to be in good shape. However, there are a couple of additional items that need attention:

1. It is highly recommended to document the newly added configurations. Without proper documentation, end-users will have no knowledge of the added feature. Please update the `external/storm-hbase/README.md` and `docs/storm-hbase.md` files accordingly.

2. Since the code already utilizes JDK 8 APIs (such as `Map.getOrDefault()`), it cannot be merged as-is for the 1.x branch. Could you please create a new pull request specifically targeting the 1.x branch?

Furthermore, it would be beneficial to conduct testing, specifically with Caffeine, on both JRE 7 (expected to not work, but we can document the prerequisite JRE version) and JRE 8 (expected to work). This will help ensure compatibility and allow us to document the requirements accurately. 

Please note, I would like to bring this to the attention of @ben-manes to confirm if my expectations align correctly.

Thank you in advance for your efforts!";documentation_debt
"""@ambud, 
Overall, the code appears to be in good shape, but there is a specific comment from @vesense that needs attention. In addition to that, there are two more items that require addressing:
1. It would greatly benefit the end-users if we document the new configurations. Without proper documentation, users will be unaware of the added feature. Please make sure to update the `external/storm-hbase/README.md` and `docs/storm-hbase.md` files accordingly.
2. The code currently utilizes JDK 8 API, such as `Map.getOrDefault()`, which means it cannot be directly used for the 1.x branch. Can you please create a new pull request specifically for the 1.x branch?
Furthermore, it would be highly appreciated if you could test the code (using Caffeine) on both JRE7 and JRE8. We expect it to not work on JRE7 (which can be documented as a precondition for the JRE version) and to work as expected on JRE8.
cc. @ben-manes, could you please confirm if my expectations are correct?
Thank you in advance!""";documentation_debt
Needs more explanation on what this is used for and how do users use this;documentation_debt
Requires further clarification regarding the purpose and practical utilization of this feature by users.;documentation_debt
Additional elaboration is necessary to outline the intended purpose of this functionality and provide practical insights into its usage by users.;documentation_debt
More detailed information is needed to elucidate the intended use cases of this feature and offer guidance to users on how to effectively incorporate it into their workflows.;documentation_debt
"-->
Fixes #<xyz>
Master Issue: #<xyz>
This mr adds an api to check if the worker is ready to serve requests.
*Describe the modifications you've done.*
This change is a trivial rework / code cleanup without any test coverage.
This change is already covered by existing tests, such as *(please describe tests)*.
This change added tests and can be verified as follows:
  - If a feature is not applicable for documentation, explain why?
  - If a feature is not documented yet in this PR, please create a followup issue for adding the documentation";documentation_debt
"""-->
Addresses #<xyz>
Master Issue: #<xyz>
This merge request introduces an API that allows checking the readiness of the worker for serving requests.
*Summary of Modifications:*
The modifications made in this change involve minor rework and code cleanup, without affecting test coverage.
These changes are already encompassed by existing tests, specifically *(please describe the relevant tests)*.
Furthermore, this update includes additional tests that can be validated using the following steps:
  - If there is a feature that does not require documentation, please provide an explanation.
  - If the feature lacks documentation in this merge request, kindly create a separate follow-up issue for adding the necessary documentation.""";documentation_debt
"""-->
Fixes #<xyz>
Master Issue: #<xyz>
In this merge request, I have introduced an API that allows checking if the worker is ready to handle requests.
*Explanation of Modifications:*
This update primarily involves minor reworking and code cleanup, and it does not introduce any new test coverage.
The existing tests, such as *(please describe tests)*, already cover this change.
Additionally, I have included new tests to validate the implementation. The verification process is as follows:
  - If there are any features that are not applicable for documentation, please provide an explanation as to why.
  - If there are any features that are not yet documented within this merge request, kindly create a separate follow-up issue to address the documentation requirements.""";documentation_debt
"<!--
Resolves #<xyz>
Master Issue: #<xyz>
This merge request introduces an API to determine the readiness of the worker for handling requests.
*Provide a description of the modifications made.*
The changes made in this commit are minor and primarily focused on code rework and cleanup. No new test cases were added as the existing test coverage adequately covers the changes made in this merge request. The modifications can be validated using the following steps:
  - If there is a feature that is not applicable for documentation, please provide an explanation as to why it is not necessary.
  - If any aspect of this feature is not documented in this merge request, please create a separate issue to track the addition of the necessary documentation.";documentation_debt
"-->
Fixes #<xyz>
Master Issue: #<xyz>

This merge request introduces an API that allows checking if the worker is ready to handle requests. 

Here is an overview of the modifications made:
- The changes made in this merge request are primarily focused on reworking and cleaning up the code, with no impact on test coverage.
- The modifications made are already covered by existing tests, specifically *(please provide a description of the relevant tests)*.
- Tests have been added to ensure the functionality of the new feature. The verification process is as follows:
   1. *(Describe the steps to verify the changes and the expected outcomes)*

Regarding documentation:
- If there are features that are not applicable for documentation, please explain the reason behind it.
- If any features included in this merge request are not yet documented, please create a separate issue to track the addition of the documentation.

Note: Please make sure to provide all necessary information and steps to validate the changes and ensure the completeness of the documentation for a comprehensive review.";documentation_debt
Can you share how you derive `yesterday_ds` with `execution_date` with macros in the docs?;documentation_debt
"""Could you please explain the process of deriving yesterday_ds using macros and the execution_date in the documentation?""";documentation_debt
"""I would appreciate it if you could provide an explanation in the documentation on how to calculate yesterday_ds using macros and the execution_date.""";documentation_debt
"""Is it possible to include documentation on the usage of macros and the execution_date to derive yesterday_ds?""";documentation_debt
"""Can you please share the method for obtaining yesterday_ds using macros and the execution_date in the documentation?""";documentation_debt
Enter is required. Otherwise, documentation is not rendered correctly.;documentation_debt
"It is essential to include the ""Enter"" key as it ensures proper rendering of the documentation.";documentation_debt
"The presence of the ""Enter"" key is necessary for correct rendering of the documentation.";documentation_debt
"Including the ""Enter"" key is crucial to ensure the accurate display of the documentation.";documentation_debt
"Proper documentation rendering requires the inclusion of the ""Enter"" key.";documentation_debt
@maektwain just iteratively follow the process described on https://github.com/apache/fineract#pull-requests ... :smiling_imp: (so add that to this PR as well). If this could be written more clearly in the README, then improve it. smiley_cat;documentation_debt
@maektwain, please iteratively follow the process outlined in the pull request instructions available at https://github.com/apache/fineract#pull-requests. Kindly include this information in the present pull request as well. If there is an opportunity to enhance the clarity of these instructions in the README, please proceed with the necessary improvements. 😈;documentation_debt
@maektwain, it is recommended to follow the step-by-step process described in the pull request instructions found at https://github.com/apache/fineract#pull-requests. Kindly include this information in the current pull request. Additionally, if there is room for enhancing the clarity of these instructions in the README, please take the opportunity to make improvements. 😺;documentation_debt
Hey @maektwain, make sure to iteratively follow the instructions provided in the pull request documentation available at https://github.com/apache/fineract#pull-requests. Please include this information in the ongoing pull request. Also, if you notice any room for enhancing the clarity of these instructions in the README, feel free to improve them. 😸;documentation_debt
@maektwain, it's important to follow the step-by-step process outlined in the pull request instructions mentioned on https://github.com/apache/fineract#pull-requests. Kindly include this information in the present pull request as well. Moreover, if there's an opportunity to make the instructions clearer in the README, please take the initiative to enhance them. 😼;documentation_debt
At this point the function is not exported or documented and threads are always used, users would need to set `options(arrow.use_threads)` to turn them off.;documentation_debt
Currently, the function is neither exported nor documented, and threads are consistently utilized. In order to disable threads, users must manually set 'options(arrow.use_threads)'.;documentation_debt
At the moment, the function lacks both exportation and documentation, and it consistently relies on threads. To disable threads, users must set 'options(arrow.use_threads)' manually.;documentation_debt
Presently, the function is not exported or documented, and it invariably employs threads. To deactivate threads, users need to manually configure 'options(arrow.use_threads)'.;documentation_debt
As it stands, the function is not exported or properly documented, and it consistently relies on the usage of threads. To disable the threading functionality, users are required to manually adjust the 'options(arrow.use_threads)' setting.;documentation_debt
Are these comments necessary?;documentation_debt
Is it necessary to include these comments?;documentation_debt
Do these comments serve a necessary purpose?;documentation_debt
Are these comments required or essential?;documentation_debt
Do these comments have a necessary role?;documentation_debt
"Rep @onurkaraman : `removeGroup` should always be guarded by the group lock inside the `GroupCoordinator`, while `getGroup` and `addGroup` are not since the group object is not available yet. I will make that more clear in the comments.
Rep @junrao : ack.";documentation_debt
"Regarding @onurkaraman's comment, it is important to ensure that removeGroup is always protected by the group lock within the GroupCoordinator. However, getGroup and addGroup do not require such protection as the group object is not yet available. I will update the comments to clarify this distinction.

@junrao, your acknowledgment is noted.";documentation_debt
"In response to @onurkaraman's feedback, it is crucial to ensure that removeGroup is consistently guarded by the group lock within the GroupCoordinator. However, the same level of protection is not necessary for getGroup and addGroup since the group object is not yet accessible. I will enhance the comments to provide a clearer explanation of this distinction.
@junrao, thank you for acknowledging the message.";documentation_debt
"Regarding @onurkaraman's input, it is essential to always protect removeGroup with the group lock inside the GroupCoordinator. On the other hand, getGroup and addGroup do not require such protection as the group object is not available at that stage. I will update the comments to provide a clearer explanation of this distinction.
Acknowledged, @junrao.";documentation_debt
"@onurkaraman rightly points out that removeGroup should always be safeguarded by the group lock within the GroupCoordinator. However, the same level of protection is not necessary for getGroup and addGroup since the group object is not yet accessible. I will make the necessary comments clearer to reflect this distinction.
@junrao, thank you for your acknowledgment.";documentation_debt
Could you add a short comment for this field to explain why we need this? The same to `CatalogManagerCalciteSchema`, `CatalogSchemaTable`, `DatabaseCalciteSchema`. For example:;documentation_debt
Would it be possible to include a brief comment for each of the fields (CatalogManagerCalciteSchema, CatalogSchemaTable, DatabaseCalciteSchema) to provide an explanation of their necessity? For instance:;documentation_debt
Could you kindly add a concise comment to elucidate the purpose behind this field? The same request applies to CatalogManagerCalciteSchema, CatalogSchemaTable, and DatabaseCalciteSchema.;documentation_debt
Would it be possible to include a short comment that clarifies the rationale for the existence of this field? The same comment is requested for CatalogManagerCalciteSchema, CatalogSchemaTable, and DatabaseCalciteSchema.;documentation_debt
Can we have a brief comment added to provide an explanation for this field? Similarly, please include comments for CatalogManagerCalciteSchema, CatalogSchemaTable, and DatabaseCalciteSchema.;documentation_debt
"1.  Input first,
2. Input output next.
3. Output variables.";documentation_debt
"First, provide the input.
Next, provide the input and output.
Specify the output variables.";documentation_debt
"Begin by entering the input.
Proceed with input and output values.
Enumerate the output variables.";documentation_debt
"The initial step is to input the required data.
Following that, input both the input and output values.
Outline the variables that represent the output.";documentation_debt
"Begin by providing the input.
Move on to the input-output.
Lastly, address the output variables.";documentation_debt
 typo: leastIndexs->leastIndexes;documentation_debt
typo: leastIndexs should be corrected to leastIndexes.;documentation_debt
"Please make the correction: change ""leastIndexs"" to ""leastIndexes.""";documentation_debt
"It seems there is a typo ""leastIndexs"" should be ""leastIndexes.""";documentation_debt
"Kindly note the typo: ""leastIndexs"" should actually be ""leastIndexes.""";documentation_debt
For fixing examples and documentation, I will do it separately.;documentation_debt
I will address the examples and documentation fixes separately.;documentation_debt
I will handle the task of fixing the examples and documentation separately.;documentation_debt
To ensure efficiency, I'll be working on the examples and documentation fixes as separate tasks.;documentation_debt
I have decided to tackle the fixes for the examples and documentation in separate efforts.;documentation_debt
please add some javadoc;documentation_debt
Could you kindly include Javadoc documentation for this code?;documentation_debt
Would you please add Javadoc documentation for this code?;documentation_debt
It would be greatly appreciated if you could include Javadoc for this code.;documentation_debt
Could you take the time to add Javadoc documentation to this code?;documentation_debt
"Dear Airflow maintainers,
    - https://issues.apache.org/jira/browse/AIRFLOW-1848
Dataflow Python operator takes in a filename without `.py` extension, which was incorrectly documented previously.
N/A, just a doc change.
    2. Subject is limited to 50 characters
    3. Subject does not end with a period
    4. Subject uses the imperative mood (""add"", not ""adding"")
    5. Body wraps at 72 characters
    6. Body explains ""what"" and ""why"", not ""how""";documentation_debt
"Dear Airflow maintainers,

I would like to bring your attention to the following issue: AIRFLOW-1848. The Dataflow Python operator currently expects a filename without the .py extension, despite the incorrect documentation stating otherwise. This change is purely related to documentation.

To ensure adherence to best practices for commit messages, please consider the following guidelines:

Limit the subject to 50 characters.
Avoid ending the subject with a period.
Use the imperative mood in the subject (e.g., ""add"" instead of ""adding"").
Wrap the body at 72 characters.
The body should explain ""what"" and ""why,"" rather than focusing on ""how.""
Thank you for your attention to this matter.";documentation_debt
"Dear Airflow maintainers,
I wanted to bring your attention to an issue that requires your consideration: AIRFLOW-1848. The Dataflow Python operator currently expects a filename without the .py extension, which is contrary to the previous documentation. This change is solely related to the documentation and does not affect the functionality itself.

In order to adhere to the best practices for commit messages, please take note of the following guidelines:

Limit the subject line to 50 characters.
Avoid using a period at the end of the subject line.
Use the imperative mood in the subject line (e.g., ""add"" instead of ""adding"").
Keep the body wrapped at 72 characters.
Focus on explaining ""what"" and ""why"" in the body, rather than delving into the specifics of ""how.""
Thank you for your attention to this matter.";documentation_debt
"Dear Airflow maintainers,
I would like to draw your attention to AIRFLOW-1848, an issue that needs to be addressed. The current documentation incorrectly states that the Dataflow Python operator expects a filename with the .py extension, while in reality, it should not include the extension. This change only affects the documentation.

To ensure adherence to commit message best practices, please keep in mind the following guidelines:

Limit the subject line to 50 characters.
Avoid using a period at the end of the subject line.
Use the imperative mood in the subject line (e.g., ""add"" instead of ""adding"").
Wrap the body text at 72 characters.
Focus on explaining the ""what"" and ""why"" aspects in the body, rather than delving into the details of ""how.""
Thank you for your attention to this matter.";documentation_debt
"Dear Airflow maintainers,
I wanted to bring your attention to an issue that requires your consideration: AIRFLOW-1848. The Dataflow Python operator currently expects a filename without the .py extension, which is contrary to what was documented previously. This change only applies to the documentation and does not impact the operator's functionality.

To ensure adherence to best practices for commit messages, please consider the following guidelines:

Limit the subject line to 50 characters.
Avoid ending the subject line with a period.
Use the imperative mood in the subject line (e.g., ""add"" instead of ""adding"").
Wrap the body text at 72 characters.
Focus on explaining ""what"" and ""why"" in the body, rather than the specifics of ""how.""
Thank you for your attention to this matter.";documentation_debt
"Thanks for keep working on this.
Thanks so what I think is really taking people with some surprise is that it registers Camel routes into the **same single camel context** from any bundles. The point of Apache Karaf was to be like an app server where each bundle is isolated. So this goes against this practice. This should be documented much much more clearly. And also it lacks features with the ease of use how to configure camel context itself (you end up with its defaults) and how would people do dependency injections for beans etc. 
Also it should be renamed to `camel-osgi-activator`, and moved to components (as its not a core piece, eg not used by other osgi like osgi blueprint which is the main osgi support in Camel).";documentation_debt
"Thank you for your continued efforts in working on this.

It seems that what is surprising many people is that Camel routes are registered into the same single Camel context from any bundles. This goes against the typical practice in Apache Karaf, where each bundle is meant to be isolated. It is crucial to document this behavior in a much clearer manner. Additionally, there are concerns regarding the lack of features that make it easier to configure the Camel context itself (resulting in the use of default settings) and the absence of guidance on dependency injections for beans.

Moreover, it is suggested that the component be renamed to camel-osgi-activator and moved to the components section, as it is not considered a core component and is not used by other OSGi implementations like OSGi Blueprint, which is the primary OSGi support in Camel.";documentation_debt
"Your dedication to this matter is greatly appreciated.
It appears that the unexpected aspect catching many people off guard is the registration of Camel routes into the same single Camel context from any bundles. This deviates from the conventional approach in Apache Karaf, where each bundle is typically isolated. It is imperative to provide much clearer documentation regarding this behavior. Additionally, there are concerns about the lack of convenient features for configuring the Camel context itself, resulting in a reliance on default settings, as well as a lack of guidance on dependency injection for beans.

Furthermore, it is suggested that the component be renamed to camel-osgi-activator and be relocated to the components section, considering it is not a core component and is not utilized by other OSGi frameworks like OSGi Blueprint, which serves as the primary OSGi support in Camel.";documentation_debt
"Thank you for your ongoing work on this.
One aspect that seems to be surprising many users is the registration of Camel routes into the same single Camel context from any bundles. This approach contradicts the usual practice in Apache Karaf, where each bundle is intended to be isolated. It is crucial to provide much clearer documentation that clearly explains this behavior. Furthermore, there are concerns regarding the lack of features that facilitate easy configuration of the Camel context itself, resulting in a reliance on default settings, as well as a lack of guidance on dependency injection for beans.

Additionally, it is suggested that the component be renamed to camel-osgi-activator and be moved to the components section, as it is not considered a core component and is not utilized by other OSGi frameworks like OSGi Blueprint, which serves as the primary OSGi support in Camel.";documentation_debt
"Your continuous efforts on this matter are appreciated.
The aspect that seems to be catching many people by surprise is the registration of Camel routes into the same single Camel context from any bundles. This approach goes against the typical practice in Apache Karaf, where each bundle is meant to be isolated. To avoid confusion, it is crucial to provide much clearer documentation that explicitly explains this behavior. Moreover, there are concerns about the lack of features that facilitate easy configuration of the Camel context itself, resulting in a reliance on default settings, as well as a lack of guidance on dependency injection for beans.

Furthermore, it is recommended to rename the component to camel-osgi-activator and relocate it to the components section, as it is not a core component and is not used by other OSGi frameworks like OSGi Blueprint, which serves as the primary OSGi support in Camel.";documentation_debt
just to circle back here - I asked @benstopford to remove the test and doc and do it in a new PR, since they are a bit out of context here.;documentation_debt
I understand. I have informed @benstopford to remove the test and documentation related to the previous context and handle them in a separate pull request to maintain better context and organization.;documentation_debt
I have communicated with @benstopford regarding your request to remove the test and documentation that are out of context in this pull request. They will handle them separately to ensure better context and organization.;documentation_debt
I have informed @benstopford about your suggestion to remove the test and documentation, which are not relevant to the current context. They will create a new pull request to address them separately for improved organization and clarity.;documentation_debt
I have shared your request with @benstopford to remove the test and documentation that are out of context in this pull request. They will handle them separately in a new pull request to maintain better clarity and cohesion.;documentation_debt
A few failing tests, and a couple of (very minor) comments, but other than that, looks good (once tests are passing again :-));documentation_debt
There are currently a few failing tests, and I have also addressed a couple of minor comments. Once the tests are passing again, everything should be in good shape.;documentation_debt
Currently, there are a few tests that are failing, and I have also taken care of a couple of minor comments. Once the tests are passing again, everything should be in good order.;documentation_debt
At the moment, there are a few failing tests, and I have attended to a few minor comments as well. Once the tests are passing once more, everything should be in good condition.;documentation_debt
There are a few tests that are currently failing, and I have made adjustments based on a couple of minor comments. As soon as the tests are passing again, everything should be in good standing.;documentation_debt
Ok I added back the other test but improved the commenting there.;documentation_debt
Okay, I have added the other test back while also improving the commenting in that section.;documentation_debt
I have reintroduced the other test that was previously removed, and I have made improvements to the commenting in that specific section.;documentation_debt
The other test has been reinstated, and I have taken the opportunity to enhance the commenting in that particular area.;documentation_debt
I have included the previously omitted test and made significant improvements to the commenting in that section.;documentation_debt
Doc formatting.;documentation_debt
"""Please fix the formatting in the documentation.""";documentation_debt
"""Please update the formatting of the documentation.""";documentation_debt
"""Please improve the formatting of the documentation.""";documentation_debt
"""The documentation formatting needs to be adjusted to improve its presentation and readability.""";documentation_debt
"""Environment"" was misspelled on line 348, I have corrected this typo.";documentation_debt
"The typo in which ""Environment"" was misspelled on line 348 has been corrected.";documentation_debt
"The misspelling of ""Environment"" on line 348 has been rectified.";documentation_debt
"I have fixed the typo on line 348, where ""Environment"" was misspelled.";documentation_debt
"The typo on line 348, involving the misspelling of ""Environment,"" has been addressed and corrected.";documentation_debt
"``
// below might change the type of the call (e.g. from nullable to non-nullable)
// however simplify(..) is allowed to return node with different type
// if the type should match, then `simplifyPreservingType` should be used";documentation_debt
"// The code below has the potential to change the type of the call, for example, from nullable to non-nullable.
// However, it's important to note that the `simplify(..)` method is permitted to return a node with a different type.
// If it's necessary for the type to match, then the `simplifyPreservingType` method should be used instead.";documentation_debt
"// The code below may alter the type of the call, potentially changing it from nullable to non-nullable.
// However, it's worth mentioning that the simplify(..) method is designed to allow returning a node with a different type.
// If preserving the original type is required, it's recommended to use the simplifyPreservingType method instead.";documentation_debt
"
2. ```
// The following code has the ability to modify the type of the call, which could result in a change from nullable to non-nullable.
// However, it's important to understand that the `simplify(..)` method has the flexibility to return a node with a different type.
// If maintaining type consistency is necessary, it is advised to utilize the `simplifyPreservingType` method.";documentation_debt
"// The code snippet below can potentially alter the type of the call, such as transitioning from nullable to non-nullable.
// However, it's crucial to note that the simplify(..) method is designed to permit returning a node with a different type.
// If it is required to preserve the type, the recommended approach is to use the simplifyPreservingType method instead.";documentation_debt
"
4. ```
// The code section below has the potential to change the type of the call, for instance, converting it from nullable to non-nullable.
// However, it is important to acknowledge that the `simplify(..)` method allows returning a node with a different type.
// If the type needs to remain consistent, it is advisable to use the `simplifyPreservingType` method instead.";documentation_debt
"In general, I would not use the ""kleene"" term, as we do not use it throughout the rest of the docs.
You can say sth like ""looping"" or simply oneOrMore.
In addition, it would also be more consistent to not use ""operator"" but ""pattern"". In the docs we have ""pattern sequences"" composed of ""patterns"" so we should stick to that.
So ""Kleene Operator"" -> ""looping pattern"" or ""oneOrMore pattern""";documentation_debt
"Generally, I would recommend avoiding the use of the term ""kleene"" as it is not used elsewhere in the documentation. Instead, you can consider using terms such as ""looping"" or simply ""oneOrMore"" to describe the concept. Additionally, to maintain consistency, it would be more appropriate to refer to it as a ""pattern"" rather than an ""operator."" In the documentation, we refer to ""pattern sequences"" composed of ""patterns,"" so it would be best to adhere to that terminology. Therefore, ""Kleene Operator"" can be revised to ""looping pattern"" or ""oneOrMore pattern.""";documentation_debt
"It is advisable to avoid using the term ""kleene"" as it is not used consistently throughout the rest of the documentation. Instead, you can use alternatives like ""looping"" or simply ""oneOrMore"" to describe the concept. Additionally, to maintain consistency, it would be better to refer to it as a ""pattern"" rather than an ""operator"" since we use ""pattern sequences"" composed of ""patterns"" in the documentation. Thus, ""Kleene Operator"" can be replaced with ""looping pattern"" or ""oneOrMore pattern.""";documentation_debt
"It would be more consistent to refrain from using the term ""kleene"" since it is not utilized elsewhere in the documentation. Instead, consider using terms such as ""looping"" or simply ""oneOrMore"" to describe the concept. Additionally, to maintain consistency, it would be preferable to refer to it as a ""pattern"" rather than an ""operator."" In the documentation, we refer to ""pattern sequences"" composed of ""patterns,"" so it would be more appropriate to use ""looping pattern"" or ""oneOrMore pattern"" instead of ""Kleene Operator.""";documentation_debt
"I suggest avoiding the term ""kleene"" since it is not used consistently in the rest of the documentation. It would be better to use terms like ""looping"" or simply ""oneOrMore"" to describe the concept. Additionally, to ensure consistency, it is recommended to refer to it as a ""pattern"" rather than an ""operator."" Considering that the documentation mentions ""pattern sequences"" composed of ""patterns,"" it would be more appropriate to replace ""Kleene Operator"" with ""looping pattern"" or ""oneOrMore pattern.""";documentation_debt
Fixes spelling in src;documentation_debt
I have fixed the spelling in the source code.;documentation_debt
The spelling has been corrected in the source code.;documentation_debt
Spelling errors in the source code have been fixed.;documentation_debt
I have made the necessary spelling corrections in the source code.;documentation_debt
Thanks for the review @anmolnar. Please take a look at unit tests when you get a chance. I have addressed the comments. I will also add documentation in a separate commit.;documentation_debt
Thank you for the update, @anmolnar. I will review the unit tests as soon as possible to ensure that the addressed comments have been appropriately handled. Additionally, I look forward to reviewing the documentation in the separate commit.;documentation_debt
Thank you, @anmolnar, for the review. Whenever you have the opportunity, please take a look at the unit tests. I have made the necessary adjustments based on the comments. Furthermore, I will be adding the documentation in a separate commit.;documentation_debt
Much appreciated, @anmolnar, for your review. Kindly review the unit tests when you have a chance. I have addressed the comments accordingly. I will also commit the documentation separately.;documentation_debt
Thank you, @anmolnar, for your feedback. When you find the time, please review the unit tests. I have taken care of the mentioned comments. Additionally, I will include the documentation in a separate commit.;documentation_debt
@abhiy13 short javadoc please.;documentation_debt
Could you please provide a brief Javadoc for this method, @abhiy13?;documentation_debt
@abhiy13, would you mind providing a brief Javadoc for this method?;documentation_debt
@abhiy13, could you please offer a concise Javadoc for this method?;documentation_debt
@abhiy13, it would be appreciated if you could provide a short Javadoc for this method.;documentation_debt
I can answer that myself, the countdown stuff is useful to figure out if the computation has completed. Please add some doc here.;documentation_debt
I can answer that myself. The countdown functionality is useful for determining whether the computation has completed. It would be beneficial to add some documentation here to provide an explanation.;documentation_debt
I can provide an answer to that. The countdown mechanism serves the purpose of tracking the completion of the computation. It is advisable to include documentation at this point to offer an explanation.;documentation_debt
I am able to answer that question myself. The countdown feature is valuable for monitoring the progress of the computation. It is recommended to provide documentation at this stage to clarify its usage.;documentation_debt
I can provide the answer to that inquiry. The countdown functionality is essential in determining the completion of the computation. It would be helpful to add documentation here to provide a clear explanation.;documentation_debt
I fix one spelling mistake;documentation_debt
I fixed one spelling mistake.;documentation_debt
I have corrected a spelling error.;documentation_debt
One spelling mistake has been rectified.;documentation_debt
I noticed and corrected a spelling error.;documentation_debt
Worth having some javadoc (even though the method is small, it's behaviour is non-obvious). It wasn't clear until looking carefully why would increment the iterator sometimes (i.e. call `next()`) and not other times.;documentation_debt
It would be valuable to include some Javadoc for this method, even though it is small. The behavior of the method may not be obvious at first glance, so providing documentation will help clarify its purpose. Upon closer examination, it may not be immediately clear why the iterator is incremented at times (i.e., why `next()` is called) and not at other times.;documentation_debt
It is recommended to provide Javadoc comments for this method, despite its small size, as its behavior may not be immediately apparent. Without clear documentation, it can be confusing to understand the reasons behind incrementing the iterator in certain cases (such as when next() is called) and not in others.;documentation_debt
Adding Javadoc comments would be beneficial for this method, even though it is relatively small, as its behavior may not be self-evident. Without proper documentation, it can be unclear why the iterator is incremented at certain times (such as when next() is invoked) and not at other times.;documentation_debt
It is worth considering adding Javadoc comments for this method, even though it is small in size, due to its non-obvious behavior. Without adequate documentation, it may be difficult to understand the rationale behind occasionally incrementing the iterator (via the next() call) and not doing so in other cases.;documentation_debt
Remove the commented out code.;documentation_debt
Please remove the commented out code.;documentation_debt
Kindly delete the code that is currently commented out.;documentation_debt
Please remove the commented out code from the file.;documentation_debt
It would be advisable to eliminate the code that is currently commented out.;documentation_debt
"nit: Capital ""A"" in ""Authorization"" please since it's a title. 
nit: There's a bit more formatting to do in the text like enclosing class names in backticks.
I think it would be worth adding some note here to providers to say that while Gremlin Server supports this authorization feature it is not a feature that TinkerPop requires of graph providers as part of the agreement between client and server. Graph providers may choose to implement their own methods for authorization in the manner they see fit.  I would say a similar ""IMPORTANT"" callout box should probably be added to the reference documentation to alert users to this notion. Finally, as you draw closer to a final body of work, this is a neat new feature that should have upgrade documentation. (and perhaps more user facing documentation?))
UPDATE: I read a bit further on and saw you linked from the user documentation to this page....that could suffice, but if I'm thinking of this feature right I sense that users will write these authorizors and i think it could be a popular feature which means more front facing documentation.";documentation_debt
"Nit: Please capitalize the ""A"" in ""Authorization"" since it is a title.

Nit: There is some additional formatting that needs to be done in the text, such as enclosing class names in backticks.

I believe it would be valuable to add a note here for providers, stating that while Gremlin Server supports this authorization feature, it is not a requirement imposed by TinkerPop for graph providers as part of the client-server agreement. Graph providers may choose to implement their own methods of authorization as they see fit. It might be helpful to include a similar ""IMPORTANT"" callout box in the reference documentation to alert users to this concept. Additionally, as you approach the final version of the documentation, it would be beneficial to include upgrade documentation for this new feature. It might also be worth considering expanding the user-facing documentation, as this feature could potentially be popular and users may be interested in writing their own authorizers. I noticed that you linked from the user documentation to this page, which could suffice, but I have a sense that users will want more detailed information, making additional front-facing documentation beneficial.
";documentation_debt
"Nit: Please capitalize ""A"" in ""Authorization"" since it is a title.

Nit: There is some additional formatting to be done in the text, such as enclosing class names in backticks.

I believe it would be beneficial to add a note to providers, stating that while Gremlin Server supports this authorization feature, it is not mandatory for graph providers to implement it as part of the client-server agreement set by TinkerPop. Graph providers have the flexibility to implement their own methods of authorization as they see fit. It may be helpful to include an ""IMPORTANT"" callout box in the reference documentation to ensure users are aware of this. Additionally, as you approach the final version of the documentation, it would be valuable to provide upgrade documentation for this new feature. Furthermore, considering the potential popularity of this feature and the likelihood of users writing their own authorizers, it might be worthwhile to create more user-facing documentation or expand on the existing user documentation by providing comprehensive coverage of this feature.

Update: I noticed that you have already linked from the user documentation to this page, which could suffice. However, if my understanding of this feature is correct and users will indeed be writing their own authorizers, I believe it could become a widely used feature, warranting more extensive front-facing documentation.";documentation_debt
"Nit: Please capitalize ""A"" in ""Authorization"" since it is a title.

Nit: There is some additional formatting to be done in the text, such as enclosing class names in backticks.

I believe it would be valuable to include a note here for providers, clarifying that while Gremlin Server supports this authorization feature, it is not a requirement imposed by TinkerPop for graph providers in the client-server agreement. Graph providers are free to implement their own methods of authorization as they deem appropriate. It may be helpful to add a similar ""IMPORTANT"" callout box in the reference documentation to alert users to this concept. Additionally, as the work nears completion, it would be beneficial to include upgrade documentation for this new feature. Furthermore, considering the potential popularity of this feature and the likelihood of users writing their own authorizers, it might be necessary to provide more user-facing documentation. Although I noticed that there is a link from the user documentation to this page, it could be sufficient however, it is important to consider providing more comprehensive front-facing documentation for this feature.

UPDATE: Upon further reading, I noticed that you have linked from the user documentation to this page, which might be adequate. However, considering the nature of this feature and the possibility that users will be writing their own authorizers, it could be a popular feature, necessitating more extensive front-facing documentation.";documentation_debt
"Nit: Please capitalize the ""A"" in ""Authorization"" since it is a title.

Nit: There is some additional formatting to be done in the text, such as enclosing class names in backticks.

I believe it would be beneficial to include a note for providers, stating that while Gremlin Server supports the authorization feature, it is not mandatory for graph providers to implement it as part of the client-server agreement. Graph providers have the flexibility to implement their own methods for authorization as they see fit. It might be helpful to add an ""IMPORTANT"" callout box to the reference documentation to ensure users are aware of this aspect. Additionally, as you approach the finalization of the work, it would be valuable to prepare upgrade documentation for this new feature, and perhaps consider developing more user-facing documentation if necessary.

Update: I read further and noticed that you have linked this page from the user documentation, which may suffice. However, if my understanding of this feature is correct, users will likely be writing their own authorizers, and I believe this feature could become quite popular. Hence, it would be advantageous to provide more front-facing documentation to support users in utilizing this feature effectively.";documentation_debt
To check whether a file is zip-format we must read its actual content. If user specifies a DFS url the check will introduce additional IO. Suffix checking is also hard because there are too many file formats are actually zip format. Maybe rewriting the doc string in detail to tell users what file format are actually supported is a better choice?;documentation_debt
In order to determine if a file is in zip format, it is necessary to read its actual content. However, when a user specifies a Distributed File System (DFS) URL, this check introduces additional input/output operations. Additionally, relying solely on suffix checking is challenging due to the fact that numerous file formats are actually zip formats. Considering these challenges, perhaps a better approach would be to provide users with a detailed docstring that clearly states which file formats are supported. This would offer users more precise information regarding the supported file formats.;documentation_debt
To verify if a file is in zip format, it is essential to read its content directly. However, when a user specifies a Distributed File System (DFS) URL, this verification process introduces additional input/output operations. Moreover, relying solely on suffix checking becomes problematic due to the existence of various file formats that can be considered as zip formats. In light of these challenges, it might be more effective to enhance the docstring by providing detailed information on the specific file formats supported.;documentation_debt
In order to determine if a file is in zip format, it is necessary to read the actual content. However, if a user specifies a Distributed File System (DFS) URL, performing this check will introduce additional input/output operations. Moreover, relying solely on suffix checking is difficult due to the presence of numerous file formats that can be categorized as zip formats. Considering these complexities, it may be more beneficial to revise the docstring by providing a comprehensive explanation of the supported file formats.;documentation_debt
The process of determining whether a file is in zip format requires reading its content directly. However, when a user specifies a Distributed File System (DFS) URL, this check introduces additional input/output operations. Furthermore, relying solely on suffix checking poses challenges due to the abundance of file formats that can be considered as zip formats. To address these concerns, it might be more advantageous to provide users with a detailed docstring that explicitly outlines the supported file formats.;documentation_debt
"Typo.
Also, why do you ever need to set the task version after-the-fact?";documentation_debt
"There appears to be a typo.

Additionally, could you please clarify the reason behind the need to set the task version after it has already occurred?";documentation_debt
"It seems there is a typo.
Furthermore, could you provide an explanation for why it is necessary to set the task version after it has already been completed?";documentation_debt
"There is a typo in the text.
Moreover, could you shed light on the rationale behind requiring the task version to be set after it has already taken place?";documentation_debt
"It appears that there is a mistake in the text.
Moreover, I would appreciate it if you could clarify the purpose or necessity of setting the task version after it has occurred.";documentation_debt
"could you also update this doc with the new API? https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md
It could serve as a tutorial for users to use it";documentation_debt
Would you mind updating the documentation with the new API as well? You can find the document at https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md. This update would greatly benefit users by providing a tutorial on how to utilize the new API.;documentation_debt
Could you please include the new API in the documentation located at https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md? This update would serve as a valuable tutorial for users to effectively utilize the new API.;documentation_debt
It would be greatly appreciated if you could update the documentation at https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md with the new API. This updated document can then serve as a tutorial to guide users in utilizing the new API.;documentation_debt
Is it possible for you to update the documentation at https://github.com/apache/incubator-parquet-mr/blob/master/parquet_cascading.md with the details of the new API? By doing so, users will have access to a comprehensive tutorial on how to leverage the new API effectively.;documentation_debt
[MINOR] Fix Typos;documentation_debt
This is a minor fix to address some typos.;documentation_debt
Minor correction made to fix typos.;documentation_debt
Typos have been corrected in this minor update.;documentation_debt
This is a small fix to address typos.;documentation_debt
Docs improved: more details about caching and memory for segments on historicals;documentation_debt
"""Enhanced documentation to provide in-depth information about caching and memory utilization for segments on historical nodes.""";documentation_debt
"""Expanded documentation with additional details concerning caching and memory management for segments on historical nodes.""";documentation_debt
"""Improved documentation by including comprehensive explanations about caching and memory considerations for segments on historical nodes.""";documentation_debt
"""Updated documentation to offer a more detailed understanding of caching and memory usage for segments on historical nodes.""";documentation_debt
"fix some typo errors to make the context consistent:
some are ""streamOfWords"" but some are ""dataStreamOfWords""";documentation_debt
"""Corrected several typographical errors to ensure consistency in the context. Some instances have been updated from 'streamOfWords' to 'dataStreamOfWords'.""";documentation_debt
"""Fixed various typos to maintain consistency in the context. Specifically, 'streamOfWords' has been replaced with 'dataStreamOfWords' where appropriate.""";documentation_debt
"""Addressed multiple typographical errors to establish context consistency. Notably, 'streamOfWords' has been rectified to 'dataStreamOfWords' for coherence.""";documentation_debt
"""To maintain consistency in the context, a number of typographical errors have been resolved. Specifically, instances of 'streamOfWords' have been amended to 'dataStreamOfWords' as required.""";documentation_debt
please keep this javadoc to be just about the preCombine() method, without any context from this PRs scenarios.;documentation_debt
"""Please ensure that the Javadoc for the preCombine() method only provides information specific to the method itself, without including any context from the scenarios mentioned in this pull request.""";documentation_debt
"""Please ensure that the Javadoc for the preCombine() method focuses solely on its functionality and does not include any specific context from this pull request's scenarios.""";documentation_debt
"""Please retain the Javadoc for the preCombine() method, focusing solely on its purpose and functionality, without including any specific context from this pull request's scenarios.""";documentation_debt
"""Please ensure that the Javadoc for the preCombine() method focuses solely on its functionality and purpose, without including any specific context related to the scenarios addressed in this pull request. The Javadoc should provide clear and concise information about the method's behavior and usage.""";documentation_debt
"This deserves a comment so someone doesn't ""simplify"" it back into the old code.";documentation_debt
"""It is important to add a comment here to ensure that someone doesn't mistakenly ""simplify"" the code back to its previous version.""";documentation_debt
"""We should add a comment here to prevent someone from mistakenly simplifying the code back to its previous version.""";documentation_debt
"""It is important to add a comment here to prevent someone from mistakenly simplifying the code back to its previous version.""";documentation_debt
"""It is important to add a comment here to prevent someone from mistakenly simplifying the code back to its previous form.""";documentation_debt
comment is out of date now;documentation_debt
The comment is no longer up to date given the recent changes.;documentation_debt
"""Since the comment is no longer relevant due to recent updates, I propose removing it from the codebase. The comment has become outdated and no longer provides accurate information. Please review the changes and let me know if you agree with this update.""";documentation_debt
"""The comment in question is no longer up to date and needs to be updated accordingly.""";documentation_debt
"""Note: The previously provided comment is now outdated and no longer applicable in the current context. Please disregard it.""";documentation_debt
The comment above doesn't seem to fit the case, perhaps the result of a refactor somewhere along the way. Shall we move it below to the actual `Dead` case?;documentation_debt
"""Considering that the comment above doesn't seem to be relevant in this particular case, it might be appropriate to move it below to the actual Dead case. This adjustment would better align the comment with the corresponding code. Please review and consider this proposal.""";documentation_debt
"""Considering that the provided comment seems to be out of place and might have resulted from a refactor at some point, it would be more appropriate to move it below to the actual Dead case.""";documentation_debt
"""Considering the refactor that has taken place, it seems that the comment above is no longer aligned with the current scenario. To address this, it would be appropriate to move the comment below to the actual Dead case.""";documentation_debt
"""The above comment doesn't seem to be relevant in this context, possibly due to a refactor that occurred at some point. It would be more appropriate to move this comment below to the actual Dead case.""";documentation_debt
"The PR applies the website theme to each version. The navigation will be the same, so the option for Clojure needs to be handled properly for old versions. For this I use the .htaccess file to redirect users to an API error page. For good measure, I also added a custom 404 error page.
This PR stacks on #12413 (has the same changes in build_all_version.sh), plus a change to copy the theme, and fixes my concerns there with the theme.
http://34.201.8.176/
You can test the redirect if you switch to an old version like 1.0.0 and go to API --> Clojure
You can look at the 404:
http://34.201.8.176/error/404.html 
I'm sure there's probably some fancy regex that would collapse the clojure rules to one line, but I'll let someone else get fancy.";documentation_debt
"The pull request applies the website theme to each version, ensuring consistent navigation. To handle the option for Clojure in older versions, an appropriate solution is implemented using the .htaccess file to redirect users to an API error page. Additionally, a custom 404 error page is added for improved user experience.

This pull request builds upon #12413, which includes changes in the `build_all_version.sh` file. It also addresses concerns regarding the theme by copying it appropriately.

You can test the redirect functionality by switching to an old version like 1.0.0 and navigating to API --> Clojure. To view the custom 404 error page, you can visit http://34.201.8.176/error/404.html.

Please note that there might be more efficient ways, such as using advanced regex, to condense the Clojure rules into a single line. However, for now, the current implementation serves the purpose effectively.";documentation_debt
"This pull request applies the website theme to each version and ensures consistent navigation. Specifically, it addresses the proper handling of the Clojure option for older versions by utilizing the .htaccess file to redirect users to an API error page. Additionally, a custom 404 error page has been added for improved user experience.

This pull request builds upon #12413, which includes the same changes in the `build_all_version.sh` script. It also incorporates a change to copy the theme and resolves any concerns regarding the theme in the previous PR.

To test the redirect functionality, you can switch to an old version like 1.0.0 and navigate to API -> Clojure. You can also explore the custom 404 error page by visiting the following URL: [http://34.201.8.176/error/404.html](http://34.201.8.176/error/404.html).

Please note that while it is possible to use fancy regex to collapse the Clojure rules into a single line, this implementation has been left for someone else to handle.

You can access the current version of the website at [http://34.201.8.176/](http://34.201.8.176/).";documentation_debt
"The pull request applies the website theme to each version, ensuring consistent navigation. Specifically, it handles the option for Clojure appropriately in older versions by utilizing the .htaccess file to redirect users to an API error page. Additionally, a custom 404 error page has been added for improved user experience.

This pull request is built upon #12413, which includes similar changes in build_all_version.sh. It also addresses concerns related to the theme and fixes them accordingly.

To test the redirect functionality, you can switch to an old version like 1.0.0 and navigate to API --> Clojure. You can also explore the 404 error page by visiting http://34.201.8.176/error/404.html.

Note that while there might be more advanced regex techniques to consolidate the Clojure rules into a single line, that complexity has been left for someone else to handle.

Please visit http://34.201.8.176/ to review the changes.";documentation_debt
The purpose of this pull request is to apply the website theme to each version while maintaining consistent navigation. In older versions, the option for Clojure is handled appropriately by utilizing the .htaccess file to redirect users to an API error page. Additionally, a custom 404 error page has been implemented for better user experience. This pull request builds upon #12413, addressing the same changes in build_all_version.sh and resolving concerns related to the theme. To test the redirect functionality, you can switch to an old version like 1.0.0 and navigate to API --> Clojure. You can also view the custom 404 error page at http://34.201.8.176/error/404.html. Please review the changes at http://34.201.8.176/.;documentation_debt
KAFKA-9068: Fix incorrect JavaDocs for `Stores.xxxSessionStore(...)`;documentation_debt
"""KAFKA-9068: Correct JavaDocs for Stores.xxxSessionStore(...)""";documentation_debt
"""KAFKA-9068: Update JavaDocs for Stores.xxxSessionStore(...)""";documentation_debt
"""KAFKA-9068: Fix inaccurate JavaDocs for Stores.xxxSessionStore(...)""";documentation_debt
"""KAFKA-9068: Rectify incorrect JavaDocs for Stores.xxxSessionStore(...)""";documentation_debt
Could we put those comments in a more prominent place like the beginning of the class? With v1 message format, we are adding a timestamp, a timestamp type attribute, and are using a relative for inner message. It would be useful to document the format in a bit more details for both the outer and the inner message. For example, should the timestamp type attribute be set for inner messages?;documentation_debt
"/**
 * Class XYZProcessor.
 * 
 * Description: This class processes XYZ messages using the v1 message format.
 * The v1 format includes a timestamp, a timestamp type attribute, and a relative for inner message.
 * It is important to note the format details for both the outer and inner messages.
 * 
 * Modifications:
 * - Moved comments to a more prominent location at the beginning of the class.
 * - Added details about the message format, including the presence of a timestamp type attribute for inner messages.
 * 
 * @author YourName
 */
public class XYZProcessor {
   // Class implementation
}";documentation_debt
"
2. ```java
/**
 * Class XYZProcessor.
 *
 * Description: This class is responsible for processing XYZ messages using the v1 message format.
 * The v1 format encompasses a timestamp, a timestamp type attribute, and employs a relative structure for inner messages.
 * It is crucial to understand the specific format specifications for both the outer and inner message components.
 *
 * Modifications:
 * - Relocated comments to the top of the class to provide greater prominence.
 * - Added comprehensive information about the message format, including the requirement of a timestamp type attribute for inner messages.
 *
 * @author YourName
 */
public class XYZProcessor {
   // Class implementation
}";documentation_debt
"
4. ```java
/**
 * Class XYZProcessor.
 *
 * Description: This class handles the processing of XYZ messages using the v1 message format.
 * The v1 format entails the presence of a timestamp, a timestamp type attribute, and the use of a relative structure for inner messages.
 * It is crucial to grasp the specific format requirements for both the outer and inner message components.
 *
 * Modifications:
 * - The comments have been placed prominently at the beginning of the class to draw attention.
 * - Additional details have been provided to clarify the message format, including the necessity of setting a timestamp type attribute for inner messages.
 *
 * @author YourName
 */
public class XYZProcessor {
   // Class implementation
}";documentation_debt
"/**

Class XYZProcessor.
Description: This class is responsible for processing XYZ messages based on the v1 message format.
The v1 format introduces a timestamp, a timestamp type attribute, and utilizes a relative structure for inner messages.
It is important to have a clear understanding of the format specifications for both the outer and inner message components.
Modifications:
Comments have been moved to the beginning of the class for better visibility.
Additional details have been included to describe the message format, including the inclusion of a timestamp type attribute for inner messages.
@author YourName
*/
public class XYZProcessor {
// Class implementation
}";documentation_debt
"""if the stream is not in state Open"" would be clearer.";documentation_debt
"""If the stream is not in the Open state, it would be clearer.""";documentation_debt
"""if the stream is not in the 'Open' state"" would provide better clarity.";documentation_debt
"""if the stream is not in the 'Open' state""";documentation_debt
"""If the stream is not in the Open state, it would be clearer.""";documentation_debt
I'm actually happy to just drop this though if we can update the documentation in our wiki to suggest people use hub. @JoshRosen or @rxin would one of you guys be able to put a few lines in https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools with the process you use? I can then verify and if it's all good I can just drip this PR.;documentation_debt
"""Since we have the option to update the documentation in our wiki, I suggest adding a few lines to the 'Useful Developer Tools' page in Confluence. @JoshRosen or @rxin, would one of you be able to provide some insights into the process you use with Hub? Once the information is added and verified, I can proceed to close this pull request. Thank you.""";documentation_debt
"""Considering the possibility of updating our documentation in the Confluence wiki, I propose adding a section to the 'Useful Developer Tools' page. @JoshRosen or @rxin, could one of you please contribute a brief explanation of the process you employ with Hub? Once the details are incorporated and verified, I will close this pull request accordingly. Thank you.""";documentation_debt
"""Given that we have the opportunity to enhance our documentation in the Confluence wiki, I recommend including a dedicated section on the 'Useful Developer Tools' page. @JoshRosen or @rxin, could you kindly share a few insights about your usage of Hub? Once the content is added and validated, I will proceed to close this pull request. Your cooperation is greatly appreciated.""";documentation_debt
"""As we can update our documentation in the Confluence wiki, it would be beneficial to include information about using Hub on the 'Useful Developer Tools' page. @JoshRosen or @rxin, would either of you be able to provide a brief outline of the process you follow? Once the details are inserted and verified, I can proceed with closing this pull request. Thank you for your assistance in this matter.""";documentation_debt
"@tianyouyangying +1 for @qiuchenjian 's suggestion.
1. Please change all chinese to english.
2. Please change PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity'
@qiuchenjian The origin SQL in doc actually are two statements, we should seperate them to avoid ambiguity.
Thanks.";documentation_debt
"""@tianyouyangying +1 for @qiuchenjian's suggestion.

Please translate all Chinese text into English.
Please modify the PR title to '[HOTFIX][DOC] Fix the SQL format in dml-of-carbondata.md to eliminate ambiguity.'
@qiuchenjian The original SQL in the documentation consists of two statements. To avoid ambiguity, we should separate them.
Thank you.""
Pull Request Title: ""[HOTFIX][DOC] Fix the SQL format in dml-of-carbondata.md to eliminate ambiguity""";documentation_debt
"""@tianyouyangying +1 for @qiuchenjian's suggestion.
Kindly translate all Chinese text to English.
Please update the PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to remove ambiguity.'
@qiuchenjian The original SQL statements in the documentation are actually two separate statements, and it is necessary to separate them to avoid ambiguity.
Thank you.""
Pull Request Title: ""[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to remove ambiguity""";documentation_debt
"""@tianyouyangying +1 for @qiuchenjian's suggestion.
Please convert all the Chinese text to English.
Request to modify the PR title to '[HOTFIX][DOC] Fix the SQL format in dml-of-carbondata.md to eliminate ambiguity.'
@qiuchenjian The original SQL in the documentation comprises two statements, and it is recommended to separate them to avoid ambiguity.
Thank you.""
Pull Request Title: ""[HOTFIX][DOC] Fix the SQL format in dml-of-carbondata.md to eliminate ambiguity""";documentation_debt
" ""@tianyouyangying +1 for @qiuchenjian's suggestion.
Please translate all Chinese content to English.
It is requested to change the PR title to '[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity.'
@qiuchenjian The SQL statements originally provided in the documentation are two separate statements, and it is advised to separate them to prevent any ambiguity.
Thank you.""
Pull Request Title: ""[HOTFIX][DOC] Fix the format of SQL in dml-of-carbondata.md to avoid ambiguity""";documentation_debt
"Thanx @vinayakumarb  for checking. Seems comment isn't coming, some problem, will check. it isn't coming for patches too.
Anyway the build seems cleans.";documentation_debt
"""Thanks, @vinayakumarb, for checking. It appears that the comment is not being displayed due to some issue. I will investigate and address the problem. This issue seems to affect patches as well. Nevertheless, the build appears to be clean.""";documentation_debt
"Rephrased comment:

""Thank you, @vinayakumarb, for checking. It appears that the comment is not displaying, which could be due to some issue. I will investigate the problem, as it is also not appearing for patches. Nonetheless, the build appears to be clean.""";documentation_debt
"""Thank you, @vinayakumarb, for checking. It appears that the comment is not displaying due to some issue. I will investigate the problem, as it is also not appearing for patches. However, I want to note that the build appears to be clean.""";documentation_debt
"""Thank you, @vinayakumarb, for checking. It appears that the comment isn't displaying due to some issue. I will investigate and address the problem. This issue seems to apply to patches as well. Nevertheless, the build appears to be clean.""";documentation_debt
The documentation need to be updated. Does copying from mkl-dnn array to sparse ndarray work??;documentation_debt
Does copying from mkl-dnn array to sparse ndarray work in the current documentation?;documentation_debt
Does the copying from mkl-dnn array to sparse ndarray work? It is important to update the documentation to reflect this information accurately.;documentation_debt
Does copying from an mkl-dnn array to a sparse ndarray work? This question needs clarification in the documentation and should be updated accordingly.;documentation_debt
Does the copying process from mkl-dnn array to sparse ndarray work? This information should be updated in the documentation.;documentation_debt
[SPARK-8639] [Docs] Fixed Minor Typos in Documentation;documentation_debt
[SPARK-8639] [Docs] Corrected minor typos in the documentation;documentation_debt
[SPARK-8639] [Docs] Addressed minor typos in the documentation;documentation_debt
[SPARK-8639] [Docs] Resolved minor typos in the documentation;documentation_debt
[SPARK-8639] [Docs] Resolved minor typos in the documentation;documentation_debt
We also need to add user docs along with examples for the new function;documentation_debt
In addition, we need to include user documentation and examples for the new function.;documentation_debt
In addition, it is necessary to include user documentation and examples for the newly introduced function.;documentation_debt
In addition, it is necessary to include user documentation and provide examples for the newly introduced function.;documentation_debt
In addition, it is necessary to include user documentation and examples for the new function.;documentation_debt
"This also helps us get rid of the sparkr-docs maven profile as docs are now built by just using -Psparkr when the roxygen2 package is available
Related to discussion in #6567 
cc @pwendell @srowen -- Let me know if this looks better";documentation_debt
"""This change also eliminates the need for the sparkr-docs Maven profile, as the documentation can now be built simply by using the -Psparkr option when the roxygen2 package is available. This is related to the discussion in #6567. cc @pwendell @srowen - Please let me know if this improvement meets the requirements.""";documentation_debt
"""This change also eliminates the need for the sparkr-docs Maven profile, as the documentation can now be built using only the -Psparkr flag when the roxygen2 package is available. This resolves the discussion mentioned in #6567. cc @pwendell @srowen -- Please let me know if this improvement meets the requirements.""";documentation_debt
"""This change has the added benefit of eliminating the need for the sparkr-docs Maven profile. Now, the documentation can be built simply by using -Psparkr when the roxygen2 package is available. This addresses the discussion in #6567. cc @pwendell @srowen -- Please let me know if this improvement meets your expectations.""";documentation_debt
"""This change also eliminates the need for the sparkr-docs Maven profile. Now, the documentation can be built simply by using the -Psparkr flag when the roxygen2 package is available. This addresses the discussion in #6567.
cc @pwendell @srowen -- Please let me know if this improvement meets your expectations.""";documentation_debt
Well, I did not see anything yet in the Github. However, if you add just a snippet of comments explaining why we need such method, I am fine with it.;documentation_debt
"""While I haven't seen any changes in the GitHub repository yet, I believe it would be beneficial to add a snippet of comments explaining the rationale behind the need for this method. This will provide better clarity and understanding.""";documentation_debt
"""While I haven't seen anything in the GitHub repository yet, I would be satisfied if you could add a brief comment explaining the purpose and necessity of this method. Thank you.""";documentation_debt
"""While I haven't seen any changes on GitHub yet, I'm fine with adding a small snippet of comments to explain the necessity of this method. Just a brief explanation would be sufficient.""";documentation_debt
"""I haven't seen any comments on GitHub yet. However, if you could provide a brief comment explaining the purpose and need for this method, that would be sufficient for me.""";documentation_debt
LGTM barring the typo;documentation_debt
"""Looks good to me, except for the typo.""";documentation_debt
"""Looks good to me, except for the typo. Everything else seems fine.""";documentation_debt
"""Looks good to me (LGTM), except for the typo that needs to be addressed.""";documentation_debt
"""Looks good to me, except for the typo. Everything else seems fine.""";documentation_debt
@subhankarb We should also add Redis Sink to the fault tolerance guarantee table for the connectors in the documentation. It can be found at `flink/docs/apis/streaming/fault_tolerance.md`.;documentation_debt
"""@subhankarb, let's make sure to include Redis Sink in the fault tolerance guarantee table for the connectors in the documentation. You can find the relevant information at flink/docs/apis/streaming/fault_tolerance.md.""";documentation_debt
"""@subhankarb, in addition, let's make sure to include Redis Sink in the fault tolerance guarantee table for the connectors in the documentation. You can find the relevant section at flink/docs/apis/streaming/fault_tolerance.md.""";documentation_debt
"""@subhankarb, we should include Redis Sink in the fault tolerance guarantee table for the connectors in the documentation. You can find the relevant information at flink/docs/apis/streaming/fault_tolerance.md.""";documentation_debt
"""@subhankarb, we should include Redis Sink in the fault tolerance guarantee table for the connectors in the documentation. You can find the relevant section at flink/docs/apis/streaming/fault_tolerance.md.""";documentation_debt
"The reason is I get rid of `this.dags.get(dagId)` and there's no reference to a `dag` associated with this `dagId`. So clean up has to happen before `remove`, or `dag` object won't be fetched. 
Will add comment to make it clear";documentation_debt
"""The reason for this change is to eliminate the usage of this.dags.get(dagId) and since there is no reference to a dag associated with this dagId, the cleanup must be performed before the remove operation otherwise, the dag object won't be fetched. I will add a comment to provide clarity on this matter.""";documentation_debt
"""The reason for this change is to remove this.dags.get(dagId) since there is no reference to a dag associated with this dagId. Therefore, the cleanup process needs to occur before the remove operation otherwise, the dag object won't be fetched. I will add a comment to clarify this.""";documentation_debt
"""The reason for performing the cleanup before the remove operation is that I have removed this.dags.get(dagId), and there is no reference to a dag associated with this dagId. If the cleanup is not done before the remove, the dag object will not be fetched. To provide clarity, I will add a comment explaining this in the code.""";documentation_debt
"""The reason for removing this.dags.get(dagId) is that there is no reference to a dag associated with this dagId. Therefore, the cleanup process needs to happen before the remove operation, or else the dag object won't be fetched. I will add a comment to clarify this.""";documentation_debt
"use `re.search()`.  also, I don't think this needs to be private, so remove the leading underscore.
maybe add a comment like:  ""we don't use json.loads to test validity because we don't want to propagate json syntax errors downstream to the runner""";documentation_debt
"""Consider using re.search() instead. Additionally, it may not be necessary to mark this method as private, so the leading underscore can be removed. You can add a comment explaining the rationale behind not using json.loads() for testing validity, stating that the goal is to avoid propagating JSON syntax errors downstream to the runner.""";documentation_debt
"""Please use re.search() instead. Additionally, I don't believe this needs to be a private method, so let's remove the leading underscore. It might be helpful to add a comment explaining our approach: 'We avoid using json.loads for testing validity to prevent propagating JSON syntax errors downstream to the runner.'""";documentation_debt
"""Consider using re.search() instead. Additionally, there is no need for this method to be private, so remove the leading underscore. It might be helpful to add a comment explaining the reason for not using json.loads() to test validity, as we aim to prevent JSON syntax errors from propagating downstream to the runner.""";documentation_debt
"""Please use re.search() instead. Additionally, it is not necessary for this method to be private, so please remove the leading underscore.
Consider adding a comment like: 'We don't use json.loads to test validity because we want to avoid propagating JSON syntax errors downstream to the runner.'""";documentation_debt
May be good to update doc for `hbase.coprocessor.regionserver.classes` in section `Restricting Coprocessor Usage` of cp.adoc?;documentation_debt
"""It would be beneficial to update the documentation for hbase.coprocessor.regionserver.classes in the 'Restricting Coprocessor Usage' section of cp.adoc. This update will ensure that the documentation accurately reflects the functionality and usage of the mentioned configuration property.""";documentation_debt
"""It would be beneficial to update the documentation for the hbase.coprocessor.regionserver.classes configuration in the 'Restricting Coprocessor Usage' section of cp.adoc to ensure it reflects the latest changes.""";documentation_debt
"""It would be beneficial to update the documentation for hbase.coprocessor.regionserver.classes in the 'Restricting Coprocessor Usage' section of cp.adoc. This update will provide users with accurate and up-to-date information about the configuration.""";documentation_debt
"""It would be beneficial to update the documentation for hbase.coprocessor.regionserver.classes in the Restricting Coprocessor Usage section of cp.adoc. This will ensure that the documentation reflects the current information and provides accurate guidance to users.""";documentation_debt
"This PR proposes to `include` `_images` and `_sources` directories, generated from Sphinx, in Jekyll build.
**For `_images` directory,**
After SPARK-31851, now we add some images to use within the pages built by Sphinx. It copies and images into `_images` directory. Later, when Jekyll builds, the underscore directories are ignored by default which ends up with missing image in the main doc.
Before:
After:
**For `_sources` directory,**
To show the images correctly in PySpark documentation.
No, only in unreleased branches.
Manually tested via:";documentation_debt
"""This pull request aims to include the _images and _sources directories, generated from Sphinx, in the Jekyll build process.

Regarding the _images directory, after the SPARK-31851 improvement, we have added some images for use within the pages generated by Sphinx. These images are copied into the _images directory. However, during the Jekyll build, underscore directories are ignored by default, resulting in missing images in the main documentation.

Before:
[Provide details of the issue before the change]

After:
[Describe the expected behavior and how the issue is resolved]

Regarding the _sources directory, this change is necessary to display images correctly in the PySpark documentation.

This change applies only to unreleased branches.

Manual testing has been performed using [describe the testing process and any specific test cases].";documentation_debt
"""This pull request aims to include the _images and _sources directories, generated by Sphinx, in the Jekyll build process.

Regarding the _images directory, after the changes made in SPARK-31851, we have added some images for use within the pages built by Sphinx. These images are copied into the _images directory. However, during Jekyll build, the underscore directories are ignored by default, resulting in missing images in the main documentation.

Before:
[Image]

After:
[Image]

As for the _sources directory, its inclusion is necessary to display images correctly in the PySpark documentation.

This change applies only to unreleased branches.

Manual testing has been performed using: [mention the testing process/methodology].""";documentation_debt
"""This pull request suggests including the _images and _sources directories, which are generated by Sphinx, in the Jekyll build process.

Regarding the _images directory, as a result of SPARK-31851, we have added some images to be used within the pages generated by Sphinx. These images are copied into the _images directory. However, during the Jekyll build, the underscore directories are ignored by default, leading to missing images in the main documentation.

Before:
[Insert description or example]

After:
[Insert description or example]

Regarding the _sources directory, this change is necessary to correctly display images in the PySpark documentation.

Please note that this change applies only to unreleased branches.

Manual testing has been performed by:
[Describe the manual testing process and results]""";documentation_debt
"""This pull request proposes the inclusion of the _images and _sources directories, generated by Sphinx, in the Jekyll build process.
Regarding the _images directory:
After the implementation of SPARK-31851, we have added some images for use within the pages generated by Sphinx. These images are copied into the _images directory. However, during Jekyll builds, directories with leading underscores are ignored by default, resulting in missing images in the main documentation.
Before:
[Image missing]
After:
[Image displayed correctly]
Regarding the _sources directory:
This inclusion is necessary to ensure proper display of images in the PySpark documentation.
Please note that this change applies only to unreleased branches.
Manual testing has been performed, and the changes have been verified via: [description of manual testing].""";documentation_debt
"It was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl 
https://github.com/apache/couchdb-couch-replicator/commit/b48d7bdc49d107f33d96f08603006a1c9edc322f#diff-ba1cca81bdc216835256f72cc6a72fa5R374
Beyond that not sure about its origin. Technically it is possible, say, for a couch_replicator_auth plugin to link to a process that then exit normally so we end up in this part of the code and it won't be a worker, changes reader or any other known process in this module. So perhaps we should change to a comment indicating that instead of leaving the commented code in there?";documentation_debt
"""The code snippet in question was originally derived from the old couch_replicator_scheduler_job.erl and then carried over from couch_replicator.erl. You can refer to the commit at https://github.com/apache/couchdb-couch-replicator/commit/b48d7bdc49d107f33d96f08603006a1c9edc322f#diff-ba1cca81bdc216835256f72cc6a72fa5R374 for more details. Although its specific origin is uncertain, it is worth noting that there is a possibility for a couch_replicator_auth plugin to link to a process that subsequently exits normally, resulting in execution reaching this part of the code. As an alternative to leaving the commented code as is, it may be more suitable to replace it with a comment that clarifies this behavior.""";documentation_debt
"""The code in question was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl. You can find its origin and context in the commit history: [link to commit]. However, beyond that, its exact purpose is uncertain. It is technically possible for a couch_replicator_auth plugin to link to a process that exits normally, leading to execution in this section of the code. This process may not be recognized as a worker, changes reader, or any other known process in this module. Considering this, it might be more appropriate to replace the commented code with a descriptive comment explaining this behavior.""";documentation_debt
"""The code snippet in question was originally carried over from the old couch_replicator_scheduler_job.erl and subsequently from couch_replicator.erl. You can find its origin in this commit: link.

However, its exact purpose beyond that is unclear. It is technically possible for a couch_replicator_auth plugin to link to a process that then exits normally, resulting in execution in this part of the code. In such cases, it may not be a worker, changes reader, or any other known process in this module. Consider replacing the commented code with a comment explaining this behavior instead.""";documentation_debt
"""The commented code in question was carried over from the old couch_replicator_scheduler_job.erl and then from couch_replicator.erl. You can find the relevant commit at: [link to commit]. However, its exact origin is unclear. It is worth noting that it is technically possible for a couch_replicator_auth plugin to link to a process that exits normally, leading to the execution of this code section. In such cases, the process won't be recognized as a worker, changes reader, or any other known process in this module. Considering this, it might be more appropriate to replace the commented code with a comment explaining this behavior instead.""";documentation_debt
It would be nice to have some javadocs for the new methods, although they are fairly self-explanatory;documentation_debt
It would be beneficial to include Javadoc comments for the new methods, even though their functionality is quite self-explanatory.;documentation_debt
"""It would be beneficial to include some Javadoc documentation for the new methods, even though their functionality is relatively self-explanatory.""";documentation_debt
Certainly! Although the new methods are self-explanatory, it would be beneficial to have corresponding JavaDoc comments. This will provide additional clarity and documentation for future reference.;documentation_debt
"""While the new methods are quite self-explanatory, it would be beneficial to have some Javadoc comments to provide additional documentation.""";documentation_debt
"+1 for getting this into the docs. I offer some grammatical improvements. Also, is it correct to describe ""operators are required to completely process a given watermark before forwarding it downstream"" as a general rule, meaning that it might have exceptions, or should we simply say ""operators are required ..."" without adding this caveat?
I changed behaviour to behavior because most of the docs seem to be using American spellings rather than English ones, but I'm not sure if we have a policy regarding this.";documentation_debt
"""+1 for incorporating this into the documentation. I have made some grammatical improvements. Regarding the statement 'operators are required to completely process a given watermark before forwarding it downstream,' should we describe it as a general rule or simply state 'operators are required...' without the caveat? Additionally, I changed 'behaviour' to 'behavior' to align with the predominantly American spelling used throughout the documentation, but I'm unsure if we have a specific policy on this matter.""";documentation_debt
"""+1 for including this in the documentation. I have made some grammatical improvements. Regarding the statement about operators processing a watermark, should we state it as a general rule or omit the caveat? For consistency with the majority of the documentation, I changed 'behaviour' to 'behavior.' However, I am unsure if we have a specific policy regarding American or British spellings.""";documentation_debt
"""+1 for incorporating this into the documentation. I have made some grammatical improvements. Regarding the statement about operators processing a watermark, should we describe it as a general rule or omit the caveat? As most of the documentation seems to use American spellings, I changed 'behaviour' to 'behavior.' However, I am uncertain if we have a specific policy on the use of spellings.""";documentation_debt
"""+1 for adding this to the documentation. I have made some grammatical improvements. Should we state that operators are required to completely process a watermark as a general rule or exclude the caveat? As the majority of the documentation uses American spellings, I changed 'behaviour' to 'behavior.' However, I'm unsure if there is a specific policy on this matter.""";documentation_debt
Yes, I think we randomly chose one, it depends on how much data you have, we'll update the doc for the best practice. Also there is metric to show the cache hit rate, if it's too low, maybe we need to raise the cache size.;documentation_debt
"""It appears that we randomly chose a cache size, but the optimal size depends on the amount of data. We will update the documentation to provide best practices for determining the appropriate cache size. Additionally, we have a metric to track the cache hit rate, and if the rate is consistently low, it might be necessary to increase the cache size.""";documentation_debt
"""Yes, the choice of the cache size depends on the volume of data you have. We will update the documentation to provide best practices in this regard. Additionally, there is a metric available to track the cache hit rate. If the hit rate is consistently low, it may indicate a need to increase the cache size.""";documentation_debt
"""It seems that the selection of the cache size was arbitrary, and the optimal choice depends on the volume of data. We will update the documentation to provide best practices in this regard. Additionally, there is a metric available to monitor the cache hit rate. If the rate is consistently low, it may be necessary to increase the cache size.""";documentation_debt
"""Yes, the choice of cache size depends on the volume of data you have. We will update the documentation to provide best practices in this regard. Additionally, there is a metric available to monitor the cache hit rate. If the cache hit rate is consistently low, it may indicate a need to increase the cache size.""";documentation_debt
"Can you add a doc ?
Also can you update the release notes if we add this new feature to the master - https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html ?
How about remap plugin? can you check the corresponding symbols for remap plugin as well?
What other features are you planning for this ?";documentation_debt
"""Could you please add documentation for this feature? Additionally, if we incorporate this new feature into the master branch, it would be necessary to update the release notes found at https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html. Furthermore, it would be helpful to check the corresponding symbols for the remap plugin. Are there any other features planned for implementation?""";documentation_debt
"""Could you please add documentation for this feature? Additionally, if we are adding this new feature to the master branch, it would be great to update the release notes accordingly. The release notes can be found at: https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html. Can you also check the corresponding symbols for the remap plugin? It would be helpful to include those as well. Lastly, what other features are you planning to incorporate?""";documentation_debt
"""Could you please add documentation for this feature? Additionally, don't forget to update the release notes if we include this new feature in the master branch. You can find the release notes here: [link to release notes page].
Regarding the remap plugin, could you also check the corresponding symbols for it? It would be great to have consistency in the documentation for both features.
Lastly, I'm curious to know what other features you have planned for this project.""";documentation_debt
"""Could you please add documentation for this feature? Additionally, if we are adding this new feature to the master branch, it would be great to update the release notes at https://docs.trafficserver.apache.org/en/latest/release-notes/whats-new.en.html. Could you also verify the corresponding symbols for the remap plugin? Lastly, I'm curious about any other features you have planned for this.""";documentation_debt
Would be nice to add a comment on why TreeMap (ordering) is needed.;documentation_debt
"""Consider adding a comment to explain the rationale behind using a TreeMap for ordering. This will help to clarify the reason for its usage in the code.""";documentation_debt
"""It would be beneficial to include a comment explaining the necessity of TreeMap for maintaining ordering based on the natural order of the keys.""";documentation_debt
"""Adding a comment explaining the necessity of TreeMap (ordering) would be beneficial.""";documentation_debt
"""It would be beneficial to include a comment explaining the need for TreeMap (ordering) in this code. This comment will provide clarity on the importance of maintaining ordering based on the natural order of the keys.""";documentation_debt
" - No
 - Yes. (please explain the change and update document)
 - No
 - Yes";documentation_debt
"No
Yes, please provide an explanation of the change and update the documentation accordingly.
No
Yes";documentation_debt
"No
Yes. (Please provide an explanation of the change and update the documentation accordingly)
No
Yes";documentation_debt
"No.
Yes. Please provide an explanation for the change and update the documentation accordingly.
No.
Yes.";documentation_debt
"No
Yes, please explain the change and update the documentation accordingly.
No
Yes";documentation_debt
"will add java doc to explain it.
for backward compatibility, I can add default value in this function instead of the newly introduced one.";documentation_debt
"""I will add JavaDoc to provide an explanation for this function. Additionally, to maintain backward compatibility, I can incorporate a default value in this function instead of utilizing the newly introduced one.""";documentation_debt
"""I will add JavaDoc to provide an explanation for it. Additionally, for backward compatibility, I can include a default value in this function instead of using the newly introduced one.""";documentation_debt
"""I will include JavaDoc to provide an explanation for it. Additionally, for backward compatibility, I can incorporate a default value in this function instead of using the newly introduced one.""";documentation_debt
"""I will include JavaDoc to provide an explanation for this functionality. Additionally, to ensure backward compatibility, I can update this function to include a default value instead of relying solely on the newly introduced feature.""";documentation_debt
"ignite-6774 Java doc is broken: ""LUDecomposition.java:40: warning - T""¦";documentation_debt
"""The JavaDoc in LUDecomposition.java seems to be broken, as it is generating a warning on line 40 regarding the 'T' parameter. Please address this issue.""";documentation_debt
"""There seems to be an issue with the JavaDoc in LUDecomposition.java, as it generates a warning at line 40 regarding the 'T' component.""";documentation_debt
"""The JavaDoc in LUDecomposition.java seems to be broken, showing a warning at line 40 regarding 'T'. This should be fixed.""";documentation_debt
"""The JavaDoc in LUDecomposition.java appears to be broken, generating a warning at line 40 (T). It requires fixing to ensure proper documentation generation.""";documentation_debt
"nit: Should we add a comment here that lookupKey contains only a single column ? It will make understanding ""allNull"" easier.";documentation_debt
"""Add a comment clarifying that the lookupKey parameter contains only a single column. This will facilitate the understanding of the allNull variable.""";documentation_debt
"""Add a comment to clarify that lookupKey contains only a single column. This will help in better understanding the purpose of allNull.""";documentation_debt
"""Add a comment clarifying that lookupKey contains only a single column. This will facilitate better understanding of the allNull behavior.""";documentation_debt
"""Consider adding a comment to clarify that the lookupKey variable contains only a single column. This will help in understanding the purpose and behavior of the allNull condition.""";documentation_debt
Let's add a comment?;documentation_debt
"""Propose adding a comment for additional clarification.""";documentation_debt
"""Add a comment for better clarity and understanding.""";documentation_debt
"""Suggest adding a comment for improved clarity.""";documentation_debt
"""Add a comment for additional clarification.""";documentation_debt
Can you add a comment with an example of an actual value here?;documentation_debt
"""Add a comment providing an example of an actual value for better clarity.""";documentation_debt
"""Add a comment providing an example of an actual value for better illustration.""";documentation_debt
"""Include a comment with an illustrative example of an actual value in this section.""";documentation_debt
"""Add a comment providing an example of an actual value in this context.""";documentation_debt
"Typo.""the the"" => ""then the""";documentation_debt
"""Fix typo: replace 'the the' with 'then the'.""";documentation_debt
"""Fix the typo by replacing 'the the' with 'then the'.""";documentation_debt
"""Fix the typo 'the the' by replacing it with 'then the'.""";documentation_debt
"""Fix the typo 'the the' to 'then the' for better clarity.""";documentation_debt
Typo: configureSasToken();documentation_debt
"""Correct typo in configureSasToken() method.""";documentation_debt
"""Fix typo in configureSasToken() method.""";documentation_debt
"""Correct a typographical error in the configureSasToken() method.""";documentation_debt
"""Corrected a typo in the configureSasToken() method.""";documentation_debt
typo: Gobblin config path;documentation_debt
Fixed typo: Gobblin config path;documentation_debt
"""Typo: 'Gobblin config path'""";documentation_debt
"""Corrected a typo in the Gobblin configuration path.""";documentation_debt
"""Fix typo in Gobblin config path.""";documentation_debt
Instead of these 3 comment lines, just point to the design document;documentation_debt
"""Refer to the design document for detailed information on the implementation and rationale behind this approach.""";documentation_debt
"""Refer to the design document for detailed information regarding this implementation.""";documentation_debt
"""Refer to the design document for detailed information regarding this implementation.""";documentation_debt
"""Refer to the design document for detailed information regarding the functionality and purpose of this implementation.""";documentation_debt
can you add a quick comment of what `true` means here:;documentation_debt
"""Add a brief comment explaining the significance of true in this context.""";documentation_debt
"""Add a brief comment explaining the significance of the true parameter in this context.""";documentation_debt
"""Add a brief comment to clarify the meaning of true in this context.""";documentation_debt
Could you please include a brief comment explaining the meaning of true in this context?;documentation_debt
Fix typo in err msg;documentation_debt
Corrected a typographical error in the error message.;documentation_debt
Fix a typographical error in the error message.;documentation_debt
Correct the typographical error in the error message.;documentation_debt
Fix typo in error message;documentation_debt
Is `lenghth` a typo here?;documentation_debt
"""Should the word 'lenghth' be considered a typo in this particular instance?""";documentation_debt
"Is the use of ""lenghth"" a typo in this context?";documentation_debt
"""Is the use of lenghth a typographical error in this context?""";documentation_debt
"Is the use of ""lenghth"" in this context a typographical error?";documentation_debt
I would add a comment above to state the assumption. But it's minor though.;documentation_debt
I would suggest adding a comment above to explicitly state the assumption, even though it is a minor detail.;documentation_debt
"""It would be helpful to add a comment above to explicitly state the assumption. Although it is a minor detail, providing clarity on the assumption can contribute to better code understanding.""";documentation_debt
I would suggest adding a comment above to explicitly state the assumption. Although this is a minor change, it can help improve code clarity and maintainability.;documentation_debt
I would suggest adding a comment above to explicitly state the assumption, although this is a minor detail.;documentation_debt
[NNVM][KERAS] Fix keras model converter and improve tutorial;documentation_debt
[NNVM][KERAS] Resolved issues with the Keras model converter and enhanced the tutorial;documentation_debt
"""[NNVM][KERAS] Resolved issues with the keras model converter and enhanced the tutorial for better clarity and understanding.""";documentation_debt
[NNVM][KERAS] Resolved issues with the Keras model converter and enhanced the tutorial for better clarity.;documentation_debt
[NNVM][KERAS] Resolve issues with the keras model converter and enhance the tutorial;documentation_debt
Are these documented?;documentation_debt
Are these items properly documented?;documentation_debt
Have these been documented?;documentation_debt
Is documentation available for these?;documentation_debt
Are these elements properly documented?;documentation_debt
BTW, do we need a new test case for `correctly set the active session`?;test_debt
"""By the way, should we consider adding a new test case to verify that the active session is set correctly?""";test_debt
"""Do we need to include a new test case to ensure that the active session is set correctly?""";test_debt
"""By the way, do we require a new test case to ensure that the active session is set correctly?""";test_debt
"""Should we consider adding a new test case to ensure that the active session is correctly set?""";test_debt
"""By the way, should we add a new test case to ensure that the active session is correctly set?""";test_debt
"""By the way, should we consider adding a new test case to ensure that the active session is set correctly?""";test_debt
"I think no documentation and no test. I extracted the example from the code you changed actually to show you what your change will affect. In my opinion the get/set path needs to be removed, but that is another topic. The trouble is that get/set in the MOP is in general very badly documented. So it might or it might not be, that your change is a breaking one. Well no, it is a breaking change in terms of semantics, but if real world examples will be badly affected by this? no idea. But I have another example:
    public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
the last three asserts will all fail.";test_debt
"I believe there is no documentation or test provided. I extracted an example from the code you modified to demonstrate the impact of your changes. In my opinion, the `get/set` path should be removed, but that is a separate topic. The issue here is that the documentation for `get/set` in the MOP is generally poorly documented. Therefore, it is uncertain whether your change is breaking or not. Well, in terms of semantics, it is indeed a breaking change. However, we don't know if real-world examples will be significantly affected. Let me provide another example:

```groovy
public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
```

The last three assertions will all fail.";test_debt
"I believe there is no documentation or test provided for this change. I extracted the example from the code you modified to demonstrate the impact of your change. In my opinion, the get/set path should be removed, but that is a separate topic for discussion. The issue here is that the documentation for get/set in the MOP is generally poorly documented. As a result, it's uncertain whether your change could potentially introduce breaking changes. Well, to be precise, it is a breaking change in terms of semantics. However, we don't know if real-world examples will be adversely affected by this. Here's another example to consider:

```groovy
public foo
}
a.x = 1
assert a.x == 1
a.foo = 2
assert a.@foo == 2
assert a.foo == null
assert a == [x:1]
```

The last three assertions will all fail.";test_debt
"""I believe there is a lack of documentation and test coverage for this change. I've extracted an example from the modified code to demonstrate the impact of your changes. In my opinion, the get/set path should be removed, but that is a separate discussion. The main issue is that the documentation for get/set in the MOP is generally inadequate. Consequently, it's unclear whether your change is a breaking one. Well, in terms of semantics, it is a breaking change, but its impact on real-world examples is uncertain. Here's another example to illustrate this:""";test_debt
"""I've noticed a lack of documentation and test cases for this modification. To demonstrate the impact of your changes, I've extracted an example from the affected code. In my opinion, the get/set path should be eliminated, but that's a separate matter. The problem is that the get/set functionality in the MOP lacks comprehensive documentation. Hence, it's difficult to determine whether your change introduces any breaking changes. Although semantically it does, the extent to which real-world examples are affected remains uncertain. Allow me to provide another example for illustration purposes:""";test_debt
"""It appears that there is no accompanying documentation or test suite for this change. I've taken the liberty to extract an example from the modified code to showcase how your changes may impact it. Personally, I believe the get/set path should be removed, although that is a separate topic of discussion. The challenge lies in the poor documentation of the get/set functionality in the MOP. As a result, it is unclear whether your change introduces any breaking changes. Well, semantically it does, but its implications on real-world examples are uncertain. Let me illustrate this with another example:""";test_debt
"""Based on my observation, there seems to be a lack of documentation and test coverage for this change. I've extracted an example from the modified code to demonstrate the implications of your changes. From my perspective, the get/set path should be eliminated, although that is a separate matter open for discussion. The main issue arises from the inadequate documentation of the get/set functionality in the MOP. Consequently, it's challenging to determine whether your change has any breaking effects. Although it does semantically, its impact on real-world examples remains uncertain. Here's an additional example to shed more light on this:""";test_debt
@zjffdu can u confirm it's a flaky test?;test_debt
"""@zjffdu, could you please confirm if this test is flaky? We need to verify its reliability.""";test_debt
"""@zjffdu, could you please confirm if this test is considered flaky? It would be helpful to have a confirmation regarding its reliability.""";test_debt
"""@zjffdu, could you please confirm if this test is considered flaky?";test_debt
"""@zjffdu, can you confirm if this test is flaky? We need to verify its stability and ensure consistent results.""";test_debt
"""@zjffdu, could you please confirm whether this test is considered flaky? It would be helpful to have confirmation on its reliability.""";test_debt
"""@zjffdu, could you please confirm if this test is considered flaky?";test_debt
"Overall is ok for me. 
**IT** coverage will be useful so i will be addressing through #3106";test_debt
"""Overall, I find it acceptable. Adding integration test (IT) coverage would be beneficial, and I will address it separately in pull request #3106.""";test_debt
"""Overall, I'm satisfied with the changes. Adding integration test (IT) coverage would be beneficial, and I'll be addressing it in pull request #3106.""";test_debt
"""Overall, I'm satisfied with the changes. However, it would be beneficial to have integration test (IT) coverage. I will address this in pull request #3106.""";test_debt
"""Overall, I'm satisfied with the changes. However, I believe it would be beneficial to have integration test (IT) coverage. I will address this concern in pull request #3106.""";test_debt
"""Overall, I am satisfied with the changes. Having IT coverage would be beneficial, and I will address it separately through #3106.""";test_debt
"""Overall, I'm okay with it. Adding IT coverage would be beneficial, and I will address it in #3106.""";test_debt
Thanks for the contribution, @amakaur  Could you add unit test  ?;test_debt
"""Thank you for your contribution, @amakaur. It would be great if you could include unit tests as well.""";test_debt
"""Thank you, @amakaur, for your contribution. Could you please include a unit test as well?""";test_debt
"""Thank you, @amakaur, for your contribution. Could you please add unit tests to cover this functionality?""";test_debt
"""Thank you for your contribution, @amakaur. It would be great if you could also include unit tests to ensure the functionality is thoroughly tested.""";test_debt
"""Thank you for your contribution, @amakaur. Could you please add a unit test as well?""";test_debt
"""Thank you, @amakaur, for your contribution. It would be great if you could also include unit tests for this change.""";test_debt
Update alphanumeric sort docs + more tests / examples;test_debt
"""Enhance alphanumeric sort documentation and augment test coverage with additional tests and examples.""";test_debt
"""Revise alphanumeric sort documentation and expand test suite with more tests and illustrative examples.""";test_debt
"""Refine documentation for alphanumeric sort and bolster test coverage by adding more tests and practical examples.""";test_debt
"""Improve alphanumeric sort documentation and augment test suite with additional tests and meaningful examples.""";test_debt
"""Update documentation for alphanumeric sorting and enhance test coverage by including more tests and illustrative examples.""";test_debt
"""Revamp alphanumeric sort documentation and extend test suite with additional tests and informative examples.""";test_debt
You haven't actually added tests, and that's not all this PR does. At the least, this doesn't match the intent you describe, and should be closed. I'd back up and describe the test you want in the JIRA.;test_debt
"1. ""This pull request encompasses more than just adding tests it includes a range of changes. However, the current description doesn't align with the actual content of the pull request. To ensure clarity and accuracy, it may be appropriate to close this pull request and provide a detailed description of the desired test in the associated JIRA ticket.""";test_debt
"2. ""The intent described in the pull request message does not match the actual content, which goes beyond adding tests. It seems necessary to reassess the scope and purpose of this pull request. It may be advisable to close it and instead provide a comprehensive description of the desired test in the corresponding JIRA ticket.""";test_debt
"3. ""It appears that the pull request's description does not accurately reflect its contents, as it involves more than just adding tests. To ensure alignment between the intent and the actual changes, it might be appropriate to close this pull request and instead provide a clear description of the desired test within the relevant JIRA ticket.""";test_debt
"4. ""Although the pull request message suggests adding tests, the actual changes go beyond that scope. To ensure proper alignment between the intent and the content of this pull request, it might be best to close it and provide a detailed description of the desired test in the associated JIRA ticket.""";test_debt
"5. ""The description of this pull request does not accurately reflect the changes made, as it involves more than simply adding tests. To address this discrepancy, it may be necessary to close the pull request and provide a comprehensive explanation of the desired test in the corresponding JIRA ticket.""";test_debt
"6. ""The current pull request message does not capture the complete scope of the changes being made, which extend beyond adding tests. To align the description with the actual content, it may be advisable to close this pull request and instead provide a thorough description of the desired test within the relevant JIRA ticket.""";test_debt
Ah yes. I'll update them in a while. There's actually some problem with the unit test I've written too. Travis fails sporadically.;test_debt
"1. ""I will update the unit tests shortly. It seems that there is an issue with the current test suite, as Travis sporadically fails. I will investigate the problem and address it accordingly.""";test_debt
"2. ""I plan to make updates to the unit tests shortly. However, it seems that there is an intermittent problem with the test suite on Travis, leading to occasional failures. I will look into the issue and resolve it to ensure stable test results.""";test_debt
"3. ""I will be making adjustments to the unit tests soon. It appears that there is an underlying problem causing sporadic failures in the Travis build. I will investigate the issue and work on rectifying it to establish reliable test outcomes.""";test_debt
"4. ""I intend to update the unit tests in a little while. It seems there is an issue with the current test suite, causing sporadic failures on Travis. I will carefully examine the problem and take the necessary steps to fix it and ensure consistent test results.""";test_debt
"5. ""I will be revising the unit tests shortly. However, there seems to be an intermittent problem with the test suite, resulting in sporadic failures on Travis. I will analyze the issue further and implement the required fixes to achieve stable test execution.""";test_debt
"6. ""I have plans to update the unit tests in the near future. It has come to my attention that there is an intermittent problem causing failures in the Travis build. I will conduct a thorough investigation of the issue and implement appropriate solutions to ensure consistent test outcomes.""";test_debt
Added more test cases for negative numbers and boundary conditions.;test_debt
"1. ""Expanded the test suite with additional test cases to cover negative numbers and boundary conditions.""";test_debt
"2. ""Enhanced test coverage by incorporating more test cases that handle negative numbers and boundary conditions.""";test_debt
"3. ""Included additional test cases to account for negative numbers and boundary conditions, thereby improving the comprehensiveness of the test suite.""";test_debt
"4. ""Augmented the existing test suite with new test cases specifically designed to cover scenarios involving negative numbers and boundary conditions.""";test_debt
"5. ""Enriched the test suite by adding more test cases that specifically address negative numbers and boundary conditions.""";test_debt
"6. ""Improved test coverage by introducing supplementary test cases that focus on negative numbers and boundary conditions.""";test_debt
By the way, the test still doesn't catch the bug that you're fixing (https://issues.apache.org/jira/browse/FLINK-8286). I think we need proper end-to-end tests that really test Flink-Kerberos integration on an actual YARN cluster. I have started looking onto using Docker Compose for that, i.e. bringing up a hadoop cluster in docker with Kerberos and then running Flink on that as an end-to-end test.;test_debt
"""Additionally, it's worth noting that the current test implementation does not effectively capture the bug being addressed (https://issues.apache.org/jira/browse/FLINK-8286). To ensure comprehensive testing of Flink-Kerberos integration on a live YARN cluster, it may be necessary to develop proper end-to-end tests. One potential approach under consideration is utilizing Docker Compose to set up a Hadoop cluster in a Docker environment with Kerberos, allowing for running Flink on that cluster as part of an end-to-end test suite.""";test_debt
"""Furthermore, it is important to acknowledge that the existing test setup does not fully expose the bug being resolved (https://issues.apache.org/jira/browse/FLINK-8286). To achieve robust testing of Flink-Kerberos integration on an actual YARN cluster, it is advisable to incorporate comprehensive end-to-end tests. As a potential solution, exploring the utilization of Docker Compose to orchestrate a Hadoop cluster with Kerberos in a Dockerized environment and running Flink on that cluster can facilitate the development of robust end-to-end tests.""";test_debt
"""It is worth mentioning that the current test suite does not effectively capture the specific bug being addressed (https://issues.apache.org/jira/browse/FLINK-8286). In order to ensure rigorous testing of Flink-Kerberos integration on a real YARN cluster, it may be necessary to introduce dedicated end-to-end tests. One potential avenue to explore involves leveraging Docker Compose to establish a Hadoop cluster with Kerberos within a Docker environment and then running Flink on that cluster as part of a comprehensive end-to-end test suite.""";test_debt
"""Additionally, it should be noted that the existing test coverage does not fully expose the underlying bug being fixed (https://issues.apache.org/jira/browse/FLINK-8286). To ensure robust validation of Flink-Kerberos integration on an authentic YARN cluster, it is advisable to incorporate proper end-to-end tests. A potential approach being considered involves utilizing Docker Compose to deploy a Hadoop cluster with Kerberos in a Dockerized environment and executing Flink on that cluster for comprehensive end-to-end testing purposes.""";test_debt
"""Furthermore, it is important to highlight that the current test suite does not adequately capture the bug in question (https://issues.apache.org/jira/browse/FLINK-8286). To achieve thorough testing of Flink-Kerberos integration on an actual YARN cluster, it is essential to establish comprehensive end-to-end tests. One potential strategy involves leveraging Docker Compose to set up a Hadoop cluster with Kerberos in a Docker environment, enabling the execution of Flink on that cluster as part of a comprehensive end-to-end testing framework.""";test_debt
"""It is important to note that the existing test suite does not fully reproduce the bug that is being addressed (https://issues.apache.org/jira/browse/FLINK-8286). To ensure comprehensive testing of Flink-Kerberos integration on a genuine YARN cluster, it is recommended to develop dedicated end-to-end tests. Exploring the use of Docker Compose to create a Hadoop cluster with Kerberos in a Docker environment and executing Flink on that cluster can facilitate the development of robust end-to-end tests.""";test_debt
Fixing flaky tests in PortTelnetHandlerTest;test_debt
"1. ""Resolved flaky test issues in PortTelnetHandlerTest.""";test_debt
"2. ""Fixed intermittent test failures in PortTelnetHandlerTest.""";test_debt
"3. ""Addressed flakiness in PortTelnetHandlerTest by implementing necessary fixes.""";test_debt
"4. ""Fixed recurring test failures in PortTelnetHandlerTest by resolving flaky test scenarios.""";test_debt
"5. ""Resolved intermittent failures in PortTelnetHandlerTest to ensure stable test execution.""";test_debt
"6. ""Fixed flaky tests in PortTelnetHandlerTest by implementing robust test handling.""";test_debt
NIFI-5790 removed the last test as it's causing a race condition intermittently;test_debt
"""Removed the last test in NIFI-5790 due to intermittent race condition issues.""";test_debt
"""Addressed the intermittent race condition by removing the last test in NIFI-5790.""";test_debt
"""Resolved the race condition problem by removing the final test in NIFI-5790.""";test_debt
"""Fixed the intermittent race condition by excluding the last test in NIFI-5790.""";test_debt
"""Removed the problematic test due to intermittent race condition in NIFI-5790.""";test_debt
"""Addressed the race condition issue by eliminating the last test in NIFI-5790.""";test_debt
"#1682: Enhance the test coverage part-4 : dubbo-common/src/main/java/com/alibaba/dubbo/common/status(store|threadpoolutils) modules
dubbo-common/src/test/java/com/alibaba/dubbo/common/utils/UrlUtilsTest.java
XXXXX";test_debt
"""#1682: Test Coverage Enhancement - Part 4: Improvements in the `store` and `threadpoolutils` modules of `dubbo-common` package.
Modified files:
- dubbo-common/src/main/java/com/alibaba/dubbo/common/status/store
- dubbo-common/src/main/java/com/alibaba/dubbo/common/status/threadpoolutils
- dubbo-common/src/test/java/com/alibaba/dubbo/common/utils/UrlUtilsTest.java
XXXXX""";test_debt
"""#1682: Test Coverage Enhancement - Part 4: Improvements in the com.alibaba.dubbo.common.status.store and com.alibaba.dubbo.common.threadpoolutils modules in dubbo-common.
Updated UrlUtilsTest.java in dubbo-common test package.
XXXXX""";test_debt
"""#1682: Test Coverage Enhancement - Part 4: Improvements made in the com.alibaba.dubbo.common.status.store and com.alibaba.dubbo.common.threadpoolutils modules in dubbo-common.
Revised UrlUtilsTest.java in the dubbo-common test package.
XXXXX""";test_debt
"""#1682: Enhancing Test Coverage - Part 4: Enhancements implemented in the com.alibaba.dubbo.common.status.store and com.alibaba.dubbo.common.threadpoolutils modules of dubbo-common.
Updated UrlUtilsTest.java within the dubbo-common test package.
XXXXX""";test_debt
"""#1682: Test Coverage Enhancement - Part 4: Expanded test coverage in the com.alibaba.dubbo.common.status.store and com.alibaba.dubbo.common.threadpoolutils modules of dubbo-common.
Modified UrlUtilsTest.java in the dubbo-common test package.
XXXXX""";test_debt
"""#1682: Enhancing Test Coverage - Part 4: Strengthened test coverage for the com.alibaba.dubbo.common.status.store and com.alibaba.dubbo.common.threadpoolutils modules in dubbo-common.
Updated the UrlUtilsTest.java file in the dubbo-common test package.
XXXXX""";test_debt
Can you add some tests around early publishing and loading sequence data from disk?;test_debt
It would be valuable to include tests that cover early publishing and loading sequence data from disk. This will help validate the functionality and ensure correct behavior.;test_debt
Adding tests that focus on early publishing and loading sequence data from disk is recommended. This will provide confidence in the implementation and verify the expected outcomes.;test_debt
Including tests specifically designed to cover early publishing and loading sequence data from disk is important. This will enable thorough validation of the functionality and identify any potential issues.;test_debt
Tests that encompass early publishing and loading sequence data from disk would be beneficial. This will ensure that the desired behavior is achieved and that the implementation is reliable.;test_debt
It is advisable to have tests that specifically target early publishing and loading sequence data from disk. This will allow for comprehensive testing and verification of the expected results.;test_debt
Including tests that focus on early publishing and loading sequence data from disk is crucial. This will help detect any potential issues and ensure the correctness of the implementation.;test_debt
"minor: suggest to change the PR (commit) title to ""Replace Thread.sleep() with TestHelper.verify() to fix the flaky unit tests"".";test_debt
"I recommend updating the PR (commit) title to ""Fix flaky unit tests by replacing Thread.sleep() with TestHelper.verify()"". This revised title clearly highlights the purpose of the change and the desired outcome, emphasizing the resolution of flaky unit tests by replacing the use of Thread.sleep() with TestHelper.verify().";test_debt
"I suggest changing the title of the pull request (commit) to ""Replacing Thread.sleep() with TestHelper.verify() to address flaky unit tests."" This updated title accurately reflects the purpose of the pull request and highlights the specific action taken to resolve the issue of flaky unit tests.";test_debt
"I suggest changing the title of the PR (commit) to ""Fix flaky unit tests by replacing Thread.sleep() with TestHelper.verify()"". This new title accurately reflects the purpose of the commit and highlights the specific improvement made to address the flaky unit tests.";test_debt
"I suggest updating the PR (commit) title to ""Fix flaky unit tests by replacing Thread.sleep() with TestHelper.verify()"". This new title accurately reflects the purpose of the commit and highlights the specific improvement made to address the flakiness in the unit tests.";test_debt
"I suggest updating the PR (commit) title to ""Replace Thread.sleep() with TestHelper.verify() to resolve flaky unit tests."" This revised title accurately reflects the purpose of the PR and highlights the specific change made to address the flakiness in the unit tests.";test_debt
"I recommend updating the title of the PR (commit) to ""Fix flaky unit tests by replacing Thread.sleep() with TestHelper.verify()"". This title accurately reflects the purpose of the commit and highlights the specific improvement made to address the flakiness of the unit tests.";test_debt
Seems to be no test for the Decimal precision mapping. That may be why the issue mentioned in the code slipped through: there is no unit test to catch it...;test_debt
1. It appears that there is no existing test covering the Decimal precision mapping. Consequently, the issue mentioned in the code might have gone unnoticed due to the lack of a unit test to detect it.;test_debt
2. The absence of a test specifically targeting the Decimal precision mapping suggests that the issue mentioned in the code might have slipped through without being caught by a unit test.;test_debt
3. The lack of test coverage for the Decimal precision mapping raises concerns regarding the oversight of the issue mentioned in the code, as there was no unit test to identify it.;test_debt
4. The absence of a unit test for the Decimal precision mapping potentially contributed to the issue mentioned in the code slipping through undetected.;test_debt
5. Without a dedicated unit test to cover the Decimal precision mapping, it becomes challenging to identify and address issues like the one mentioned in the code.;test_debt
6. The missing test for the Decimal precision mapping leaves room for potential issues to slip through undetected, as there is no unit test to catch them.;test_debt
"1. the current set of integration tests doesnt test the new functionality added in this PR. Its merely a check to see nothing else is broken.
2. when did we freeze? I skimmed through the mails didnt see anything about master frozen. I do not see any blocker defects for 4.10.0.0. The latest understanding I have is anybody can merge with required code LGTMs and a BVT run.
3. Which smoke tests are broken? Is it you environment issue you are talking about? If not, are these new or did we release 4.9.0 with these broken tests/features?";test_debt
"1. The current set of integration tests does not cover the new functionality introduced in this pull request. It primarily serves as a validation to ensure that no other issues have been introduced.
2. It is unclear when the freeze period began. I reviewed the emails but did not find any explicit mention of the master branch being frozen. Moreover, there are no critical defects reported for version 4.10.0.0. As far as I know, anyone can merge changes with the required code review approvals and successfully run basic verification tests (BVT).
3. It is important to clarify which smoke tests are broken. If you are referring to issues encountered in your specific environment, it may be related to the environment setup. However, if these broken tests or features are not isolated incidents and have been present since the 4.9.0 release, it raises concerns about their stability and reliability.";test_debt
"1. The current set of integration tests does not provide coverage for the new functionality introduced in this pull request. The tests primarily serve as a check to ensure that no other issues are introduced or existing functionality is broken.
2. Could you please clarify when the freeze occurred? I reviewed the emails but did not come across any specific mention of the master branch being frozen. As of now, I am not aware of any critical defects that would block the release of version 4.10.0.0. Based on the latest information available, it appears that merging code with appropriate LGTMs and conducting a BVT (Build Verification Test) run is the current process.
3. I would appreciate more details regarding the smoke tests that are reported as broken. Are these issues specific to your environment? If not, are these newly discovered problems, or were they present in the 4.9.0 release?";test_debt
"1. The current set of integration tests does not cover the new functionality introduced in this pull request. It primarily serves as a check to ensure that no other issues have been introduced.
2. Could you please clarify when the freeze occurred? I reviewed the emails, but I did not come across any communication indicating that the master branch was frozen. Additionally, I did not find any critical defects blocking the release of version 4.10.0.0. As per my understanding, anyone can merge changes with the necessary code review approvals and subsequent successful BVT (Build Verification Test) runs.
3. Could you provide more information about the specific smoke tests that are broken? If these issues are not related to your environment, were they present in the previous release (4.9.0)? It would be helpful to understand whether these broken tests or features are new or if they were already present in the previous release.";test_debt
"1. The existing set of integration tests does not provide coverage for the new functionality introduced in this pull request. Its purpose is primarily to ensure that no other issues arise.
2. I couldn't find any information about a freeze period in the recent emails. From my understanding, there are no blocking defects for the 4.10.0.0 release. As per the latest information, anyone with the necessary code review approvals and a successful BVT run can merge their changes.
3. Could you please provide more details about the smoke tests that are broken? If this is related to your local environment, it could be an isolated issue. If not, it's important to determine if these are new issues or if they were present in the previous 4.9.0 release.";test_debt
"1. The current set of integration tests does not cover the new functionality introduced in this PR. It primarily serves as a validation to ensure that no other issues are introduced.
2. I couldn't find any information regarding a freeze in the recent emails. As far as I know, there are no blocker defects for the 4.10.0.0 release. The current understanding is that anyone can merge their code with the necessary code review approvals and pass the basic verification tests (BVT).
3. Could you please provide more details about the specific smoke tests that are broken? If these issues are not related to your environment, it would be helpful to know if these problems are new or if they existed in the 4.9.0 release.";test_debt
"1. The current set of integration tests does not include coverage for the new functionality introduced in this pull request. Instead, it serves as a validation to ensure that no other issues have been introduced or existing features have been broken.
2. I couldn't find any information regarding a freeze period for the codebase. After going through the emails, I did not come across any discussions or announcements indicating a freeze. It seems that as long as the code has the necessary code review approvals and passes the basic verification tests (BVT), it can be merged.
3. Could you please provide more details about the specific smoke tests that are broken? If these are new issues, it would be helpful to understand if they are related to your local environment or if they were present in the 4.9.0 release. Clarifying these points will aid in addressing and resolving the broken tests or features effectively.";test_debt
ignite-11261: [ML] Flaky test(testNaiveBaggingLogRegression);test_debt
The test `testNaiveBaggingLogRegression` in the [ML] module is exhibiting flakiness, causing it to fail intermittently.;test_debt
The `testNaiveBaggingLogRegression` test in the [ML] module is experiencing flakiness, resulting in inconsistent test results.;test_debt
Flakiness has been observed in the `testNaiveBaggingLogRegression` test of the [ML] module, causing intermittent failures.;test_debt
The `testNaiveBaggingLogRegression` test in the [ML] module is unreliable and prone to intermittent failures.;test_debt
The [ML] module's `testNaiveBaggingLogRegression` test is encountering flakiness, leading to sporadic test failures.;test_debt
Flakiness has been detected in the `testNaiveBaggingLogRegression` test within the [ML] module, impacting the reliability of the test suite.;test_debt
I'd also like to see a test for leading space;test_debt
Adding a test case to cover leading spaces would be valuable in this context.;test_debt
It would be beneficial to include a test specifically addressing leading spaces.;test_debt
Consider adding a test scenario that focuses on leading space handling.;test_debt
Enhancing the test suite with a case that examines leading spaces would be beneficial.;test_debt
Including a test for leading space would provide comprehensive coverage of the functionality.;test_debt
To ensure thorough testing, consider incorporating a test case that verifies the behavior with leading spaces.;test_debt
"The test in
org.apache.camel.quarkus.core.CoreTest#testLookupRoutes
has a TODO as the 2nd route from RouteBuilderConfigurer is not discovered.
It uses @Produces annotation from JEE but Camel cannot discover it. Not sure how we can make this possible.
The regular RouteBuilder classes are discovered via jandex index and added during recorder magic.
I would assume a @Produces annotation from CDI/JEE would also work. But since the bean is not injected somewhere then arc may not trigger it. So maybe we need some jandex magic to discover all methods that returns a RouteBuilderConfigurer and are annotated with @Produces should then record the method, or whatever needs to be done.";test_debt
"The test located at `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` currently has a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. The test uses the `@Produces` annotation from JEE, but Camel is unable to automatically detect it. It is unclear how we can resolve this issue and make it possible for Camel to discover the annotated bean.

Typically, the regular `RouteBuilder` classes are discovered through the jandex index and added during the recorder magic. It is expected that a `@Produces` annotation from CDI/JEE would also work in a similar manner. However, since the bean is not being injected elsewhere, Arc may not trigger the discovery process. As a potential solution, we might need to employ some jandex magic to discover all methods that return a `RouteBuilderConfigurer` and are annotated with `@Produces`, and then record the method or perform any other necessary actions.";test_debt
"In the `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` test, there is a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. The issue arises from using the `@Produces` annotation from JEE, which Camel is unable to detect. It is unclear how we can enable this behavior.

Typically, the regular `RouteBuilder` classes are discovered through the jandex index and added during the recorder process. It is expected that the `@Produces` annotation from CDI/JEE would work in a similar manner. However, since the bean is not injected anywhere, Arc may not trigger the discovery process. Therefore, it may be necessary to implement some jandex magic to identify all methods that return a `RouteBuilderConfigurer` and are annotated with `@Produces`. This would allow the necessary recording or any other required actions to be performed.";test_debt
"The test `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` currently includes a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. This is due to the use of the `@Produces` annotation from JEE, which Camel is unable to detect. It is unclear how we can enable this functionality.

While regular `RouteBuilder` classes are discovered through the jandex index and added during the recorder process, it is assumed that an `@Produces` annotation from CDI/JEE would have a similar effect. However, since the bean is not injected anywhere, Arc may not trigger its discovery. Therefore, it may be necessary to utilize some jandex magic to identify all methods that return a `RouteBuilderConfigurer` and are annotated with `@Produces`. These methods should then be recorded or processed accordingly.";test_debt
"The test `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` currently has a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. This issue arises because Camel is unable to detect the route due to the usage of the `@Produces` annotation from JEE. Finding a solution to this problem is challenging as it is unclear how to enable Camel to discover routes defined with the `@Produces` annotation.

In the regular `RouteBuilder` classes, the routes are discovered through the jandex index and added during the recorder process. It is expected that the `@Produces` annotation from CDI/JEE would work similarly. However, since the bean is not injected elsewhere, the Arc container may not trigger its discovery. As a result, we may need to employ some jandex-related techniques to identify all methods annotated with `@Produces` that return a `RouteBuilderConfigurer`, and subsequently record those methods or take the necessary steps to address this issue.";test_debt
"The test `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` has a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. The issue arises from the use of the `@Produces` annotation from JEE, which Camel is unable to discover. It is uncertain how we can address this limitation and make it possible for Camel to recognize the `@Produces` annotation.

While regular `RouteBuilder` classes are discovered through the Jandex index and added during the recorder process, it is expected that the `@Produces` annotation from CDI/JEE should also work. However, since the bean is not injected elsewhere, Arc may not trigger its discovery. As a potential solution, it may be necessary to employ Jandex magic to detect all methods that return a `RouteBuilderConfigurer` and are annotated with `@Produces`, and then record the method or perform any required actions to ensure proper functionality.";test_debt
"The test `org.apache.camel.quarkus.core.CoreTest#testLookupRoutes` currently has a TODO because the second route from `RouteBuilderConfigurer` is not being discovered. This is due to the usage of the `@Produces` annotation from JEE, which Camel is unable to detect. Finding a solution to make this possible is not clear at the moment.

The regular `RouteBuilder` classes are discovered through the Jandex index and added during the recorder magic process. It is expected that the `@Produces` annotation from CDI/JEE would work in a similar way. However, since the bean is not injected anywhere, Arc may not trigger its discovery. Therefore, it may be necessary to implement some Jandex magic to discover all methods that return a `RouteBuilderConfigurer` and are annotated with `@Produces`. This would ensure that the method is properly recorded or handled as required.";test_debt
We should at least add a test case for this new option;test_debt
It would be advisable to include a test case specifically for this new option to ensure its functionality is validated.;test_debt
Adding a dedicated test case for this new option would be beneficial to verify its behavior and ensure proper functionality.;test_debt
It is recommended to incorporate a test case that focuses on testing this new option, as it will help confirm its expected behavior.;test_debt
To ensure the correctness of this new option, it is important to include a test case that covers its functionality.;test_debt
A test case dedicated to testing this new option should be added to ensure it functions as intended.;test_debt
Including a test case specifically for this new option would provide an opportunity to validate its behavior and ensure its proper functioning.;test_debt
Add a UT test for these coders. For example, we can add a test in `pyflink/fn_execution/tests`.;test_debt
It would be beneficial to add a unit test for these coders. For instance, we can consider including a test in the pyflink/fn_execution/tests directory.;test_debt
Adding a unit test for these coders is recommended. One possibility is to incorporate a test in the pyflink/fn_execution/tests module.;test_debt
It is advisable to include a unit test specifically for these coders. One option is to introduce a test case within the pyflink/fn_execution/tests directory.;test_debt
To ensure the correctness of these coders, it would be valuable to incorporate a unit test. Consider adding a test in the pyflink/fn_execution/tests module.;test_debt
Including a unit test for these coders is essential. One approach is to implement a test case in the pyflink/fn_execution/tests directory.;test_debt
It is highly recommended to have a unit test dedicated to these coders. Consider adding a test within the pyflink/fn_execution/tests module.;test_debt
"Need to explain this algorithm more thoroughly around what is done with sequences specifically.
Also unit tests focusing specifically on this algorithm, to strongly characterize proper behavior and insure full coverage.";test_debt
"The algorithm's explanation needs to be more comprehensive, particularly regarding its handling of sequences.
Additionally, we require dedicated unit tests that focus specifically on this algorithm to effectively validate its expected behavior and ensure comprehensive coverage.";test_debt
It is necessary to provide a more detailed explanation of the algorithm, specifically highlighting its implementation for sequences. Moreover, we must develop targeted unit tests that specifically assess the algorithm's functionality to ensure robustness and complete test coverage.;test_debt
We should enhance the algorithm's explanation, particularly emphasizing its utilization for sequences. Furthermore, we need to create specialized unit tests that concentrate on this algorithm to guarantee accurate behavior and achieve thorough test coverage.;test_debt
The algorithm's documentation needs to be expanded, specifically outlining its operations with sequences. Additionally, we need to design dedicated unit tests that focus exclusively on validating this algorithm's functionality, ensuring comprehensive coverage.;test_debt
The explanation of the algorithm needs to be more detailed, especially regarding its treatment of sequences. Furthermore, it is crucial to develop specific unit tests that extensively cover this algorithm, verifying its intended behavior and achieving comprehensive test coverage.;test_debt
We require a more thorough explanation of the algorithm, specifically emphasizing its handling of sequences. Additionally, we must create dedicated unit tests that solely target this algorithm, thoroughly examining its behavior and achieving full test coverage.;test_debt
"Fix flaky test cases in `WorkerTest` by mocking the `ExecutorService` in `Worker`.  Previously, when using a real thread pool executor, the task may or may not have been run by the executor until the end of the test.
Related JIRA issues:
Ran all tests (`./gradlew test`).
Ran unit tests in `connect/runtime` repeatedly.";test_debt
"Address the issue of unreliable test cases in WorkerTest by introducing a mocking mechanism for the ExecutorService within the Worker class. In the previous implementation, when utilizing an actual thread pool executor, there was uncertainty regarding whether the task would be executed by the executor before the test concluded.
Related JIRA issues:
Executed all tests (./gradlew test).
Performed iterative runs of the unit tests in the connect/runtime module.";test_debt
"Resolve the problem of intermittent test failures in WorkerTest by implementing a mock for the ExecutorService within the Worker class. Previously, when using an authentic thread pool executor, it was uncertain whether the executor would complete the task before the test finished.
Related JIRA issues:
Completed execution of all tests (./gradlew test).
Repeatedly executed the unit tests in the connect/runtime module.";test_debt
"Rectify the issue of unstable test cases in WorkerTest by introducing a mocking mechanism for the ExecutorService in the Worker class. In the previous implementation, there was uncertainty regarding the completion of the task when employing an actual thread pool executor during testing.
Related JIRA issues:
Successfully executed all tests (./gradlew test).
Conducted repeated runs of the unit tests in the connect/runtime module.";test_debt
"Resolve the flaky test case scenarios in WorkerTest by incorporating a mock for the ExecutorService within the Worker class. Previously, due to the usage of a real thread pool executor, it was indeterminate whether the executor would finish executing the task before the test concluded.
Related JIRA issues:
Executed all tests successfully (./gradlew test).
Performed repeated runs of the unit tests in the connect/runtime module.";test_debt
"Fix the issue of unreliable test cases in WorkerTest by employing a mocking technique for the ExecutorService within the Worker class. Previously, when utilizing an actual thread pool executor, there was uncertainty about whether the executor would complete the task before the test ended.
Related JIRA issues:
Successfully ran all tests (./gradlew test).
Repeatedly executed the unit tests in the connect/runtime module.";test_debt
"Address the problem of flaky test cases in WorkerTest by introducing a mock implementation for the ExecutorService in the Worker class. In the previous implementation, there was uncertainty regarding the task's execution by the actual thread pool executor before the test concluded.
Related JIRA issues:
Completed execution of all tests (./gradlew test).
Repeatedly executed the unit tests in the connect/runtime module.";test_debt
@yasserzamani I thinks because you added more conditional branches in the latest commit and that is why less LOC are covered;test_debt
@yasserzamani, I believe the increase in conditional branches in your recent commit is resulting in lower line of code (LOC) coverage.;test_debt
@yasserzamani, I think the addition of additional conditional branches in your latest commit has caused a decrease in the coverage of lines of code (LOC).;test_debt
@yasserzamani, it seems that the inclusion of extra conditional branches in your most recent commit has led to a reduction in the coverage of lines of code (LOC).;test_debt
@yasserzamani, based on my observation, the introduction of more conditional branches in your latest commit is causing a decrease in the coverage of lines of code (LOC).;test_debt
@yasserzamani, from my analysis, it appears that the incorporation of additional conditional branches in your recent commit has resulted in a lower coverage of lines of code (LOC).;test_debt
@yasserzamani, it appears that the increase in conditional branches in your latest commit is leading to a decrease in the coverage of lines of code (LOC).;test_debt
@ijuma Do you mean adding it to the comments or update the ticket description? I did not test Java 1.7 CRC32 performance. I only tested the one we used to use and the CRC32 performance in Java 1.8.;test_debt
@ijuma, are you suggesting to add the information in the comments or update the ticket description? I didn't perform any performance tests for Java 1.7 CRC32. My testing was limited to the previous version we were using and the CRC32 performance in Java 1.8.;test_debt
@ijuma, are you referring to adding this information in the comments or modifying the ticket description? Just to clarify, I haven't conducted any performance tests specifically for Java 1.7 CRC32. My testing was focused on the previous version we utilized and the performance of CRC32 in Java 1.8.;test_debt
@ijuma, are you recommending adding this information to the comments or updating the ticket description? It's important to note that I haven't conducted any performance tests specifically for Java 1.7 CRC32. My testing was solely focused on the previous version we used and the performance of CRC32 in Java 1.8.;test_debt
@ijuma, are you suggesting adding this information as a comment or updating the ticket description? Just to clarify, I didn't perform any performance tests specifically for Java 1.7 CRC32. My testing was centered around the previous version we employed and the performance of CRC32 in Java 1.8.;test_debt
@ijuma, are you proposing to include this information in the comments or update the ticket description? I want to clarify that I haven't conducted any performance tests specifically for Java 1.7 CRC32. My testing was solely focused on the previous version we were using and the performance of CRC32 in Java 1.8.;test_debt
@ijuma, are you recommending adding this information in the comments or updating the ticket description? Just to clarify, I did not perform any performance tests specifically for Java 1.7 CRC32. My testing was solely focused on the previous version we utilized and the performance of CRC32 in Java 1.8.;test_debt
Will add more test cases to cover complex dag.;test_debt
I will incorporate additional test cases to provide coverage for complex DAGs.;test_debt
I intend to add more test cases to ensure comprehensive coverage of complex DAGs.;test_debt
To adequately cover complex DAGs, I will be including additional test cases.;test_debt
I plan to enhance the test coverage by introducing more test cases that specifically address complex DAGs.;test_debt
Additional test cases will be implemented to achieve thorough coverage of intricate DAGs.;test_debt
I will augment the test suite by incorporating more test cases that specifically target complex DAG scenarios.;test_debt
"using the link on the failed GitHub CI job (the one with label ""Details""), you can select ""Rerun Jobs"". But that would re-run all GitHub based CI jobs for this PR, and that potentially can cause more flaky tests. So I think this is good enough.";test_debt
"Utilizing the link provided on the failed GitHub CI job (labeled ""Details""), you have the option to select ""Rerun Jobs."" However, it's important to note that this will rerun all GitHub-based CI jobs for this pull request, which may potentially lead to additional flaky tests. Considering this, I believe the current state is satisfactory.";test_debt
"By accessing the link associated with the failed GitHub CI job (labeled ""Details""), you can choose to ""Rerun Jobs."" It is crucial to be aware that this action will rerun all GitHub-based CI jobs for this pull request, which might result in more flaky tests. Taking this into consideration, I believe the current situation is acceptable.";test_debt
"Clicking on the link provided for the failed GitHub CI job (labeled ""Details"") allows you to select ""Rerun Jobs."" However, it's important to keep in mind that this will rerun all GitHub-based CI jobs for this pull request, potentially leading to additional instances of flaky tests. Given this possibility, I believe the current state is satisfactory.";test_debt
"By accessing the link provided for the failed GitHub CI job (labeled ""Details""), you have the option to choose ""Rerun Jobs."" However, please note that this action will rerun all GitHub-based CI jobs associated with this pull request, which may introduce further instances of flaky tests. Taking this into consideration, I believe the current state is adequate.";test_debt
"Through the link provided for the failed GitHub CI job (labeled ""Details""), you can opt to ""Rerun Jobs."" However, it's essential to be aware that this will rerun all GitHub-based CI jobs for this pull request, potentially resulting in more occurrences of flaky tests. Considering this, I believe the current state is satisfactory.";test_debt
"Clicking on the link associated with the failed GitHub CI job (labeled ""Details"") enables you to select ""Rerun Jobs."" Nevertheless, it is important to note that this action will rerun all GitHub-based CI jobs for this pull request, which may lead to additional instances of flaky tests. With this in mind, I believe the current situation is acceptable.";test_debt
I was thinking for the good test, if somehow there could be leaking across connections.;test_debt
In terms of comprehensive testing, it would be beneficial to investigate potential instances of connection leaking across connections.;test_debt
To ensure thorough testing, it would be advantageous to examine any possible occurrence of connection leaking across connections.;test_debt
As part of robust testing, it would be valuable to explore the possibility of connection leaking across connections.;test_debt
In order to conduct a comprehensive test, it would be worthwhile to consider the potential for connection leaking across connections.;test_debt
For a more comprehensive testing approach, it would be prudent to investigate the potential for connection leaking across connections.;test_debt
In the interest of thorough testing, it would be beneficial to analyze any potential occurrence of connection leaking across connections.;test_debt
Sorry I wasn't being clear, but my point is that it doesn't fail all the time, and when it does fail it delays other unrelated patches being merged. This is why we ignored it for now. We will reenable it later before we actually ship the release but we need to find a way to actually fix the flakiness before we do that.;test_debt
I apologize for the confusion. My point is that the issue doesn't occur consistently, and when it does happen, it causes delays in merging unrelated patches. That's why we decided to temporarily disregard it. We plan to reactivate it before the actual release, but we need to find a solution to address the flakiness before doing so.;test_debt
I apologize for any misunderstanding. What I meant is that the problem doesn't manifest consistently. However, when it does occur, it hampers the timely merging of unrelated patches. This is the reason we chose to overlook it for now. We intend to reenable it closer to the release date, but we must first find a resolution to the flakiness issue.;test_debt
I'm sorry for the lack of clarity. The issue doesn't occur consistently, and when it does, it creates delays in merging unrelated patches. That's why we decided to set it aside temporarily. We plan to reenable it prior to the release, but we need to find a way to address the flakiness before proceeding.;test_debt
I apologize for any confusion caused. The problem doesn't consistently cause failures, but when it does, it introduces delays in merging unrelated patches. Consequently, we made the decision to temporarily ignore it. However, we do intend to reactivate it before the release, but we must first find a solution to resolve the flakiness issue.;test_debt
I apologize for the unclear explanation. The issue doesn't consistently result in failures, but when it does, it causes delays in merging unrelated patches. Therefore, we made the decision to put it on hold for now. We do plan to reenable it before the actual release, but we need to find a way to address the flakiness before proceeding.;test_debt
I'm sorry for any misunderstanding. The problem doesn't occur consistently, but when it does, it leads to delays in merging unrelated patches. Hence, we chose to ignore it temporarily. However, we do intend to reenable it before the release, but we need to find a solution to fix the flakiness issue.;test_debt
"Currently, 1.6.x branch is failing on multiple pipelines
1. centos-cpu & centos-gpu pipelines due to 
http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline
2. Flaky test on unix-cpu mkldnn
3. Edge pipeline";test_debt
"At present, the 1.6.x branch is experiencing failures across multiple pipelines. Specifically, both the centos-cpu and centos-gpu pipelines are failing as indicated by the following link: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.

The 1.6.x branch is currently encountering failures in various pipelines. One notable issue is a flaky test within the unix-cpu mkldnn pipeline.

The 1.6.x branch is facing failures in multiple pipelines, including the Edge pipeline.";test_debt
"Multiple pipelines are failing in the 1.6.x branch. Specifically, the centos-cpu and centos-gpu pipelines are encountering failures, which can be observed from this link: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.

The 1.6.x branch is currently experiencing failures across different pipelines. Notably, there is an issue with the Edge pipeline.";test_debt
"At present, the 1.6.x branch is experiencing failures in multiple pipelines, including centos-cpu and centos-gpu. These failures can be attributed to the following link: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.

The 1.6.x branch is currently encountering issues in various pipelines. Specifically, there is a flaky test in the unix-cpu mkldnn pipeline.

The 1.6.x branch is facing failures in the Edge pipeline, in addition to the previously mentioned centos-cpu and centos-gpu pipelines. This can be observed through the following link: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.";test_debt
"Multiple pipelines, including centos-cpu and centos-gpu, are failing for the 1.6.x branch. The failures can be traced back to a specific link: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.

The 1.6.x branch is experiencing failures in several pipelines, notably the unix-cpu mkldnn pipeline due to a flaky test.";test_debt
"The 1.6.x branch is currently encountering failures in the Edge pipeline, in addition to the centos-cpu and centos-gpu pipelines. Additional information can be found at: http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fcentos-cpu/detail/v1.6.x/51/pipeline.

Multiple pipelines are currently failing for the 1.6.x branch. Specifically, there is a flaky test occurring in the unix-cpu mkldnn pipeline, and failures are also observed in the Edge pipeline.";test_debt
"At present, the 1.6.x branch is experiencing failures across multiple pipelines:
a. The centos-cpu and centos-gpu pipelines are failing due to an issue documented at [link to Jenkins log].
b. The unix-cpu mkldnn pipeline is encountering flaky test failures.
c. The Edge pipeline is also experiencing issues.";test_debt
Thank for the PR @HuangZhenQiu , the PR overall looks good, can you also add the cases for Blink planner ? Even though the code path looks the same, we still need the tests to make sure the logic works correctly.;test_debt
Thank you for the pull request, @HuangZhenQiu. Overall, the PR appears to be in good shape. Could you also include test cases for the Blink planner? Despite the similarity in code paths, it is important to have tests in place to ensure the correctness of the logic.;test_debt
Appreciate your pull request, @HuangZhenQiu. It looks promising overall. In addition to that, could you please incorporate test cases for the Blink planner as well? Even though the code path appears to be the same, having tests will help ensure the accuracy of the logic.;test_debt
Thank you, @HuangZhenQiu, for the pull request. Overall, it seems to be in good shape. However, I kindly request you to include test cases for the Blink planner as well. Despite the similarity in code paths, having tests will provide reassurance regarding the correctness of the logic.;test_debt
@HuangZhenQiu, thank you for the pull request. It is looking good overall. I would appreciate it if you could also add test cases for the Blink planner. Although the code path may appear identical, including tests will help verify the accuracy of the underlying logic.;test_debt
@HuangZhenQiu, thank you for the pull request. The overall quality looks great. However, I would like to request the addition of test cases for the Blink planner as well. Even though the code path seems to be the same, having tests in place will ensure the proper functioning of the logic.;test_debt
Thank you, @HuangZhenQiu, for the pull request. I'm pleased with the overall quality. Additionally, could you please incorporate test cases for the Blink planner? Despite the similarities in the code path, including tests is crucial to guarantee the correctness of the logic.;test_debt
CI is failing due to flaky test, for which I created ticket GEODE-7319, and proposed solution.;test_debt
The continuous integration (CI) is failing because of a flaky test, which prompted me to create ticket GEODE-7319. I have also proposed a solution to address the issue.;test_debt
Due to a flaky test causing failures in the continuous integration (CI), I have created ticket GEODE-7319 to track the problem. Additionally, I have put forth a proposed solution to resolve the issue.;test_debt
The continuous integration (CI) failures are being caused by a flaky test, leading me to open ticket GEODE-7319. I have provided a proposed solution to tackle this problem.;test_debt
A flaky test is responsible for the continuous integration (CI) failures, resulting in the creation of ticket GEODE-7319. I have outlined a proposed solution to rectify the issue.;test_debt
The failures in the continuous integration (CI) are attributable to a flaky test, prompting me to create ticket GEODE-7319. To address this problem, I have proposed a solution.;test_debt
I have created ticket GEODE-7319 to address the continuous integration (CI) failures caused by a flaky test. In order to resolve this issue, I have put forth a proposed solution.;test_debt
"Could you add tests for multiple file cases? Probably, I think you might be able to use `(new File(""/tmp/file.csv"")).setLastModified(xxx)` to control timestamp.";test_debt
"Could you please incorporate tests for multiple file cases? One possible approach could be to utilize (new File(""/tmp/file.csv"")).setLastModified(xxx) to control the timestamp.";test_debt
"It would be beneficial to include tests for scenarios involving multiple files. You might consider using (new File(""/tmp/file.csv"")).setLastModified(xxx) to manipulate the timestamp and control the behavior in these cases.";test_debt
"Adding tests for multiple file scenarios would be advantageous. A potential approach could involve using (new File(""/tmp/file.csv"")).setLastModified(xxx) to control the timestamp and ensure accurate test coverage.";test_debt
"Including tests for multiple file cases would be valuable. One way to approach this is by utilizing (new File(""/tmp/file.csv"")).setLastModified(xxx) to manipulate the timestamp and facilitate comprehensive testing.";test_debt
"It is recommended to incorporate tests that cover scenarios with multiple files. A suitable method to control the timestamp in these cases could be (new File(""/tmp/file.csv"")).setLastModified(xxx).";test_debt
"Adding tests for multiple file scenarios is highly recommended. To control the timestamp in these cases, you could utilize (new File(""/tmp/file.csv"")).setLastModified(xxx).";test_debt
"Worth adding test case(s).
Also note the related discussion the mailing list. An alternative suggestion is that we pick up _all_ files with the given name on the classpath (rather than all those in a given directory). I personally prefer the approach of all files in the directory. That would allow us to more easily incrementally add things (e.g. have separate files for upgrading between versions).
---
I wonder about a nicer package name than `org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames`. I imagine many people will just put this in their `./conf/` directory, so we don't want them to have to create a really deep nested directory.
---
Another thing we could add (in the future?) is if there are conflicting changes - e.g. A is renamed to B in the first file, and B is renamed to C in the second file. Currently, the result would depend on the other the files were processed: i.e. it could be ""B"" or ""C"". It would be good to be more predictable.
I'm fine with that being deferred for now.";test_debt
"It would be worthwhile to include test case(s) to ensure comprehensive coverage. Also, please take note of the related discussion on the mailing list. One alternative suggestion is to retrieve all files with the given name from the classpath, rather than limiting it to a specific directory. Personally, I prefer the approach of including all files within the directory, as it allows for incremental additions (such as having separate files for version upgrades). Additionally, it might be worth considering a more descriptive package name than org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames. Since many users will likely place this in their ./conf/ directory, we should avoid requiring them to create deeply nested directories. Another potential enhancement for the future could be handling conflicting changes, such as if A is renamed to B in the first file, and B is renamed to C in the second file. Currently, the result depends on the order in which the files are processed, resulting in either ""B"" or ""C"". It would be beneficial to introduce more predictability in this aspect. However, I agree that this can be deferred for now.";test_debt
"It is worth considering the addition of test case(s) to ensure comprehensive coverage. Furthermore, please make note of the ongoing discussion on the mailing list. One alternative suggestion is to retrieve all files with the specified name from the classpath, rather than restricting it to a specific directory. From a personal standpoint, I favor the approach of including all files within the directory, as it facilitates incremental updates (such as having separate files for version transitions). Additionally, it would be advantageous to explore alternative package naming options for org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames. Given that many users will likely place this in their ./conf/ directory, it would be preferable to avoid creating excessively nested directories. Another consideration for the future could involve addressing conflicting changes, such as when A is renamed to B in the first file and B is renamed to C in the second file. Currently, the outcome depends on the processing order of the files, resulting in either ""B"" or ""C"". It would be beneficial to introduce greater predictability in such cases. However, I agree that this can be deferred for now.";test_debt
"Including test case(s) would be worthwhile to achieve comprehensive coverage. Additionally, please take note of the ongoing discussion on the mailing list. An alternative suggestion is to retrieve all files with the specified name from the classpath, rather than being limited to a specific directory. Personally, I prefer the approach of including all files within the directory, as it allows for incremental additions (such as separate files for version upgrades). It might also be worth considering a more suitable package name than org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames. Considering that many users will likely place this in their ./conf/ directory, it would be preferable to avoid deep nesting of directories. Furthermore, for future improvements, it could be beneficial to address conflicting changes, such as when A is renamed to B in the first file and B is renamed to C in the second file. Currently, the result depends on the processing order of the files, potentially resulting in ""B"" or ""C"". It would be advantageous to introduce greater predictability in such scenarios. However, I agree that this can be deferred for now.";test_debt
"Adding test case(s) would be valuable to ensure comprehensive coverage. Furthermore, please take note of the ongoing discussion on the mailing list. One alternative suggestion is to retrieve all files with the given name from the classpath, instead of limiting it to a specific directory. Personally, I favor the approach of including all files within the directory, as it allows for incremental additions (e.g., separate files for version upgrades). It might be worth considering a more appropriate package name than org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames, keeping in mind that many users will likely place this in their ./conf/ directory and we want to avoid deep nesting of directories. Additionally, in the future, we could explore handling conflicting changes, such as when A is renamed to B in the first file and B is renamed to C in the second file. Currently, the outcome depends on the order in which the files are processed, potentially resulting in either ""B"" or ""C"". It would be beneficial to introduce more predictability in such cases. However, I agree that this can be deferred for now.";test_debt
"It would be worthwhile to add test case(s) for comprehensive coverage. Please also take note of the ongoing discussion on the mailing list. An alternative suggestion is to retrieve all files with the specified name from the classpath, rather than restricting it to a particular directory. From my perspective, I prefer the approach of including all files within the directory, as it allows for incremental additions (e.g., separate files for version upgrades). It might be worth considering a more suitable package name than org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames, considering that many users will likely place this in their ./conf/ directory and deep nesting of directories should be avoided. Furthermore, in the future, we could explore handling conflicting changes, such as when A is renamed to B in the first file and B is renamed to C in the second file. Currently, the outcome depends on the processing order of the files, potentially resulting in either ""B"" or ""C"". It would be advantageous to introduce more predictability in such situations. However, I agree that this can be deferred for now.";test_debt
"Including test case(s) would be beneficial to ensure comprehensive coverage. Additionally, please make note of the ongoing discussion on the mailing list. An alternative suggestion is to retrieve all files with the specified name from the classpath, rather than limiting it to a specific directory. From my perspective, I prefer the approach of including all files within the directory, as it allows for incremental additions (e.g., separate files for version upgrades). It might be worth considering a more descriptive package name than org.apache.brooklyn.core.mgmt.persist.deserializingClassRenames, especially considering that many users will likely place this in their ./conf/ directory and we want to avoid creating deeply nested directories. Furthermore, for future enhancements, we could explore handling conflicting changes, such as when A is renamed to B in the first file and B is renamed to C in the second file. Currently, the outcome depends on the processing order of the files, resulting in either ""B"" or ""C"". It would be advantageous to introduce more predictability in such cases. However, I agree that this can be deferred for now.";test_debt
[MXNET-367] update mkldnn to v0.14 and disable building test examples;test_debt
[MXNET-367] Upgrade mkldnn to version v0.14 and disable the build of test examples.;test_debt
[MXNET-367] Update mkldnn to v0.14 and modify the build configuration to disable building test examples.;test_debt
[MXNET-367] Upgrade mkldnn to version v0.14 and configure the build process to exclude the compilation of test examples.;test_debt
[MXNET-367] Update mkldnn to v0.14 and adjust the build settings to disable the compilation of test examples.;test_debt
[MXNET-367] Upgrade mkldnn to version v0.14 and modify the build procedure to skip the compilation of test examples.;test_debt
[MXNET-367] Update mkldnn to v0.14 and configure the build process to exclude the generation of test examples.;test_debt
test this please;test_debt
Please conduct a test on this request.;test_debt
Kindly perform a test to verify this, please.;test_debt
Could you please test this and provide feedback?;test_debt
I would appreciate it if you could test this and share the results.;test_debt
It would be helpful if you could conduct a test for this.;test_debt
Please execute a test for this and let me know the outcome.;test_debt
I think we don't need this test here.;test_debt
I believe it is unnecessary to include this particular test in this context.;test_debt
In my opinion, this test is not required for the current situation.;test_debt
Based on my assessment, there is no need to include this test here.;test_debt
I think we can omit this test for now in this scenario.;test_debt
Considering the circumstances, I don't believe we need to include this test at this point.;test_debt
After careful consideration, I have concluded that this test is not relevant in this context.;test_debt
It would be nice to allow the staging receives to complete and verify that the closing channel is also empty like in the other test.;test_debt
It would be beneficial to enable the staging receivers to complete and ensure that the closing channel is also empty, similar to the other test.;test_debt
It would be advantageous to let the staging receivers finish their execution and verify that the closing channel is empty, as done in the other test.;test_debt
It would be preferable to allow the staging receivers to complete and verify that the closing channel is empty, mirroring the approach taken in the other test.;test_debt
It would be a good idea to permit the staging receivers to finish their operations and validate that the closing channel is empty, similar to what is done in the other test.;test_debt
It would be valuable to observe the completion of the staging receivers and verify that the closing channel is empty, following the methodology employed in the other test.;test_debt
It would be desirable to let the staging receivers conclude their tasks and ensure that the closing channel is empty, as demonstrated in the other test.;test_debt
9769 test flakiness;test_debt
The test with ID 9769 is experiencing flakiness.;test_debt
Test 9769 is exhibiting inconsistent behavior and flakiness.;test_debt
There are issues of flakiness associated with test 9769.;test_debt
The reliability of test 9769 is compromised due to its flakiness.;test_debt
Test 9769 is prone to flakiness, causing unreliable results.;test_debt
Flakiness is affecting the consistency of test 9769.;test_debt
minor - no test coverage for lines 221 - 225 in the unit test;test_debt
There is a lack of test coverage for lines 221-225 in the unit test.;test_debt
The unit test is missing test coverage for lines 221 to 225.;test_debt
Lines 221-225 in the unit test are not covered by any tests.;test_debt
Test coverage is incomplete for lines 221-225 in the unit test.;test_debt
The unit test does not include tests for lines 221-225.;test_debt
Lines 221-225 in the unit test are not currently being tested.;test_debt
Good catch! Can you add a unit test for queue input stream? It could be in the InputStreamsSuite.scala.;test_debt
Well spotted! Could you please incorporate a unit test for the queue input stream? It can be included in the InputStreamsSuite.scala file.;test_debt
Great observation! It would be helpful if you could add a unit test for the queue input stream. You can include it in the InputStreamsSuite.scala file.;test_debt
Excellent find! Can you please include a unit test for the queue input stream? It can be added to the InputStreamsSuite.scala file.;test_debt
Impressive catch! It would be beneficial to include a unit test for the queue input stream. Consider adding it to the InputStreamsSuite.scala file.;test_debt
Nicely noticed! Can you augment the test suite with a unit test for the queue input stream? You can include it within the InputStreamsSuite.scala file.;test_debt
Good job identifying this! Please ensure that a unit test for the queue input stream is added. The ideal location for this test would be the InputStreamsSuite.scala file.;test_debt
Sure, makes sense. I've got the changes almost done - just need to clean up the unit tests.;test_debt
Understood. It sounds reasonable. I'm almost finished with the changes, and the only remaining task is to tidy up the unit tests.;test_debt
I agree. That sounds like a logical approach. I've almost completed the changes, and the only remaining task is to clean up the unit tests.;test_debt
Makes sense. I'm nearly finished with the changes, and the only thing left to do is to tidy up the unit tests.;test_debt
Got it. That sounds like a good plan. I'm close to finishing the changes, and the final step is to clean up the unit tests.;test_debt
I agree with you. It sounds like the right course of action. The changes are almost complete, and all that's left is to clean up the unit tests.;test_debt
Absolutely. It's important to clean up the unit tests as part of the process. I'm almost done with the changes, and that will be the final step.;test_debt
The test cases in `kvstore` are just unit test cases. We also need integration tests for ensuring they work as expected.;test_debt
The test cases in kvstore currently focus solely on unit testing. However, to ensure their expected functionality, it is necessary to supplement them with integration tests.;test_debt
While the existing test cases in kvstore are classified as unit tests, it is important to augment them with integration tests to validate their proper functionality.;test_debt
The current test cases in kvstore are limited to unit testing. To ensure their desired behavior, it is essential to introduce integration tests as well.;test_debt
The test cases in kvstore are currently confined to unit testing. To verify their functionality as intended, the addition of integration tests is necessary.;test_debt
Although the existing test cases in kvstore are categorized as unit tests, it is crucial to incorporate integration tests to validate their expected behavior.;test_debt
The current set of test cases in kvstore only covers unit testing. To ensure their correct operation, it is vital to include integration tests as well.;test_debt
"Classloading won't help, because we still need a separate JVM. When JVM loads classes, it trys in parent classloader first. If the class is already there it won't load. So we need at least one separate process.
I don't like to try many times each with separate JVM. Maybe only try once (like in the other test with codecs). It may not fail every time, but sometimes test fails.
I am also not sure if we really need a test for this. If we may get a static checker that finds classes that initialize their subclasses in their own static initializer, we can prevent similar cars in future.";test_debt
Using classloading alone won't suffice since a separate JVM is still required. When the JVM loads classes, it first checks the parent classloader. If the class is already loaded, it won't load it again. Therefore, we need at least one separate process. However, I prefer attempting the test only once, similar to the other test with codecs. Although the test may not fail consistently, there are instances when it does. Additionally, I'm uncertain if we truly need a test for this scenario. It is possible that with a static checker, we can identify classes that initialize their subclasses in their own static initializer, thus preventing similar issues in the future.;test_debt
Relying solely on classloading won't solve the problem because a separate JVM is still necessary. When classes are loaded by the JVM, it checks the parent classloader first. If the class is already loaded, it won't be loaded again. Therefore, we need a separate process at least once. However, I prefer attempting the test only once, similar to the test with codecs. Although the test may not fail consistently, there are cases where it does fail. Moreover, I'm uncertain if it is essential to have a test specifically for this scenario. It might be possible to leverage a static checker to identify classes that initialize their subclasses in their own static initializer, which would help prevent similar issues in the future.;test_debt
Merely relying on classloading won't address the issue because a separate JVM is still required. The JVM follows a hierarchy where it checks the parent classloader first when loading classes. If the class is already loaded, it won't be loaded again. Hence, we need to run at least one separate process. However, I prefer attempting the test only once, similar to the test with codecs. While the test may not consistently fail, there are instances where it does. Additionally, I'm unsure if it is truly necessary to have a test specifically for this case. It is worth considering the use of a static checker to identify classes that initialize their subclasses in their own static initializer, which would help prevent similar issues in the future.;test_debt
Classloading alone won't provide a solution because a separate JVM is still required. During class loading, the JVM checks the parent classloader first. If the class is already loaded, it won't be loaded again. Hence, we need a separate process at least once. However, I prefer attempting the test only once, similar to the codec test. Although the test may not consistently fail, there are instances where it does fail. Furthermore, I'm uncertain about the necessity of having a test specifically for this scenario. It might be beneficial to explore the use of a static checker that can detect classes initializing their subclasses in their own static initializer, which would help prevent similar issues in the future.;test_debt
Depending solely on classloading won't suffice because a separate JVM is still required. When classes are loaded by the JVM, it checks the parent classloader first. If the class is already loaded, it won't reload it. Therefore, we need to use a separate process at least once. However, I prefer attempting the test only once, similar to the codec test. Although the test may not fail consistently, there are cases when it does. Additionally, I'm unsure about the necessity of having a specific test for this situation. It might be worth considering the use of a static checker to identify classes that initialize their subclasses in their own static initializer, as this would help prevent similar issues from occurring in the future.;test_debt
Reliance solely on classloading will not resolve the issue because a separate JVM is still required. When classes are loaded by the JVM, it checks the parent classloader first. If the class is already present, it will not be loaded again. Hence, we need to utilize a separate process, at least once. However, I prefer attempting the test only once, similar to the other test involving codecs. Although the test may not consistently fail, there are occasions when it does. Furthermore, I'm uncertain if having a dedicated test for this scenario is truly necessary. It might be advantageous to explore the adoption of a static checker that can identify classes initializing their subclasses in their own static initializer, which would help prevent similar issues in the future.;test_debt
Also, if I'm correct, it would be good to include a test case for this case.  What happens if you actually do operations on the null value stored as an enum?  Is the nullability of the resulting schema correct. Do operations like `.isNotNull` work correctly?;test_debt
If I understand correctly, it would be beneficial to include a test case for this scenario. It would help ascertain the behavior when performing operations on a null value stored as an enum. Specifically, we can evaluate the correctness of nullability in the resulting schema and ensure that operations like .isNotNull function as expected.;test_debt
If my understanding is accurate, it would be advantageous to incorporate a test case to address this situation. By doing so, we can examine the behavior when executing operations on a null value stored as an enum. This will allow us to validate the correctness of the resulting schema's nullability and verify if operations like .isNotNull are functioning correctly.;test_debt
If I'm correct in my understanding, it would be wise to include a test case for this particular scenario. By doing so, we can assess the behavior when performing operations on a null value that is stored as an enum. Additionally, we can verify the accuracy of the resulting schema's nullability and evaluate the correct functioning of operations like .isNotNull.;test_debt
If my understanding is accurate, it would be beneficial to incorporate a test case to cover this specific scenario. This will enable us to observe the behavior when conducting operations on a null value stored as an enum. Furthermore, we can validate the correctness of the resulting schema's nullability and confirm whether operations such as .isNotNull are functioning correctly.;test_debt
If I comprehend correctly, it would be prudent to include a test case for this particular case. This will allow us to investigate the behavior when performing operations on a null value that is stored as an enum. Additionally, we can verify the correctness of the resulting schema's nullability and assess the proper functioning of operations like .isNotNull.;test_debt
If my understanding is correct, it would be advisable to include a test case specifically for this scenario. This will help us explore the behavior when executing operations on a null value stored as an enum. Furthermore, we can ascertain the accuracy of the resulting schema's nullability and evaluate the correct behavior of operations such as .isNotNull.;test_debt
See my comment in the issue, were you able to reproduce? Do you have some evidence to declare it as a flaky test?  Unless there's more data I would be against disabling this test with the information that we have so far to prevent lowering the quality bar.;test_debt
I have reviewed your comment in the issue. Were you able to reproduce the problem? Do you have any evidence to support classifying it as a flaky test? Without additional data, I would advise against disabling this test at the moment, as it could potentially compromise the quality standards.;test_debt
I have read your comment in the issue. Have you been able to reproduce the issue yourself? Is there any concrete evidence to support the classification of this test as flaky? Without further data, I would recommend not disabling the test based on the information available, as it could lead to a reduction in quality standards.;test_debt
I have gone through your comment in the issue. Have you successfully replicated the problem? Is there any solid evidence to support the claim that this test is flaky? Without additional information, I would suggest refraining from disabling the test to maintain the current quality standards.;test_debt
I have carefully reviewed your comment in the issue. Were you able to replicate the issue in question? Is there any substantial evidence available to classify this test as flaky? Without further data, I would advise against disabling the test based on the current information, as it could potentially lower the overall quality standards.;test_debt
I have examined your comment in the issue. Have you managed to reproduce the issue yourself? Do you have any conclusive evidence to support labeling this test as flaky? In the absence of additional data, I would recommend not disabling the test at this point to uphold the existing quality standards.;test_debt
I have taken note of your comment in the issue. Have you successfully reproduced the problem? Is there sufficient evidence to suggest that this test is flaky? Until more data is available, I would advise against disabling the test to prevent any potential compromise in the quality standards.;test_debt
It'd be really great to have unit tests for many of these methods. The `performTaskAssignment(...)` method is already pretty lengthy, and there are just a few unit tests whereas there seem to be lots of permutations and branches. Not only would they help with confidence, but they'd help with regression testing if/when we have to get back into this code.;test_debt
It would be highly beneficial to have unit tests for many of these methods. The performTaskAssignment(...) method is already quite long, and although there are a few existing unit tests, there are numerous permutations and branches that would benefit from additional test coverage. Not only would these tests provide confidence in the code, but they would also serve as effective regression tests for future engagements with this codebase.;test_debt
Having unit tests for many of these methods would be extremely valuable. The performTaskAssignment(...) method is already quite extensive, and while there are a few existing unit tests, there are numerous permutations and branches that warrant additional testing. These tests would not only boost confidence in the code but also serve as valuable regression tests for future maintenance of this codebase.;test_debt
It would be highly advantageous to include unit tests for many of these methods. Given the length of the performTaskAssignment(...) method and the presence of various permutations and branches, additional test coverage would be beneficial. These unit tests would enhance confidence in the code and provide valuable regression testing capabilities for future modifications to this codebase.;test_debt
Having unit tests for many of these methods would be of great value. Considering the length of the performTaskAssignment(...) method and the numerous permutations and branches involved, there is a need for additional test coverage. These unit tests would instill confidence in the code and facilitate regression testing as future work is conducted on this codebase.;test_debt
It would be highly desirable to incorporate unit tests for many of these methods. With the performTaskAssignment(...) method already being quite lengthy and containing numerous permutations and branches, additional test coverage is crucial. These unit tests would not only provide confidence in the code but also serve as effective regression tests for future maintenance efforts involving this codebase.;test_debt
Including unit tests for many of these methods would be greatly advantageous. Given the extensive nature of the performTaskAssignment(...) method, which encompasses various permutations and branches, there is a clear need for additional test coverage. These unit tests would foster confidence in the code and facilitate regression testing for future engagements with this codebase.;test_debt
"@erikdubbelboer ok that makes sense.
Can you please:
1. Add a comment to the code as to why this does not use `_`
2. Add unit tests to verify functionality.";test_debt
"""@erikdubbelboer, that explanation is clear. Could you please add a comment to the code clarifying why this does not use _?

Additionally, it would be beneficial to include unit tests to validate the functionality.";test_debt
"""@erikdubbelboer, I appreciate the clarification. To provide further context, could you add a comment to the code explaining why _ is not used?

Furthermore, it would be advantageous to incorporate unit tests to ensure the functionality is verified.";test_debt
"""@erikdubbelboer, your explanation makes sense. To enhance readability and understanding, could you please add a comment to the code explaining why _ is not utilized?

Moreover, it would be valuable to introduce unit tests to validate and confirm the expected functionality.";test_debt
"""@erikdubbelboer, I understand the reasoning behind this approach. To provide clarity and context for future developers, could you please add a comment to the code explaining the decision to not use _?

Additionally, it would be prudent to include unit tests that verify the functionality and ensure it meets the expected requirements.";test_debt
"""@erikdubbelboer, I appreciate the explanation. To improve code comprehension, could you please add a comment explaining why _ is not employed?

Furthermore, it would be wise to incorporate unit tests to verify the functionality and guarantee its correctness.";test_debt
"""@erikdubbelboer, thank you for the clarification. To enhance code understanding, please add a comment to the code elucidating the reason behind not using _.

Additionally, including unit tests to validate the functionality would be highly beneficial.";test_debt
"#4490 does not cover all the cases. While performing end-to-end tests with this snapshot, I found that traces were still not aggregating correctly. This PR fixes the problem.
I am not sure why the tests did not catch this issue, so unfortunately cannot provide better automatic coverage.
Below are the contribution guidelines:
https://github.com/apache/camel/blob/master/CONTRIBUTING.md";test_debt
"The issue addressed in #4490 was not able to cover all the cases. During end-to-end testing with this snapshot, it was discovered that traces were still not aggregating correctly. This pull request resolves the issue and fixes the problem. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"Although #4490 attempted to address the problem, it did not encompass all the necessary cases. During end-to-end testing using this snapshot, it became apparent that traces were still not aggregating correctly. This pull request addresses the issue and resolves the problem. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"The problem reported in #4490 was not fully resolved and did not cover all the relevant cases. After conducting end-to-end tests with this snapshot, it was identified that traces were still not aggregating correctly. This pull request rectifies the issue and fixes the problem. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"The issue described in #4490 was not comprehensive enough to cover all the scenarios. Subsequent end-to-end testing using this snapshot revealed that traces were still not being aggregated correctly. This pull request resolves the issue and resolves the problem. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"Despite the efforts made in #4490, the issue at hand was not fully resolved and did not encompass all the necessary cases. Upon conducting end-to-end tests with this snapshot, it was determined that traces were still not aggregating correctly. This pull request addresses the issue and provides the fix. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"The problem discussed in #4490 was not adequately addressed and did not encompass all the relevant cases. Upon performing end-to-end tests with this snapshot, it was observed that traces were still not being aggregated correctly. This pull request tackles the issue and resolves the problem. Regarding the contribution guidelines, you can find them here: https://github.com/apache/camel/blob/master/CONTRIBUTING.md""";test_debt
"Can you add comment saying why we have excluded these test cases?
Do we have custom tests for these somewhere?";test_debt
Could you please add a comment explaining the reason for excluding these test cases? Additionally, do we have any custom tests for these scenarios located elsewhere?;test_debt
It would be helpful to include a comment explaining the rationale behind excluding these test cases. Furthermore, do we have any dedicated custom tests for these particular scenarios?;test_debt
Adding a comment clarifying why these test cases have been excluded would be beneficial. Additionally, are there any specific custom tests that cover these scenarios?;test_debt
It is advisable to include a comment detailing the justification for excluding these test cases. Moreover, do we have any dedicated custom tests addressing these specific scenarios?;test_debt
Please consider adding a comment to elucidate the decision to exclude these test cases. Additionally, is there any existing suite of custom tests that covers these scenarios?;test_debt
It would be advantageous to have a comment explaining why these test cases have been excluded. Furthermore, are there any specialized custom tests designed to handle these scenarios?;test_debt
"We can get rid of WIN_GPU_MKLDNN tests altogether but that still leaves us with the flakiness of WIN_GPU as can be seen in these builds
For roughly same code of this PR & same windows AMI, below are the results so far
WIN_GPU | WIN_GPU_MKLDNN | Build Number | Link
-- | -- | -- | -- |
âœ–ï¸Ž| âœ”ï¸Ž|15 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/15/pipeline
âœ–ï¸Ž| âœ”ï¸Ž| 14 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/14/pipeline
âœ”ï¸Ž|âœ”ï¸Ž| 12 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/12/pipeline
âœ”ï¸Ž| âœ–ï¸Ž| 13 | http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/mxnet-validation%2Fwindows-gpu/detail/PR-17808/13/pipeline
Ofcourse your tests on local have a different story to tell...";test_debt
"Although we can eliminate the WIN_GPU_MKLDNN tests entirely, we still face the issue of flakiness with WIN_GPU, as evident in these builds. For the same codebase as this pull request and using the same Windows AMI, the following results have been observed so far:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It is worth noting that the local tests may provide different results.";test_debt
"Although we can consider removing the WIN_GPU_MKLDNN tests, we still face the issue of flakiness with WIN_GPU, as evidenced by the builds mentioned below. These builds correspond to the same codebase as this pull request and utilize the same Windows AMI:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It should be noted that the results from your local tests may differ.";test_debt
"While we can consider removing the WIN_GPU_MKLDNN tests, the issue of flakiness with WIN_GPU remains. This can be observed in the following builds, which pertain to the same codebase as this pull request and employ the same Windows AMI:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It is important to note that the results from your local tests may vary.";test_debt
"Although we can opt to remove the WIN_GPU_MKLDNN tests, we are still faced with the issue of flakiness in the WIN_GPU tests, as evidenced by the following builds:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It is worth noting that the results obtained from local tests may present a different outcome.";test_debt
"While we can consider eliminating the WIN_GPU_MKLDNN tests, we still encounter the issue of flakiness with WIN_GPU, as observed in the following builds:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It is important to consider that the local tests may yield different results.";test_debt
"Although we can consider removing the WIN_GPU_MKLDNN tests, we still face the issue of flakiness with WIN_GPU, as evident from the following builds:

WIN_GPU	WIN_GPU_MKLDNN	Build Number	Link
❌	✔️	15	Link
❌	✔️	14	Link
✔️	✔️	12	Link
✔️	❌	13	Link
It should be noted that your local tests may present a different perspective.";test_debt
"I like the idea - that means that the webfrontend now shows what values are actually used, even if they are not in the config.
To make it even nicer, it would be cool to:
- Show which values come from the config (top) and show for which ones what default is used (in a second section below)
- add a Unit test that checks that all keys from the `ConfigConstants` are converted. I think it is easy to check this via enumerating all fields in the class using reflection.";test_debt
"I appreciate the concept, as it allows the web frontend to display the actual values being used, even if they are not present in the configuration. To enhance the functionality further, it would be great to:

Clearly indicate which values originate from the config (at the top).
Present the defaults used for values that do not exist in a separate section below.
Incorporate a unit test that verifies the conversion of all keys from ConfigConstants. This can be achieved by iterating through all fields in the class using reflection.";test_debt
"The idea is commendable, as it enables the web frontend to display the actual values employed, even if they are absent in the configuration. To enhance this feature, it would be beneficial to:

Clearly denote the values sourced from the config (at the top).
Specify the defaults used for values that are not present in a separate section below.
Include a unit test that validates the conversion of all keys from ConfigConstants. This can be accomplished by using reflection to iterate through all the fields in the class.";test_debt
"I find the idea intriguing, as it allows the web frontend to exhibit the actual values being utilized, even if they are not specified in the configuration. To enhance this functionality, the following improvements can be implemented:

Clearly indicate the values derived from the config (at the top).
Provide information about the defaults used for values that are not available in a dedicated section below.
Introduce a unit test that verifies the conversion of all keys from ConfigConstants. This can be achieved by employing reflection to iterate through all the fields in the class.";test_debt
"The idea is quite appealing, as it enables the web frontend to display the actual values being utilized, even if they are not explicitly set in the configuration. To further enhance this feature, consider the following suggestions:

Clearly distinguish the values originating from the config (at the top).
Present the defaults used for values that are not present in a separate section below.
Incorporate a unit test that ensures the conversion of all keys from ConfigConstants. This can be accomplished by leveraging reflection to iterate through all the fields in the class.";test_debt
"I find the idea intriguing, as it allows the web frontend to showcase the actual values being used, even if they are not specified in the configuration. To improve this functionality, consider implementing the following enhancements:

Clearly indicate the values sourced from the config (at the top).
Display the defaults used for values that are not available in a distinct section below.
Integrate a unit test that validates the conversion of all keys from ConfigConstants. This can be achieved by employing reflection to iterate through all the fields in the class.";test_debt
"The concept is appealing, as it empowers the web frontend to present the actual values being utilized, even if they are not explicitly defined in the configuration. To further augment this capability, consider the following enhancements:

Clearly identify the values originating from the config (at the top).
Specify the defaults used for values that are not present in a separate section below.
Implement a unit test that verifies the conversion of all keys from ConfigConstants. This can be accomplished by utilizing reflection to iterate through all the fields in the class.";test_debt
"I haven't measured how much time increase it causes. The purpose of this change is to make each test independent of each other. If we use BeforeClass and AfterClass tags, then these methods need to be static and the variables instantiated (i.e. producer, systemAdmin) needs to be static as well. These variables will then be instantiated exactly once for the test suite and used by all unit tests in this test suite. Thus kind allows interference between tests and make make test development difficult.
During development of this patch the interference must have caused some problem which required me to make this change. I don't exactly remember what that problem is now. But in general it seems like good practice to make unit test independent of each other. What do you think?";test_debt
"I haven't measured the extent to which this change affects the execution time. The primary objective of this modification is to ensure that each test is independent of the others. By using BeforeClass and AfterClass tags, the associated methods must be static, and the instantiated variables (such as producer and systemAdmin) must also be static. This approach would result in these variables being instantiated once for the entire test suite and shared among all unit tests within that suite. Consequently, interference between tests can occur, making test development more challenging.
While developing this patch, I encountered an issue that was likely caused by this interference, prompting me to make this change. I don't recall the specifics of that problem at the moment. Nonetheless, it seems like a good practice to make unit tests independent of each other. What are your thoughts on this?";test_debt
"I have not measured the impact on execution time caused by this change. The purpose of this modification is to ensure that each test is self-contained and independent. If we were to utilize BeforeClass and AfterClass tags, the corresponding methods would need to be static, and the instantiated variables (such as producer and systemAdmin) would also need to be static. This approach would lead to these variables being instantiated once for the entire test suite and shared across all unit tests within that suite. Consequently, this setup can introduce interference between tests, making test development more complex.
While developing this patch, I encountered a problem likely caused by such interference, which prompted me to implement this change. Unfortunately, I cannot recall the specifics of that issue. Nevertheless, it appears to be a good practice to ensure independence among unit tests. What is your perspective on this matter?";test_debt
"I have not conducted measurements to determine the extent of the time increase caused by this change. The objective of this modification is to establish independence among individual tests. Utilizing BeforeClass and AfterClass tags would necessitate static methods and static instantiation of variables (e.g., producer, systemAdmin). Consequently, these variables would be instantiated only once for the entire test suite, leading to potential interference between tests and complicating test development.
During the development of this patch, I encountered a problem that was likely caused by such interference, prompting me to implement this change. Regrettably, I do not recall the specific details of that issue. However, it generally seems to be good practice to ensure that unit tests are independent of each other. I would appreciate hearing your thoughts on this matter.";test_debt
"I have not performed any measurements to determine the impact on execution time caused by this change. The main objective of this modification is to ensure that each test is isolated from the others. By using BeforeClass and AfterClass tags, the associated methods would need to be static, and the instantiated variables (such as producer and systemAdmin) would also need to be static. This approach would result in these variables being instantiated once for the entire test suite and shared among all unit tests within that suite. Consequently, interference between tests could arise, making test development more challenging.
While developing this patch, I encountered a problem likely caused by such interference, motivating me to introduce this change. Unfortunately, I do not recollect the specific nature of that problem. Nevertheless, it generally seems like a good practice to ensure that unit tests are independent of each other. I would appreciate hearing your perspective on this matter.";test_debt
"I have not measured the impact on execution time resulting from this change. The primary purpose of this modification is to establish independence among tests. If we were to employ BeforeClass and AfterClass tags, the corresponding methods would need to be static, and the instantiated variables (such as producer and systemAdmin) would also need to be static. Consequently, these variables would be instantiated once for the entire test suite and shared across all unit tests within that suite. This setup could lead to interference between tests, making test development more cumbersome.
While developing this patch, I encountered an issue that was likely caused by such interference, prompting me to make this change. Unfortunately, I cannot recall the specific details of that problem. Nonetheless, it seems advisable to ensure independence among unit tests as a general practice. I am interested in hearing your thoughts on this matter.";test_debt
"The impact on execution time resulting from this change has not been measured. The main aim of this modification is to establish independence among the tests. Utilizing BeforeClass and AfterClass tags would require the associated methods to be static, and the instantiated variables (such as producer and systemAdmin) would also need to be static. As a consequence, these variables would be instantiated once for the entire test suite and shared among all unit tests within that suite. This approach introduces the potential for interference between tests, making test development more complex.
While working on this patch, I encountered a problem likely caused by such interference, which motivated me to implement this change. Unfortunately, I cannot recall the specific details of that issue. However, it appears to be a good practice to ensure independence among unit tests. I am curious to know your thoughts on this matter.";test_debt
"Add documentation on GPU performance on Quantization example so end user knows that GPU performance is expected to be slower than CPU
Fixes https://github.com/apache/incubator-mxnet/issues/10897
- Unit tests are added for small changes to verify correctness (e.g. adding a new operator)
- Nightly tests are added for complicated/long-running ones (e.g. changing distributed kvstore)
- Build tests will be added for build configuration changes (e.g. adding a new build option with NCCL)
- For user-facing API changes, API doc string has been updated. 
- For new C++ functions in header files, their functionalities and arguments are documented. 
- For new examples, README.md is added to explain the what the example does, the source of the dataset, expected performance on test set and reference to the original paper if applicable
- Check the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html
- If this change is a backward incompatible change, why must this change be made.
- Interesting edge cases to note here
@ThomasDelteil @reminisce 
@mxnet-label-bot [pr-awaiting-review]";test_debt
"Include documentation regarding the GPU performance of the Quantization example to inform end users that GPU performance is expected to be slower than CPU. This addresses the issue outlined in https://github.com/apache/incubator-mxnet/issues/10897.
Unit tests have been added to validate the correctness of minor changes, such as the addition of a new operator.
Nightly tests have been incorporated to cover complex or long-running scenarios, such as modifications to the distributed kvstore.
Build tests will be implemented to evaluate changes related to build configurations, such as the addition of a new build option with NCCL.
User-facing API changes have been documented by updating the API doc string.
New C++ functions in header files have been documented to provide information on their functionalities and arguments.
For newly introduced examples, a README.md file has been included to explain the purpose of the example, the source of the dataset, the expected performance on the test set, and a reference to the original paper if applicable.
Please refer to the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html for further details.
If this change is a backward incompatible change, it is important to specify the reason why this change must be made.
Additionally, please consider any noteworthy edge cases in this context.
Reviewers: @ThomasDelteil @reminisce
[pr-awaiting-review]";test_debt
"Provide documentation on the GPU performance of the Quantization example to inform end users that GPU performance is expected to be slower than CPU performance.
Resolves https://github.com/apache/incubator-mxnet/issues/10897.
Added unit tests to validate small changes and ensure correctness, such as the addition of a new operator.
Introduced nightly tests for complex or long-running scenarios, such as modifications to the distributed kvstore.
Implemented build tests for changes in build configurations, such as the addition of a new build option with NCCL.
Updated API doc strings for user-facing API changes.
Documented the functionalities and arguments of new C++ functions in header files.
Added a README.md file to provide an overview of new examples, including their purpose, dataset source, expected performance on the test set, and references to the original paper if applicable.
Checked the API documentation at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html.
If this change introduces backward incompatibility, the reasons for making the change are clearly explained.
Notable edge cases worth mentioning.
CC: @ThomasDelteil @reminisce
@mxnet-label-bot [pr-awaiting-review]";test_debt
"Include documentation regarding the GPU performance of the Quantization example to inform end users that GPU performance is expected to be slower than CPU.
Fixes: GitHub Issue #10897

Added unit tests to ensure the correctness of small changes (e.g., adding a new operator).
Nightly tests have been implemented for complex or long-running changes (e.g., modifications to the distributed kvstore).
Build tests will be incorporated for changes in build configuration (e.g., adding a new build option with NCCL).

Updated the API doc string for user-facing API changes.

Documented the functionalities and arguments of new C++ functions in header files.

Added a README.md file for new examples, providing an explanation of what the example does, the source of the dataset, expected performance on the test set, and a reference to the original paper if applicable.

Refer to the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html for more information.

If this change represents a backward incompatible change, please specify the reasons for making the change.
Additionally, please take note of any interesting edge cases related to this change.

Reviewers: @ThomasDelteil @reminisce
Label: @mxnet-label-bot [pr-awaiting-review]";test_debt
"Documentation should be added to clarify the GPU performance on the Quantization example, ensuring that end users are aware that GPU performance is expected to be slower than CPU.
Fixes: GitHub Issue #10897

Unit tests have been included to validate the correctness of small changes, such as the addition of a new operator.
Nightly tests have been implemented for more complex or time-consuming scenarios, such as modifications to the distributed kvstore.
Build tests will be added to assess changes related to build configurations, such as the inclusion of a new build option with NCCL.

User-facing API changes have been accompanied by updates to the API doc string, ensuring that the documentation reflects the latest modifications.

For new C++ functions in header files, their functionalities and arguments have been documented, providing clear explanations for their usage.

New examples have been supplemented with a README.md file, offering insights into the purpose of the example, the source of the dataset, expected performance on the test set, and references to the original paper if applicable.

To review the API documentation, please refer to this link.

If this change results in backward incompatibility, it is essential to clarify the reasons necessitating this modification.
Additionally, please consider highlighting any interesting edge cases related to this change.

Requested reviewers: @ThomasDelteil, @reminisce
Label for MXNet: [pr-awaiting-review]";test_debt
"Provide documentation on the GPU performance of the Quantization example to inform end users that GPU performance is expected to be slower than CPU performance.
Fixes GitHub Issue #10897.

Unit tests have been included to verify the correctness of small changes, such as the addition of a new operator.
Nightly tests have been added for more complex or time-consuming tasks, such as modifications to the distributed kvstore.
Build tests will be implemented for changes related to build configuration, such as the introduction of a new build option with NCCL.
User-facing API changes have been accompanied by updates to the API doc string.
New C++ functions in header files have been documented, highlighting their functionalities and arguments.
New examples now include a README.md file that explains the purpose of the example, the source of the dataset, expected performance on the test set, and references to the original paper if applicable.

Please refer to the API documentation at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html for detailed information.

If this change introduces backward incompatibility, it is important to clarify the reasons why this change must be made.

Notable edge cases that deserve attention are:

Attention: @ThomasDelteil @reminisce

@mxnet-label-bot [pr-awaiting-review]";test_debt
"Provide documentation on the GPU performance of the Quantization example to inform end users that GPU performance is expected to be slower than CPU performance.
Resolves https://github.com/apache/incubator-mxnet/issues/10897.
Added unit tests to ensure correctness for small changes (e.g., adding a new operator).
Included nightly tests for complex or long-running scenarios (e.g., modifying distributed kvstore).
Incorporated build tests for changes in build configurations (e.g., introducing a new build option with NCCL).
Updated API doc strings for user-facing API changes.
Documented functionalities and arguments for new C++ functions in header files.
Added README.md for new examples, explaining their purpose, dataset source, expected performance on the test set, and providing a reference to the original paper, if applicable.
Verify the API doc at http://mxnet-ci-doc.s3-accelerate.dualstack.amazonaws.com/PR-$PR_ID/$BUILD_ID/index.html.
If this change is a backward incompatible change, please provide justification for why it must be made.
Notable edge cases to be aware of.
Review requested from @ThomasDelteil and @reminisce.
Label bot: [pr-awaiting-review].";test_debt
"@dsclose I think it is better to split this PR into some isolated PRs, as the issues are isolated.
to be honest, some commits looks good to me ( as we have similar fix in our production), others need testing.";test_debt
@dsclose, I believe it would be more beneficial to split this pull request into several isolated ones, as the issues at hand are distinct and separate.;test_debt
In my opinion, it would be advantageous to divide this PR into smaller, focused PRs, considering that the problems being addressed are isolated.;test_debt
@dsclose, I suggest breaking down this pull request into multiple smaller PRs since the issues being tackled are independent of each other.;test_debt
It seems more appropriate to split this PR into separate PRs, as the issues being addressed are isolated and distinct from one another.;test_debt
@dsclose, I recommend separating this pull request into smaller, self-contained PRs, given that the issues at hand can be addressed individually.;test_debt
I believe it would be more effective to split this PR into multiple smaller PRs, considering that the issues being resolved are separate from one another.;test_debt
This is a bit concerning that we are commenting out a good number of tests here. I would prefer to fix them, before check-in.;test_debt
It is worrisome that a significant number of tests are being commented out. I would strongly prefer to address and fix these tests before proceeding with the check-in.;test_debt
The fact that a considerable portion of tests is being commented out raises concerns. It would be more favorable to resolve and rectify these tests before proceeding with the check-in process.;test_debt
Commenting out a substantial number of tests is a cause for concern. It would be preferable to address and resolve these tests before proceeding with the check-in.;test_debt
I am concerned about the decision to comment out a significant number of tests. It would be more appropriate to fix these tests before proceeding with the check-in.;test_debt
The fact that a notable number of tests are being commented out is troubling. It is my preference to address and resolve these tests before proceeding with the check-in.;test_debt
I find it concerning that a substantial portion of tests is being commented out. I would strongly recommend fixing these tests before proceeding with the check-in process.;test_debt
KAFKA-9896: fix flaky StandbyTaskEOSIntegrationTest;test_debt
KAFKA-9896: Resolve intermittent failures in StandbyTaskEOSIntegrationTest;test_debt
KAFKA-9896: Resolved flakiness in StandbyTaskEOSIntegrationTest.;test_debt
StandbyTaskEOSIntegrationTest: Fixed flakiness issue (KAFKA-9896).;test_debt
Fixed flaky StandbyTaskEOSIntegrationTest (KAFKA-9896).;test_debt
StandbyTaskEOSIntegrationTest: Flakiness issue addressed and resolved (KAFKA-9896).;test_debt
Resolved flakiness in StandbyTaskEOSIntegrationTest (KAFKA-9896).;test_debt
"The `thrift` directory makes sense.
The test # 4 was relying on `sleep` in test script that is inherently flaky.
I've removed a double pclose in parent process code so we'll see if it fixes the problem in #368 result.
Another problem is that Appveyor CI is failing due to include error.
Not sure why it's only happening on Windows.";test_debt
"The inclusion of the thrift directory is logical and appropriate.
Test case #4 was dependent on the usage of sleep in the test script, which inherently introduces flakiness.
In the parent process code, I have eliminated a redundant pclose operation. Let's observe if this resolves the issue mentioned in #368 result.
There is an issue with the Appveyor CI, which is failing due to an inclusion error. It is puzzling why this error occurs only on Windows.";test_debt
"I have addressed the reliance on sleep in test case #4, which inherently introduces flakiness.
By removing the redundant pclose in the parent process code, we can assess if the problem reported in #368 result is resolved.
An issue has arisen in the Appveyor CI, where it fails due to an error in the inclusion process. It remains unclear why this issue is specific to Windows.";test_debt
"The inclusion of the thrift directory is logical and appropriate.
Test number 4 was susceptible to flakiness due to its reliance on the sleep function within the test script. To address this, I have removed a double pclose in the parent process code. This change will be monitored to determine if it resolves the issue encountered in result #368.
An additional challenge has arisen as the Appveyor CI is failing due to an include error. The cause of this error specifically on Windows platforms remains unclear.";test_debt
"The presence of the thrift directory aligns with the overall structure and purpose of the project.
Test case number 4 exhibited flakiness due to the use of the sleep function in the test script. To mitigate this issue, a double pclose in the parent process code has been eliminated. The impact of this change will be evaluated to assess its effectiveness in addressing the problem encountered in result #368.
It is worth noting that the thrift directory serves a logical purpose within the project.
A notable challenge has arisen with the Appveyor CI, which is experiencing a failure attributed to an include error. The root cause of this issue, specifically on Windows platforms, is currently unknown.";test_debt
"The inclusion of the thrift directory seems appropriate.
Test #4 exhibited flakiness due to its reliance on the sleep command in the test script. This inherent flakiness has been addressed.
I have removed a duplicate pclose in the parent process code. This change will be evaluated to determine if it resolves the issue observed in result #368.
Additionally, there is an include error causing the failure of the Appveyor CI. It is unclear why this error specifically occurs on Windows.";test_debt
"The thrift directory is a logical addition to the project.
Test #4 experienced flakiness due to the reliance on the sleep command within the test script. This issue has been resolved.
I have addressed the double pclose in the parent process code and will evaluate if it resolves the problem encountered in result #368. Furthermore, there seems to be an issue with an include error in the Appveyor CI, specifically on Windows.";test_debt
These tests all have to duplicate the same verbose boilerplate. Centralizing that in `SlaveTester`.;test_debt
It would be beneficial to centralize the verbose boilerplate present in these tests by introducing a SlaveTester for code reuse.;test_debt
Consider centralizing the redundant and verbose boilerplate found in these tests by implementing a SlaveTester class for improved code reuse and maintainability.;test_debt
Introducing a SlaveTester class to centralize the repetitive and verbose boilerplate in these tests would enhance code organization and promote reusability.;test_debt
To reduce code duplication and improve maintainability, it is advisable to create a SlaveTester class that encapsulates the verbose boilerplate shared across these tests.;test_debt
A recommended approach to address the duplication of verbose boilerplate in these tests is to introduce a SlaveTester class, allowing for improved code reuse and readability.;test_debt
By centralizing the shared verbose boilerplate in a SlaveTester class, the tests can be streamlined, reducing duplication and enhancing code maintainability.;test_debt
Besides not removing this, it seems we should also probably have a unit test validating the new behavior for naming consumer groups in sinks?;test_debt
Instead of removing this, it appears that we should consider adding a unit test to validate the new behavior of naming consumer groups in sinks.;test_debt
In addition to not removing this, it seems appropriate to include a unit test that verifies the new behavior of naming consumer groups in sinks.;test_debt
Instead of removing this, it would be advisable to augment the code with a unit test that ensures the correct behavior of naming consumer groups in sinks.;test_debt
Besides not removing this, it is worth considering the inclusion of a unit test to validate the updated behavior of naming consumer groups in sinks.;test_debt
In addition to not removing this, it is recommended to introduce a unit test that validates the expected behavior of naming consumer groups in sinks.;test_debt
Rather than removing this, it would be beneficial to incorporate a unit test that verifies the desired behavior of naming consumer groups in sinks.;test_debt
I will add a test case.;test_debt
I intend to include an additional test case.;test_debt
I will contribute a new test case.;test_debt
Rest assured, I will add a test case to cover this scenario.;test_debt
To ensure comprehensive coverage, I will incorporate a new test case.;test_debt
It is on my agenda to include a test case for this.;test_debt
I am planning to add a test case to address this requirement.;test_debt
"This is just a rebase of  #719 from @xurror and @percyashu and I'm just curious if this passes... I ran this locally x3 times, and it failed with https://issues.apache.org/jira/browse/FINERACT-855 every time - may be that is not just a flaky test, but really something that this PR breaks, for some (strange, yes) reason.
NB that other PRs passed today (e.g. #723 and #725) so if this still fails, perhaps the new RestAssured version is somehow changing some timing or something which causes IT test failures?!";test_debt
"This pull request is essentially a rebase of #719 from contributors @xurror and @percyashu. I am particularly interested to see if it passes the tests. I have run it locally three times, and each time it failed with the issue identified in https://issues.apache.org/jira/browse/FINERACT-855. It is possible that this is not merely a flaky test, but rather something that this PR unintentionally disrupts for some peculiar reason.
It is worth noting that other pull requests, such as #723 and #725, passed successfully today. If this PR continues to fail, it could be due to the new RestAssured version potentially affecting timing or introducing other factors leading to failures in the integration tests.";test_debt
"This pull request is essentially a rebased version of #719, initially submitted by contributors @xurror and @percyashu. I am eager to determine whether it passes the tests. In my local testing, I ran it three times, and each time it encountered a failure related to the issue logged in https://issues.apache.org/jira/browse/FINERACT-855. It appears that this failure may not be attributed to a flaky test, but rather an unintended impact of this PR for some peculiar reason.
It is important to note that other pull requests, such as #723 and #725, successfully passed today. If this PR continues to fail, it is plausible that the updated RestAssured version is affecting timing or introducing other factors that result in integration test failures.";test_debt
"The purpose of this pull request is to rebase the changes made in #719 by contributors @xurror and @percyashu. I am particularly interested in determining whether it passes the tests. Upon running it locally three times, I consistently encountered a failure related to the issue described in https://issues.apache.org/jira/browse/FINERACT-855. It is conceivable that this failure is not merely a flaky test but rather an unintended consequence of this PR for some peculiar reason.
It is worth mentioning that other pull requests, such as #723 and #725, successfully passed today. If this PR continues to fail, it is possible that the new RestAssured version is altering timing or introducing other factors that lead to failures in the integration tests.";test_debt
"This pull request is essentially a rebased version of #719, which was initially contributed by @xurror and @percyashu. I am keen to determine whether it successfully passes the tests. During my local testing, I executed it three times, and on each occasion, it failed with the issue identified in https://issues.apache.org/jira/browse/FINERACT-855. This failure may not be attributed solely to a flaky test but rather an unintentional consequence of this PR for some unusual reason.
It is important to note that other pull requests, including #723 and #725, passed successfully today. If this PR continues to fail, it is possible that the updated RestAssured version is influencing timing or introducing other factors that lead to integration test failures.";test_debt
"This pull request is essentially a rebase of #719 from contributors @xurror and @percyashu. I am particularly interested in determining whether it successfully passes the tests. In my local testing, I ran it three times, and each time it resulted in a failure associated with the issue described in https://issues.apache.org/jira/browse/FINERACT-855. This failure may not be limited to a flaky test but rather a consequence of this PR unintentionally affecting the functionality for some peculiar reason.
It is worth noting that other pull requests, such as #723 and #725, passed successfully today. If this PR continues to fail, it is plausible that the updated RestAssured version is influencing timing or introducing other factors that lead to failures in the integration tests.";test_debt
"This pull request is essentially a rebased version of #719, originally submitted by contributors @xurror and @percyashu. I am specifically interested in determining whether it passes the tests. In my local testing, I executed it three times, and each time it encountered a failure related to the issue reported in https://issues.apache.org/jira/browse/FINERACT-855. It is conceivable that this failure is not just a result of a flaky test, but rather an unintended consequence of this PR introducing some unexpected impact.
It is worth mentioning that other pull requests, such as #723 and #725, successfully passed today. If this PR continues to fail, it is possible that the updated RestAssured version is altering timing or introducing other factors that result in integration test failures.";test_debt
please add a test where both _SYSTEM_ and _SESSION_ options are changed, and confirm the reset is working as expected.;test_debt
It would be beneficial to add a test case where both the SYSTEM and SESSION options are modified, and then verify that the reset functionality behaves as expected.;test_debt
I suggest including a test case that covers the scenario where both the SYSTEM and SESSION options are changed, and subsequently validating that the reset mechanism functions correctly.;test_debt
Adding a test case that encompasses changing both the SYSTEM and SESSION options, and confirming the expected behavior of the reset feature, would be valuable.;test_debt
It is important to incorporate a test case that addresses the situation where both the SYSTEM and SESSION options are altered, and then ensure that the reset functionality operates as intended.;test_debt
I recommend including a test case that specifically tests the scenario where both the SYSTEM and SESSION options are modified, and subsequently verifying that the reset behaves as expected.;test_debt
In order to validate the proper functioning of the reset mechanism, it is advisable to add a test case where both the SYSTEM and SESSION options are changed, and subsequently confirm the expected behavior.;test_debt
"Well, the previous non-REST test implemented a client which did not send anything to the server but just returned a successful response or (if `clusterSaturated` was set to true) a temporary failure.
But I'm ok to remove the Test class if it's too much work to rewrite it for the REST client.
I've tested the PR but the initial rounds failed for about 50% of the pages/documents:
I got it fixed by using XContentBuilder to pass document as JSON to ES client, you'll find the necessary changes in [this branch](https://github.com/sebastian-nagel/nutch/tree/NUTCH-2739). Also:
- updated the description how to upgrade the dependencies in the plugin.xml and added few exclusions of dependencies already provided by Nutch core.
- changed the default properties in index-writers.xml.template so that the indexer-elastic plugin works out-of-the-box with default settings
So far, I didn't run any tests at scale. Should be to make sure we are able to index millions of documents with the given settings.";test_debt
"In the previous non-REST test implementation, the client did not send any data to the server. Instead, it returned a successful response or, if clusterSaturated was set to true, a temporary failure. However, if rewriting the test for the REST client proves to be too challenging, I am open to removing the Test class altogether.
I have conducted testing on the PR, but initial rounds resulted in failures for approximately 50% of the pages/documents. To address this, I made changes in this branch, where I used XContentBuilder to pass the document as JSON to the Elasticsearch client.
Additionally, I have updated the description on how to upgrade the dependencies in the plugin.xml and added a few exclusions for dependencies already provided by Nutch core.
The default properties in index-writers.xml.template have been modified to ensure that the indexer-elastic plugin works seamlessly with default settings.";test_debt
"At this stage, I have not executed any large-scale tests. However, it is essential to validate the indexing of millions of documents with the given settings to ensure functionality and performance.
I have made necessary changes in the code to address the failures encountered during initial testing. The changes involve utilizing XContentBuilder to pass the document as JSON to the Elasticsearch client.
In addition to the code modifications, I have updated the instructions for upgrading dependencies in the plugin.xml file and included exclusions for dependencies already provided by Nutch core. The default properties in index-writers.xml.template have also been adjusted to facilitate smooth operation of the indexer-elastic plugin with default settings. It is important to conduct large-scale tests to assess the indexing capability and ensure successful indexing of millions of documents under the given configuration.";test_debt
"In the previous non-REST test, the implemented client did not send any requests to the server but returned a successful response or, if clusterSaturated was set to true, a temporary failure. However, I am willing to remove the Test class if it would require significant effort to rewrite it for the REST client.
I have tested the pull request, but initially, around 50% of the pages/documents failed. To address this, I made changes by utilizing XContentBuilder to pass the document as JSON to the Elasticsearch client. You can find the necessary changes in this branch. Additionally:
I updated the description on how to upgrade the dependencies in the plugin.xml file and included a few exclusions for dependencies already provided by Nutch core.
The default properties in index-writers.xml.template were modified to ensure that the indexer-elastic plugin works seamlessly with default settings.
It is important to note that I have not yet conducted any large-scale tests. Running tests at scale will help ensure that we can successfully index millions of documents with the given settings.";test_debt
"Regarding the previous non-REST test, its client implementation did not send any requests to the server instead, it returned a successful response or a temporary failure if clusterSaturated was set to true. If rewriting this test for the REST client would be overly burdensome, I am open to removing it.
After testing the pull request, I encountered initial failures for approximately 50% of the pages/documents. To resolve this, I made the necessary changes by leveraging XContentBuilder to pass the document as JSON to the Elasticsearch client. You can review the specific modifications in this branch. Additionally:
I updated the instructions on upgrading the dependencies in the plugin.xml file and added exclusions for dependencies already provided by Nutch core.
The default properties in index-writers.xml.template were adjusted to ensure that the indexer-elastic plugin seamlessly functions with the default settings.
It is worth mentioning that I have not yet conducted tests at a large scale. It is crucial to verify that the given settings allow for successful indexing of millions of documents.";test_debt
"I understand that the previous non-REST test involved a client that did not actually send requests to the server but returned either a successful response or a temporary failure when clusterSaturated was set to true. If rewriting this test for the REST client would pose significant challenges, I am willing to remove it.
Following my testing of the pull request, I encountered initial failures for approximately 50% of the pages/documents. To address this issue, I made the necessary adjustments by utilizing XContentBuilder to pass the document as JSON to the Elasticsearch client. The specific changes can be found in this branch. Additionally:
I updated the instructions for upgrading dependencies in the plugin.xml file and included exclusions for dependencies already provided by Nutch core.
Modifications were made to the default properties in index-writers.xml.template to ensure that the indexer-elastic plugin functions smoothly with default settings.
However, it is important to note that I have not yet conducted tests on a large scale. It is crucial to verify the ability to index millions of documents using the given settings.";test_debt
"The previous non-REST test featured a client that didn't send any requests to the server but returned a successful response or, in the case of the clusterSaturated flag being set to true, a temporary failure. However, if rewriting the test for the REST client is too complex, I am open to removing the Test class.
If rewriting the previous non-REST test for the REST client proves to be challenging, I am willing to remove the Test class altogether.
During testing of the PR, initial rounds resulted in about 50% of the pages/documents failing. To address this, I made changes using XContentBuilder to pass the document as JSON to the ES client. You can find the necessary modifications in this branch. Additionally, I updated the description on how to upgrade the dependencies in the plugin.xml file and added a few exclusions of dependencies already provided by Nutch core. Furthermore, default properties in index-writers.xml.template were adjusted so that the indexer-elastic plugin can function seamlessly with default settings.";test_debt
"Thanks for the updates @wsry ! 
I think most of my previous comments were addressed except for https://github.com/apache/flink/pull/13595#discussion_r514988635. And as @gaoyunhaii also mentioned above, it is better to supplement some tests for covering empty subpartition case.";test_debt
Thank you, @wsry, for addressing the majority of my previous comments. However, the concern raised in https://github.com/apache/flink/pull/13595#discussion_r514988635 remains unresolved. Additionally, as mentioned by @gaoyunhaii earlier, it would be beneficial to include additional tests that cover the case of empty subpartitions.;test_debt
@wsry, I appreciate the updates you've made so far. Most of my previous comments have been addressed, except for the issue highlighted in https://github.com/apache/flink/pull/13595#discussion_r514988635. Furthermore, I agree with @gaoyunhaii's suggestion that it would be advantageous to include supplementary tests to cover scenarios involving empty subpartitions.;test_debt
Many thanks, @wsry, for taking care of most of my previous comments. However, the concern raised in https://github.com/apache/flink/pull/13595#discussion_r514988635 remains unresolved. Additionally, as mentioned by @gaoyunhaii earlier, it would be helpful to incorporate additional tests that encompass the scenario of empty subpartitions.;test_debt
@wsry, thank you for the updates you've made. I am pleased to see that most of my previous comments have been addressed. However, the issue mentioned in https://github.com/apache/flink/pull/13595#discussion_r514988635 still requires attention. Additionally, I support @gaoyunhaii's suggestion to include supplementary tests that cover the case of empty subpartitions.;test_debt
I want to express my gratitude to @wsry for addressing the majority of my previous comments. However, the concern raised in https://github.com/apache/flink/pull/13595#discussion_r514988635 has not been resolved yet. Furthermore, as highlighted by @gaoyunhaii earlier, it would be beneficial to incorporate additional tests to cover scenarios involving empty subpartitions.;test_debt
@wsry, thank you for your efforts in addressing most of my previous comments. However, the issue discussed in https://github.com/apache/flink/pull/13595#discussion_r514988635 still requires attention. Additionally, as suggested by @gaoyunhaii, it would be advantageous to include supplementary tests that cover the case of empty subpartitions.;test_debt
ping. it'd be good to get this in since it'll help with debugging tests that flake pretty consistently on jenkins (though we may also need some follow up to turn up logging output during tests, I think a bunch of modules don't even have logging turned on).;test_debt
Just a friendly reminder, it would be beneficial to merge this pull request as it will aid in debugging tests that consistently fail on Jenkins. However, we may need to consider additional steps to increase the logging output during tests, as several modules currently have logging turned off.;test_debt
I wanted to highlight the importance of merging this pull request, as it will provide valuable assistance in troubleshooting tests that consistently fail on Jenkins. Additionally, it might be necessary to implement further measures to enable increased logging output during tests, especially for modules that currently have logging disabled.;test_debt
It would be highly advantageous to prioritize the integration of this pull request, as it will greatly facilitate the debugging of tests that consistently fail on Jenkins. To enhance the effectiveness of the debugging process, we may also need to explore additional actions such as enabling logging in modules that currently lack logging output.;test_debt
I wanted to emphasize the significance of merging this pull request, as it will contribute significantly to the resolution of consistent test failures on Jenkins. Additionally, we should consider implementing measures to increase logging output during tests, particularly in modules where logging is currently disabled.;test_debt
This is a gentle reminder that merging this pull request will be of great value in addressing the consistent test failures on Jenkins. Furthermore, we should investigate the possibility of enabling logging output in modules where it is currently turned off to enhance the debugging process.;test_debt
It is essential to consider merging this pull request promptly, as it will play a vital role in troubleshooting the persistent test failures on Jenkins. Additionally, we should assess the logging configuration in modules that lack logging output to ensure adequate visibility during the testing phase.;test_debt
I tend to always work in an IDE that supports project view (Visual Studio, Xcode), wherein I already know I'm working with a test project. Hence the Test suffix is a bit redundant. Also, we already have 4 tests in the new framework that don't use the Test suffix.;test_debt
In my workflow, I typically utilize an IDE that supports a project view, such as Visual Studio or Xcode. Since I'm already aware that I'm working with a test project, the Test suffix feels somewhat redundant. Additionally, it's worth noting that we already have four tests in the new framework that don't employ the Test suffix.;test_debt
Given that I predominantly work within an IDE that offers project view capabilities, such as Visual Studio or Xcode, the Test suffix seems somewhat unnecessary as I'm already aware that I'm working with a test project. Furthermore, it's worth mentioning that we already have four tests in the new framework that don't utilize the Test suffix.;test_debt
As I primarily work in an IDE that supports project view, like Visual Studio or Xcode, I find the Test suffix to be somewhat redundant since I'm already aware that I'm working with a test project. It's worth mentioning that we already have four tests in the new framework that don't include the Test suffix.;test_debt
Since I predominantly work in an IDE with a project view, such as Visual Studio or Xcode, I find the Test suffix to be unnecessary as I'm already aware that I'm working on a test project. Additionally, it's worth noting that we already have four tests in the new framework that don't follow the Test suffix convention.;test_debt
Given that I typically work in an IDE that supports project view, such as Visual Studio or Xcode, the Test suffix feels somewhat superfluous as I already know I'm working with a test project. Furthermore, it's worth mentioning that we already have four tests in the new framework that don't adhere to the Test suffix convention.;test_debt
In my usual development environment, which includes IDEs like Visual Studio or Xcode with project view support, I find the Test suffix to be redundant since I'm already aware that I'm working on a test project. Additionally, it's worth highlighting that we already have four tests in the new framework that deviate from the Test suffix convention.;test_debt
Just not supported yet, the type TIME_STAMP_WITH_LOCAL_TIME_ZONE is rarely used. But I think we should both support them in this version.;requirement_debt
We haven't implemented support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. Although it is not commonly used, I believe it should be included in this version.;requirement_debt
At the moment, we don't have support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. While it may not be widely utilized, I think it's important to include support for it in this release.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is currently not supported. Although it's not frequently employed, I believe it's necessary to provide support for it in this version.;requirement_debt
We haven't added support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. While it may not be commonly used, I think it's crucial to incorporate support for it in this release.;requirement_debt
Currently, we don't have functionality for handling the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. Despite its infrequent usage, I believe it's necessary to implement support for it in this version.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not yet supported. Although it's not widely utilized, I think it should be included in this version.;requirement_debt
Support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type has not been implemented yet. Despite its limited usage, I believe it's important to incorporate support for it in this release.;requirement_debt
We haven't included support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. Although it's rarely used, I think it should be accommodated in this version.;requirement_debt
Currently, we don't have support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. Despite its low frequency of use, I believe it's necessary to provide support for it in this release.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not yet supported. Although it's not commonly employed, I think it should be implemented in this version.;requirement_debt
We haven't implemented support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. While it may not be widely adopted, I believe it's essential to support it in this release.;requirement_debt
At present, the TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not supported. Although it's seldom used, I think it's important to add support for it in this version.;requirement_debt
We haven't added support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. Despite its limited usage, I believe it should be included in this version.;requirement_debt
Currently, we don't have functionality for handling the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. Although it's not frequently used, I think it's necessary to implement support for it in this version.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not yet supported. Despite its infrequent usage, I believe it's important to incorporate support for it in this version.;requirement_debt
Support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type has not been implemented yet. Although it's rarely used, I believe it should be included in this release.;requirement_debt
We haven't included support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. While it's not commonly used, I think it should be accommodated in this version.;requirement_debt
Currently, we don't have support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. Despite its low frequency of use, I believe it's necessary to provide support for it in this release.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not yet supported. Although it's not widely employed, I think it should be implemented in this version.;requirement_debt
We haven't implemented support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. While it may not be widely adopted, I believe it's essential to support it in this release.;requirement_debt
At present, the TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not supported. Although it's seldom used, I think it's important to add support for it in this version.;requirement_debt
We haven't added support for the TIME_STAMP_WITH_LOCAL_TIME_ZONE type yet. Despite its limited usage, I believe it should be included in this version.;requirement_debt
Currently, we don't have functionality for handling the TIME_STAMP_WITH_LOCAL_TIME_ZONE type. Although it's not frequently used, I think it's necessary to implement support for it in this version.;requirement_debt
The TIME_STAMP_WITH_LOCAL_TIME_ZONE type is not yet supported. Despite its infrequent usage, I believe it's important to incorporate support for it in this version.;requirement_debt
"Regarding your questions:
Can you provide a common streaming use case where a non-event-base processing of metrics is useful? The `Observation` proposes a ""get me the metrics"" API that hides listeners and multi-threading. In streaming context, there are no finite metrics.
Can you elaborate on this, please? Do you mean an async `get`? Can you provide some pseudo-code on how to use the metrics async.
It is not thread-safe in a scenario where the action on the observed dataset is called in a different thread than where the metrics are retrieved. I'd say such a multi-thread driver process is not the general use case for batch processing. Even if, the proposed `Observation` API aims at hiding multi-threading complexity so that users can use `Dataset.observe` in single-threaded scenarios. Users that are experience with multi-threading have no problem in using the existing listeners approach. They are not the target user of this extension.
Agreed.";requirement_debt
In terms of non-event-based processing of metrics, could you provide a typical streaming use case where it would be beneficial? The Observation introduces an API for obtaining metrics that abstracts away listeners and multi-threading. However, in a streaming context, metrics are not finite. Could you elaborate on this further? Are you suggesting an asynchronous get operation? If so, could you provide some pseudo-code demonstrating how to asynchronously retrieve metrics? Additionally, it's worth noting that the Observation API is not thread-safe in scenarios where the action on the observed dataset is invoked in a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Users who are experienced with multi-threading should have no issues utilizing the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this assessment?;requirement_debt
"Regarding your queries: Can you provide a practical use case in streaming where non-event-based processing of metrics would be valuable? The Observation introduces an API that offers a ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in the context of streaming, metrics are not finite in nature. Could you provide more details on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code demonstrating how to use metrics asynchronously. Additionally, I would like to highlight that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. I believe that the use of a multi-threaded driver process is not a common scenario for batch processing. Even if it were, the intention behind the proposed Observation API is to simplify the handling of multi-threading complexities, allowing users to leverage Dataset.observe in single-threaded scenarios. Users who are experienced with multi-threading can continue to use the existing listeners approach, as they are not the primary target audience for this extension. Do you concur with this viewpoint?";requirement_debt
"Can you provide a real-world streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces a convenient ""get me the metrics"" API that abstracts away listeners and multi-threading intricacies. However, in the context of streaming, metrics are not finite but rather continuous. Could you please elaborate on this? Are you referring to an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to use metrics asynchronously. It's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to leverage the existing listeners approach, as they are not the primary target users for this extension. Are you in agreement with this line of thinking?";requirement_debt
"Could you provide an example of a common use case in streaming where non-event-based processing of metrics would be advantageous? The Observation introduces an API that offers a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading. However, in streaming scenarios, metrics are not limited or finite. Could you please provide further details on this? Are you referring to an asynchronous get operation? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be retrieved asynchronously. Additionally, it's important to consider that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is executed from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to use Dataset.observe in single-threaded scenarios. Users experienced in multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you concur with this analysis?";requirement_debt
"In terms of a non-event-based processing of metrics, could you provide a typical use case in the streaming context where this approach would be beneficial? The Observation API introduces a simplified ""get me the metrics"" API that abstracts away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite and have a continuous nature. Could you elaborate further on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to utilize metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to leverage Dataset.observe in single-threaded scenarios. Users experienced with multi-threading can continue to use the existing listeners approach, as they are not the primary target audience for this extension. Can we agree on this perspective?";requirement_debt
"Can you provide a real-world streaming use case where non-event-based processing of metrics would be useful? The Observation introduces an API that offers a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in the streaming context, metrics are not finite and have a continuous nature. Could you provide more details on this? Are you referring to an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how metrics can be retrieved asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users for this extension. Do you agree with this assessment?";requirement_debt
"Regarding your questions about non-event-based processing of metrics, can you provide a practical streaming use case where this approach would be beneficial? The Observation API proposes a convenient ""get me the metrics"" API that abstracts away listeners and multi-threading complexities. However, in the context of streaming, metrics are not finite and continue to evolve. Could you provide more insights into this? Are you suggesting an asynchronous get operation? If so, it would be great to see some pseudo-code demonstrating how to use metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to use Dataset.observe in single-threaded scenarios. Users with experience in multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Does this align with your understanding?";requirement_debt
"Can you provide an example of a common streaming use case where non-event-based processing of metrics would be useful? The Observation API introduces a convenient ""get me the metrics"" API that hides listeners and multi-threading complexities. However, in the streaming context, metrics are not finite. Could you elaborate on this? Are you referring to an asynchronous get operation? If so, it would be helpful to see some pseudo-code demonstrating how to asynchronously retrieve metrics. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the target users of this extension. Do you agree with this assessment?";requirement_debt
"Can you provide a typical streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces an API for obtaining metrics with a ""get me the metrics"" functionality that abstracts away listeners and multi-threading. However, in the streaming context, metrics are not finite in nature. Could you provide more details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code illustrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not a common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Would you agree with this perspective?";requirement_debt
"Regarding your questions about non-event-based processing of metrics, can you provide a common use case in streaming where this approach would be beneficial? The Observation API offers a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in a streaming context, metrics are not finite. Can you provide further details on this? Are you referring to an asynchronous get operation? If so, it would be great to see some pseudo-code demonstrating how to use metrics asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is executed from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the general use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to use Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you share this viewpoint?";requirement_debt
"Can you provide a practical streaming use case where non-event-based processing of metrics would be valuable? The Observation introduces an API for obtaining metrics with a ""get me the metrics"" functionality, hiding listeners and multi-threading complexities. However, in a streaming context, metrics are not finite. Could you elaborate on this further? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be retrieved asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Does this resonate with your understanding?";requirement_debt
"In terms of non-event-based processing of metrics, can you provide a common use case in streaming where this approach would be beneficial? The Observation API offers a simplified ""get me the metrics"" API that abstracts away listeners and multi-threading complexities. However, in the context of streaming, metrics are not finite. Can you elaborate on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to use metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to leverage Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target audience for this extension. Would you concur with this analysis?";requirement_debt
"Can you provide an example of a practical streaming use case where non-event-based processing of metrics would be useful? The Observation API introduces a convenient ""get me the metrics"" API that hides listeners and multi-threading complexities. However, in a streaming context, metrics are not finite in nature. Could you provide further details on this? Are you suggesting an asynchronous get operation? If so, it would be beneficial to see some pseudo-code illustrating how to use metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this viewpoint?";requirement_debt
"Regarding your questions about non-event-based processing of metrics, can you provide a common use case in streaming where this approach would be beneficial? The Observation API introduces a convenient ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in the streaming context, metrics are not finite and continue to evolve. Could you provide further details on this? Are you referring to an asynchronous get operation? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be retrieved asynchronously. Additionally, it's important to consider that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to use Dataset.observe in single-threaded scenarios. Users experienced in multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Does this align with your understanding?";requirement_debt
"Can you provide a typical streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces an API for obtaining metrics with a ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite in nature. Could you provide more details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code illustrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this assessment?";requirement_debt
"Can you provide a real-world streaming use case where non-event-based processing of metrics would be valuable? The Observation API introduces an API for obtaining metrics with a ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite in nature. Could you provide further details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this perspective?";requirement_debt
"Can you provide an example of a common streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces an API that offers a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite and have a continuous nature. Could you provide more details on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to use metrics asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is executed from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Does this resonate with your understanding?";requirement_debt
"Can you provide a practical streaming use case where non-event-based processing of metrics would be useful? The Observation API introduces a simplified ""get me the metrics"" API that abstracts away listeners and multi-threading complexities. However, in the streaming context, metrics are not finite in nature. Could you provide more details on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to use metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target audience for this extension. Would you concur with this analysis?";requirement_debt
"In terms of non-event-based processing of metrics, can you provide a common use case in streaming where this approach would be beneficial? The Observation API offers a convenient ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in a streaming context, metrics are not finite. Can you provide further details on this? Are you referring to an asynchronous get operation? If so, it would be great to see some pseudo-code demonstrating how to use metrics asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is executed from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the general use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to use Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you share this viewpoint?";requirement_debt
"Can you provide a real-world streaming use case where non-event-based processing of metrics would be valuable? The Observation API introduces an API that offers a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite in nature. Could you provide further details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is called from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, enabling users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this perspective?";requirement_debt
"Can you provide a typical streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces an API for obtaining metrics with a ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite in nature. Could you provide more details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code illustrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the common use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Does this resonate with your understanding?";requirement_debt
"Can you provide a real-world streaming use case where non-event-based processing of metrics would be useful? The Observation API introduces an API for obtaining metrics with a ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite. Could you provide further details on this? Are you suggesting an asynchronous get operation for retrieving metrics? If so, it would be helpful to see some pseudo-code demonstrating how metrics can be asynchronously retrieved. Additionally, it's worth mentioning that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are comfortable with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you agree with this analysis?";requirement_debt
"Can you provide an example of a typical streaming use case where non-event-based processing of metrics would be beneficial? The Observation API introduces a convenient ""get me the metrics"" functionality, abstracting away listeners and multi-threading complexities. However, in streaming scenarios, metrics are not finite. Could you elaborate on this further? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code illustrating how to use metrics asynchronously. Additionally, it's worth noting that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. In my opinion, a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to utilize Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target users of this extension. Do you concur with this perspective?";requirement_debt
"Can you provide a practical streaming use case where non-event-based processing of metrics would be useful? The Observation API introduces a convenient ""get me the metrics"" functionality that abstracts away listeners and multi-threading complexities. However, in the streaming context, metrics are not finite. Can you provide further details on this? Are you suggesting an asynchronous get operation? If so, it would be helpful to see some pseudo-code demonstrating how to use metrics asynchronously. Additionally, it's important to note that the current implementation of the Observation API is not thread-safe when the action on the observed dataset is invoked from a different thread than where the metrics are retrieved. I believe that a multi-threaded driver process is not the typical use case for batch processing. Even if it were, the proposed Observation API aims to simplify the handling of multi-threading complexities, allowing users to use Dataset.observe in single-threaded scenarios. Experienced users who are familiar with multi-threading can continue to use the existing listeners approach, as they are not the primary target audience for this extension. Would you concur with this analysis?";requirement_debt
"To fix MiMa, you need to add a line to the `mimaProjects` definition to temporarily exclude the new project: https://github.com/dbtsai/spark/blob/b9870d6c62d75cd59698bd72751bc4f7854cd2e3/project/SparkBuild.scala#L254
Also, add a TODO and followup task for post-2.0-release to remove the exclude once 2.0 has been published and there is a previous artifact for MiMa to compare against.";requirement_debt
To resolve the MiMa issue, we can include a line in the `mimaProjects` definition to exclude the new project temporarily. You can find an example of how to do this here: [link to the relevant code snippet](https://github.com/dbtsai/spark/blob/b9870d6c62d75cd59698bd72751bc4f7854cd2e3/project/SparkBuild.scala#L254). Additionally, it would be helpful to add a TODO comment and a follow-up task to remove the exclusion after the 2.0 release, once there is a previous artifact available for MiMa to compare against.;requirement_debt
"To resolve the MiMa issue, please modify the mimaProjects definition by including a line that excludes the new project temporarily. You can refer to the specific location here.

Additionally, make sure to add a TODO comment and a follow-up task after the 2.0 release, indicating the removal of the exclusion once the release is published and a previous artifact is available for MiMa comparison.";requirement_debt
"In order to address the MiMa issue, an additional line should be added to the mimaProjects definition, excluding the new project temporarily. You can refer to the code snippet here.

Furthermore, it is recommended to include a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and there is a previous artifact available for MiMa comparison.";requirement_debt
"To resolve the MiMa issue, please update the mimaProjects definition by temporarily excluding the new project. You can find an example of this exclusion here.

Additionally, remember to include a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and there is a previous artifact available for MiMa comparison.";requirement_debt
"In order to fix the MiMa issue, it is necessary to add a line to the mimaProjects definition, excluding the new project temporarily. You can refer to the code snippet at this location.

Additionally, make sure to add a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and there is a previous artifact available for MiMa comparison.";requirement_debt
"To address the MiMa issue, you need to modify the mimaProjects definition by adding a line to temporarily exclude the new project. You can find an example of this exclusion here.

Additionally, it is important to include a TODO comment and a follow-up task for post-2.0 release, indicating the need to remove the exclusion once 2.0 has been published and there is a previous artifact available for MiMa comparison.";requirement_debt
"In order to resolve the MiMa issue, please include an additional line in the mimaProjects definition, excluding the new project temporarily. You can refer to the relevant code snippet here.

Additionally, remember to add a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and there is a previous artifact available for MiMa comparison.";requirement_debt
"To fix the MiMa issue, it is necessary to update the mimaProjects definition by adding a line that temporarily excludes the new project. You can find an example of this exclusion in the code snippet located here.

Additionally, make sure to include a TODO comment and a follow-up task to remove the exclusion post-2.0 release. This should be done after the release is published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"In order to address the MiMa issue, it is necessary to add a line to the mimaProjects definition, temporarily excluding the new project. You can find an example of this in the following location: GitHub link.

Additionally, it is recommended to add a TODO comment and a follow-up task for post-2.0 release, indicating the need to remove the exclusion once version 2.0 has been published and a previous artifact is available for MiMa to compare against.";requirement_debt
"To resolve the MiMa issue, please modify the mimaProjects definition by including a line that excludes the new project temporarily. You can refer to the specific location here.

Additionally, make sure to add a TODO comment and a follow-up task after the 2.0 release, indicating the removal of the exclusion once the release is published and a previous artifact is available for MiMa comparison.";requirement_debt
"In order to address the MiMa issue, an additional line should be added to the mimaProjects definition, excluding the new project temporarily. You can refer to the code snippet here.

Furthermore, it is recommended to include a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and a previous artifact is available for MiMa comparison.";requirement_debt
"To resolve the MiMa issue, please update the mimaProjects definition by temporarily excluding the new project. You can find an example of this exclusion here.

Additionally, remember to include a TODO comment and a follow-up task to remove the exclusion after the 2.0 release. This should be done once the release is published and a previous artifact is available for MiMa comparison.";requirement_debt
"In order to address the issue with MiMa, it is necessary to include a line in the mimaProjects definition that temporarily excludes the new project. You can find the specific line of code that needs to be added in this link: GitHub - SparkBuild.scala.

Additionally, please make sure to add a TODO comment and a follow-up task for post-2.0-release. The purpose of this task is to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To resolve the issue with MiMa, you can modify the mimaProjects definition in the SparkBuild.scala file by excluding the new project temporarily. The specific line of code that needs to be added can be found in this link: GitHub - SparkBuild.scala.

Additionally, please include a TODO comment and a follow-up task for post-2.0-release. This task aims to remove the exclusion once version 2.0 has been published and a previous artifact is available for MiMa to compare against.";requirement_debt
"In order to resolve the MiMa issue, it is necessary to temporarily exclude the new project by adding a line to the mimaProjects definition. You can find the specific line that needs to be added in the following link: GitHub - SparkBuild.scala.

Additionally, please include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To address the MiMa issue, it is necessary to modify the mimaProjects definition by excluding the new project temporarily. This can be achieved by adding a specific line of code, as mentioned in this link: GitHub - SparkBuild.scala.

Additionally, don't forget to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"In order to fix the MiMa issue, it is necessary to temporarily exclude the new project from the mimaProjects definition. You can do this by adding a specific line of code, which is provided in this link: GitHub - SparkBuild.scala.

Additionally, remember to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To resolve the MiMa issue, it is necessary to add a line to the mimaProjects definition in order to temporarily exclude the new project. The specific line of code that needs to be added can be found in this link: GitHub - SparkBuild.scala.

Additionally, make sure to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"In order to fix the MiMa issue, you need to temporarily exclude the new project from the mimaProjects definition. This can be done by adding a specific line of code, as indicated in this link: GitHub - SparkBuild.scala.

Additionally, don't forget to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To address the MiMa issue, it is necessary to modify the mimaProjects definition by excluding the new project temporarily. You can find the specific line of code that needs to be added in this link: GitHub - SparkBuild.scala.

Additionally, please include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"In order to resolve the MiMa issue, it is necessary to temporarily exclude the new project from the mimaProjects definition. You can do this by adding a specific line of code, which is provided in this link: GitHub - SparkBuild.scala.

Additionally, remember to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To fix the MiMa issue, it is necessary to add a line to the mimaProjects definition in order to temporarily exclude the new project. You can find the specific line of code that needs to be added in this link: GitHub - SparkBuild.scala.

Additionally, make sure to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"In order to fix the MiMa issue, you need to temporarily exclude the new project from the mimaProjects definition. This can be done by adding a specific line of code, as indicated in this link: GitHub - SparkBuild.scala.

Additionally, don't forget to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
"To address the MiMa issue, you need to modify the mimaProjects definition by temporarily excluding the new project. You can find the specific line of code that needs to be added in this link: GitHub - SparkBuild.scala.

Additionally, make sure to include a TODO comment and a follow-up task for post-2.0-release to remove the exclusion once version 2.0 has been published and there is a previous artifact available for MiMa to compare against.";requirement_debt
Well, it's definitely sketchy to be calling aggregate and get concurrently. HyperLogLogCollector isn't thread safe. It's possible that you'll get bizarre values from time to time, like if the offset of an HLLC is in the process of being incremented in one thread while it'sÂ being read in another thread. So I think this PR has value.;requirement_debt
It is certainly concerning to call aggregate and get concurrently, especially considering that HyperLogLogCollector is not thread-safe. This could lead to unpredictable values, particularly when an HLLC offset is being incremented in one thread while being read in another. Given this situation, I believe that this pull request holds significance.;requirement_debt
It is important to note the potential risks involved in concurrently calling aggregate and get, particularly when dealing with HyperLogLogCollector, which is not thread-safe. This could result in unexpected values, especially if an HLLC offset is being modified in one thread while being accessed in another. Considering these concerns, I believe that this pull request brings value.;requirement_debt
The fact that HyperLogLogCollector is not thread-safe raises legitimate concerns about concurrently calling aggregate and get. In such scenarios, there is a possibility of encountering peculiar values, particularly when an HLLC offset is being modified in one thread while being accessed in another. Given these considerations, I believe that this pull request holds significance.;requirement_debt
The lack of thread safety in HyperLogLogCollector raises valid concerns about concurrently calling aggregate and get. This situation could lead to unexpected values, especially if an HLLC offset is being modified in one thread while being read in another. In light of these considerations, I believe that this pull request provides value.;requirement_debt
It is important to acknowledge the potential issues arising from calling aggregate and get concurrently, particularly with a non-thread-safe component like HyperLogLogCollector. This could result in unpredictable values, especially when an HLLC offset is being modified in one thread while being accessed in another. Taking these factors into account, I believe that this pull request offers value.;requirement_debt
It's crucial to note that calling aggregate and get concurrently is not recommended as the HyperLogLogCollector is not thread-safe. This can lead to unpredictable and unusual values, especially if an HLLC's offset is being incremented in one thread while it's being read in another thread. Therefore, I believe this pull request holds significance in addressing this issue.;requirement_debt
It's important to acknowledge the potential risks of calling aggregate and get concurrently, as the HyperLogLogCollector is not designed to be thread-safe. This can result in unexpected and irregular values, particularly when an HLLC's offset is being modified in one thread while being accessed in another. Considering these circumstances, I believe this pull request is valuable in addressing this concern.;requirement_debt
It's worth mentioning that calling aggregate and get concurrently can lead to undesired outcomes due to the lack of thread-safety in the HyperLogLogCollector. This can cause anomalies in the values, especially when an HLLC's offset is being modified in one thread while being retrieved in another. Given these considerations, I believe this pull request provides value by addressing this issue.;requirement_debt
It is crucial to be aware of the potential risks associated with calling aggregate and get concurrently, as the HyperLogLogCollector is not thread-safe. This can result in unexpected and inconsistent values, particularly when modifications to an HLLC's offset are made in one thread while it is being accessed in another. With these considerations in mind, I believe this pull request is valuable in addressing this concern.;requirement_debt
It is important to note that calling aggregate and get concurrently raises concerns since HyperLogLogCollector is not thread-safe. There is a possibility of obtaining inconsistent or unexpected values, especially when an HLLC's offset is being incremented in one thread while being read in another. Considering this, I believe that this PR holds significant value.;requirement_debt
It's worth highlighting that invoking both aggregate and get concurrently is questionable due to the lack of thread-safety in HyperLogLogCollector. In such scenarios, there is a risk of encountering irregular or unexpected values. For instance, if one thread increments the offset of an HLLC while another thread attempts to read it simultaneously. Given these considerations, I believe that this PR brings substantial value.;requirement_debt
It is important to be cautious when calling both aggregate and get concurrently since HyperLogLogCollector lacks thread-safety. This can lead to unexpected or inconsistent values, particularly when an HLLC's offset is being modified in one thread while it is being accessed in another. Considering these implications, I believe that this PR is highly valuable.;requirement_debt
It is worth noting the potential risks associated with concurrently invoking aggregate and get due to the absence of thread-safety in HyperLogLogCollector. In such cases, there is a possibility of obtaining unusual or inconsistent values, especially when an HLLC's offset is being modified in one thread while being accessed in another. Considering these factors, I believe that this PR holds significant value.;requirement_debt
It is crucial to exercise caution when simultaneously calling aggregate and get due to the lack of thread-safety in HyperLogLogCollector. This can result in unpredictable or inconsistent values, especially when an HLLC's offset is being incremented in one thread while being read in another. Given these considerations, I strongly believe that this PR brings substantial value.;requirement_debt
It is important to be aware of the potential issues that may arise from concurrently calling aggregate and get, as HyperLogLogCollector is not thread-safe. This can lead to unexpected or inconsistent values, particularly when one thread modifies the offset of an HLLC while another thread attempts to access it. Considering these factors, I believe that this PR is highly valuable.;requirement_debt
Calling both aggregate and get concurrently raises concerns about the thread safety of HyperLogLogCollector. In such scenarios, the risk of obtaining abnormal or inconsistent values is present, especially when an HLLC's offset is being modified in one thread while being accessed in another. Given these factors, I believe that this PR is of great value.;requirement_debt
It's certainly risky to simultaneously invoke aggregate and get methods. The HyperLogLogCollector is not designed to be thread-safe, which means that concurrent access could result in unexpected values. For instance, if one thread is incrementing the offset of an HLLC while another thread is reading it, you may encounter unusual values. Considering this, I believe that this pull request holds significance.;requirement_debt
There is a clear concern when it comes to invoking aggregate and get concurrently. The HyperLogLogCollector is not thread-safe, meaning that simultaneous access from multiple threads could lead to unpredictable outcomes. For instance, if one thread is incrementing the offset of an HLLC while another thread is attempting to read it, the results may be erratic. Given this potential issue, I believe that this pull request is valuable.;requirement_debt
It is indeed risky to invoke both the aggregate and get methods concurrently. The HyperLogLogCollector does not provide thread safety, which means that accessing it from multiple threads simultaneously can lead to unexpected values. For example, if one thread is incrementing the offset of an HLLC while another thread is reading it, the results could be inconsistent. Considering this, I believe that this pull request is important in addressing the issue.;requirement_debt
Calling the aggregate and get methods concurrently poses a significant risk. The HyperLogLogCollector is not designed to be thread-safe, so accessing it from multiple threads at the same time can produce unpredictable outcomes. For instance, if one thread is incrementing the offset of an HLLC while another thread is in the process of reading it, the values obtained may be nonsensical. Given these concerns, I believe that this pull request holds value in addressing the issue at hand.;requirement_debt
It's certainly risky to simultaneously invoke the aggregate and get methods. The HyperLogLogCollector class is not thread-safe, which means that accessing it concurrently can lead to unexpected outcomes. For instance, there may be cases where the offset of an HLLC is being incremented in one thread while it's being read in another thread. Given these circumstances, I believe this pull request holds significance.;requirement_debt
Calling both the aggregate and get methods concurrently raises concerns. The HyperLogLogCollector class lacks thread-safety, implying that simultaneous access can result in unpredictable behavior. For example, one thread may be incrementing the offset of an HLLC while another thread is attempting to read it. Considering these implications, I believe this pull request offers valuable improvements.;requirement_debt
Combining calls to aggregate and get concurrently introduces risks. The HyperLogLogCollector class lacks thread-safety, meaning that simultaneous access may yield unexpected outcomes. In scenarios where one thread is incrementing the offset of an HLLC while another thread is reading it, unusual values may arise. Given these considerations, I believe this pull request delivers valuable enhancements.;requirement_debt
Invoking both the aggregate and get methods concurrently raises concerns about thread safety. The HyperLogLogCollector class does not provide the necessary guarantees in this regard. Consequently, simultaneous access can lead to unexpected values, especially when one thread is incrementing the offset of an HLLC while another thread is attempting to read it. Considering the potential risks, I find this pull request to be of significant value.;requirement_debt
"Even though `Bindings` is based on a `ThreadLocal<T>` instance, I think the implementation is not thread-safe.
For example: Multiple tasks executed serially on the same thread, modifying the same bindings dictionary.
Besides not being thread-safe, it doesn't support defining a binding on 1 thread and adding the step on another, sample:";requirement_debt
While Bindings relies on a ThreadLocal<T> instance, I believe its implementation lacks thread-safety. Consider a scenario where multiple tasks are executed sequentially on the same thread, all of which modify the same bindings dictionary. Apart from the absence of thread-safety, Bindings also does not facilitate defining a binding on one thread and adding the step on another. Allow me to provide an example for better clarity:;requirement_debt
Despite Bindings being based on a ThreadLocal<T> instance, I have concerns regarding its thread-safety. For instance, if multiple tasks are executed consecutively on the same thread, all of which make modifications to the same bindings dictionary, issues may arise. Furthermore, in addition to its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. To illustrate this point, consider the following example:;requirement_debt
While Bindings utilizes a ThreadLocal<T> instance, I have reservations about its thread-safety implementation. For instance, if multiple tasks are executed sequentially on the same thread and they make modifications to the same bindings dictionary, conflicts may occur. Moreover, apart from its lack of thread-safety, Bindings does not provide support for defining a binding on one thread and adding the step on another. To help clarify this, consider the following example:;requirement_debt
Although Bindings utilizes a ThreadLocal<T> instance, I have doubts about its thread-safety implementation. In situations where multiple tasks are executed in sequence on the same thread and they modify the same bindings dictionary, issues can arise. Furthermore, in addition to its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. To illustrate this point, consider the following example:;requirement_debt
Despite Bindings relying on a ThreadLocal<T> instance, I have concerns about its thread-safety implementation. If multiple tasks are executed serially on the same thread and they modify the same bindings dictionary, conflicts may arise. Moreover, in addition to its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. Allow me to provide an example to illustrate this:;requirement_debt
While Bindings utilizes a ThreadLocal<T> instance, its thread-safety implementation is questionable. In scenarios where multiple tasks are executed sequentially on the same thread and they modify the same bindings dictionary, issues can occur. Additionally, besides its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. To exemplify this, consider the following scenario:;requirement_debt
Despite being based on a ThreadLocal<T> instance, I have reservations about the thread-safety of the Bindings implementation. If multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary, conflicts can arise. Furthermore, in addition to its lack of thread-safety, Bindings does not provide the ability to define a binding on one thread and add the step on another. To illustrate this point, consider the following example:;requirement_debt
While Bindings is built upon a ThreadLocal<T> instance, I question its thread-safety implementation. In cases where multiple tasks are executed serially on the same thread, all of which modify the same bindings dictionary, conflicts can occur. Additionally, apart from its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. Consider the following example to better understand the situation:;requirement_debt
Despite utilizing a ThreadLocal<T> instance, the implementation of Bindings does not appear to be thread-safe. This becomes evident when multiple tasks are executed sequentially on the same thread, all of which modify the same bindings dictionary. In addition to the lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Here's an example to illustrate this scenario:;requirement_debt
While Bindings is implemented using a ThreadLocal<T> instance, it does not guarantee thread-safety. This is particularly evident in situations where multiple tasks are executed sequentially on the same thread and they modify the same bindings dictionary. In addition to its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. Consider the following example to better understand this:;requirement_debt
Despite relying on a ThreadLocal<T> instance, the thread-safety of Bindings is questionable. When multiple tasks are executed one after the other on the same thread, all of which make modifications to the same bindings dictionary, conflicts can arise. Moreover, in addition to its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Let's explore an example to better illustrate this:;requirement_debt
While Bindings utilizes a ThreadLocal<T> instance, its thread-safety implementation is debatable. In scenarios where multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary, issues can emerge. Additionally, in addition to its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. To illustrate this point, consider the following example:;requirement_debt
Despite its reliance on a ThreadLocal<T> instance, the thread-safety of Bindings is in question. When multiple tasks are executed sequentially on the same thread and they modify the same bindings dictionary, conflicts can occur. Moreover, apart from its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. Consider the following example for a clearer understanding:;requirement_debt
While Bindings is based on a ThreadLocal<T> instance, its thread-safety implementation appears to be inadequate. This becomes apparent when multiple tasks are executed serially on the same thread and they modify the same bindings dictionary. Additionally, in addition to its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Allow me to provide an example to illustrate this:;requirement_debt
Despite being implemented using a ThreadLocal<T> instance, Bindings lacks proper thread-safety measures. When multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary, conflicts may arise. Furthermore, besides its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. To better comprehend this situation, consider the following example:;requirement_debt
Although Bindings relies on a ThreadLocal<T> instance, its thread-safety implementation is questionable. In scenarios where multiple tasks are executed sequentially on the same thread and they modify the same bindings dictionary, conflicts can arise. Additionally, apart from its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Allow me to illustrate this with an example:;requirement_debt
While Bindings is built on a ThreadLocal<T> instance, its thread-safety implementation raises concerns. If multiple tasks are executed in sequence on the same thread and they modify the same bindings dictionary, conflicts may occur. Moreover, besides its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. Consider the following example for better understanding:;requirement_debt
Despite utilizing a ThreadLocal<T> instance, the thread-safety of Bindings is not guaranteed. This becomes evident when multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary. Moreover, apart from its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Here's an example to further illustrate the situation:;requirement_debt
While Bindings is based on a ThreadLocal<T> instance, its thread-safety implementation is questionable. In cases where multiple tasks are executed sequentially on the same thread and they modify the same bindings dictionary, conflicts can occur. Additionally, besides its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. To better understand this, consider the following example:;requirement_debt
Despite relying on a ThreadLocal<T> instance, the thread-safety of Bindings is uncertain. When multiple tasks are executed in succession on the same thread and they modify the same bindings dictionary, issues can arise. Furthermore, apart from its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. Allow me to provide an example to clarify this:;requirement_debt
While Bindings utilizes a ThreadLocal<T> instance, its thread-safety implementation is not comprehensive. In scenarios where multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary, conflicts may arise. Additionally, in addition to its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Consider the following example for better comprehension:;requirement_debt
Despite being based on a ThreadLocal<T> instance, the thread-safety of Bindings is not guaranteed. If multiple tasks are executed serially on the same thread and they modify the same bindings dictionary, conflicts can occur. Furthermore, in addition to its lack of thread-safety, Bindings does not facilitate defining a binding on one thread and adding the step on another. Consider the following example to better understand this:;requirement_debt
While Bindings relies on a ThreadLocal<T> instance, its thread-safety implementation raises concerns. If multiple tasks are executed consecutively on the same thread and they modify the same bindings dictionary, conflicts may arise. Moreover, in addition to its lack of thread-safety, Bindings does not allow for defining a binding on one thread and adding the step on another. Allow me to provide an example to illustrate this:;requirement_debt
Despite its utilization of a ThreadLocal<T> instance, the thread-safety implementation of Bindings is questionable. When multiple tasks are executed one after the other on the same thread and they modify the same bindings dictionary, conflicts can occur. Additionally, apart from its lack of thread-safety, Bindings does not support defining a binding on one thread and adding the step on another. Here's an example to provide further clarity:;requirement_debt
"I gave it a try at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 but I am quite unsure if this is what we were talking about. Consequently, I dropped some FIXME/TODO questions.
For this reason, I didn't open a related (new) PR yet. Maybe you can have a look at it?";requirement_debt
I made an attempt to address the concerns raised at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely certain if this aligns with our previous discussions. As a result, I left some FIXME/TODO questions in the code. Consequently, I haven't opened a new related pull request yet. It would be great if you could take a look and provide your feedback.;requirement_debt
I took a shot at implementing the changes discussed at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm unsure if this is exactly what we were referring to. As a result, I added some FIXME/TODO questions to address the remaining uncertainties. Therefore, I haven't created a new pull request related to this yet. I would appreciate it if you could review the changes and share your thoughts.;requirement_debt
I made an attempt to incorporate the suggested modifications at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, although I'm not entirely certain if this aligns with our previous discussions. To address any remaining doubts, I left some FIXME/TODO questions in the code. Consequently, I haven't opened a new pull request related to this yet. I would appreciate it if you could take a look and provide your input.;requirement_debt
I gave it a try by implementing the changes mentioned at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm unsure if this is exactly what we discussed. Therefore, I included some FIXME/TODO questions in the code to address any remaining uncertainties. As a result, I haven't created a new pull request specifically for this yet. I would appreciate it if you could review the changes and share your thoughts.;requirement_debt
I attempted to address the issues raised at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely certain if this is precisely what we had in mind. Consequently, I added some FIXME/TODO questions to tackle any remaining doubts. Thus, I haven't opened a new pull request related to this matter at the moment. It would be great if you could take a look and provide your insights.;requirement_debt
I made an attempt to address the issue at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm uncertain if this aligns with our previous discussions. As a result, I added some FIXME/TODO questions to highlight areas of uncertainty. Consequently, I haven't opened a new PR related to this yet. I would appreciate it if you could take a look and provide your feedback.;requirement_debt
I took a shot at resolving the problem on the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely sure if this is what we discussed earlier. As a result, I included some FIXME/TODO questions to address areas of uncertainty. Therefore, I haven't created a new PR specifically related to this yet. I would appreciate it if you could review the changes and provide your insights.;requirement_debt
I made an attempt to implement the solution at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely sure if it matches our previous discussions. Consequently, I added some FIXME/TODO questions to address areas of uncertainty. For this reason, I haven't opened a new PR related to this yet. I would appreciate it if you could take a look and provide your feedback.;requirement_debt
I tried to address the issue on the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm unsure if it aligns with our previous discussions. As a result, I included some FIXME/TODO questions to highlight areas of uncertainty. Consequently, I haven't created a new PR specifically related to this yet. I kindly request you to review the changes and share your thoughts.;requirement_debt
I made an attempt to resolve the problem at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely certain if this is what we discussed earlier. Hence, I added some FIXME/TODO questions to address areas of uncertainty. Therefore, I haven't opened a new PR specifically related to this yet. Your feedback and input would be highly appreciated.;requirement_debt
I gave it a try on the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 to address the issue, but I'm unsure if it corresponds to our previous discussions. As a result, I included some FIXME/TODO questions to address areas of uncertainty. Consequently, I haven't created a new PR specifically related to this yet. I would greatly appreciate it if you could review the changes and provide your insights.;requirement_debt
I attempted to tackle the issue on the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm not entirely confident if it matches our previous discussions. For this reason, I added some FIXME/TODO questions to address areas of uncertainty. As a result, I haven't opened a new PR specifically related to this yet. Your feedback and review would be highly valuable.;requirement_debt
I took a shot at implementing it in this branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm not entirely confident if this is what we discussed. As a result, I added some FIXME/TODO questions. Due to these uncertainties, I haven't opened a new PR yet. Could you please take a look and provide your feedback?;requirement_debt
I made an attempt to address it in this branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm uncertain if this aligns with our previous discussions. As a result, I included some FIXME/TODO questions to highlight areas that require clarification. Consequently, I haven't created a related PR at this stage. I would appreciate it if you could review the branch and provide your insights.;requirement_debt
I gave it a try by working on this branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm not entirely sure if this is what we had in mind. Therefore, I added some FIXME/TODO questions to address any uncertainties. As a result, I haven't opened a new PR yet. I would greatly appreciate it if you could take a look at the branch and share your thoughts.;requirement_debt
I made an attempt to implement it in this branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm not entirely confident if this aligns with our previous discussions. To address this, I added some FIXME/TODO questions to seek clarification. Hence, I haven't opened a new PR at this point. I kindly request you to review the branch and provide your insights.;requirement_debt
I took a shot at addressing it in this branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. Nevertheless, I'm uncertain if this fully captures our previous discussions. Consequently, I included some FIXME/TODO questions to highlight areas that require further clarification. Due to these uncertainties, I haven't created a related PR yet. Your review and input on the branch would be highly appreciated.;requirement_debt
I attempted the changes in the following branch: https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm uncertain if this aligns with our previous discussion. As a result, I added some FIXME/TODO comments to address any remaining questions. Due to this uncertainty, I haven't opened a related pull request yet. Would you mind taking a look and providing your feedback?;requirement_debt
I made an attempt at implementing the changes in the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm unsure if this exactly matches what we discussed. Consequently, I included some FIXME/TODO questions to address any remaining uncertainties. Because of this, I haven't opened a new pull request yet. Would you be able to review it and provide your input?;requirement_debt
I gave it a try by making the changes in the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2. However, I'm not entirely certain if this is exactly what we discussed earlier. As a result, I added some FIXME/TODO questions to clarify any remaining doubts. Due to this uncertainty, I haven't opened a new pull request related to these changes. Would you mind reviewing the branch and sharing your thoughts?;requirement_debt
I attempted the modifications in the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 as we discussed. However, I'm not entirely sure if this implementation aligns with our previous conversation. To address any remaining uncertainties, I included some FIXME/TODO questions. Consequently, I haven't opened a new pull request yet. Could you please review the branch and provide your feedback?;requirement_debt
I made an effort to incorporate the changes in the branch https://github.com/rzo1/tomee/tree/TOMEE-2324-v2 based on our discussion. However, I'm uncertain if this exactly matches our initial understanding. To address any remaining doubts, I added some FIXME/TODO comments. Consequently, I haven't opened a new pull request for these changes. Would you be able to review the branch and provide your insights?;requirement_debt
I attempted to address the issue at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm uncertain if it aligns with our previous discussions. As a result, I added some FIXME/TODO questions. Due to this uncertainty, I haven't opened a new related pull request yet. Could you please take a look and provide your feedback?;requirement_debt
I made an attempt to tackle the problem at https://github.com/rzo1/tomee/tree/TOMEE-2324-v2, but I'm unsure if it aligns with our previous discussions. Consequently, I left some FIXME/TODO questions to address any uncertainties. For this reason, I haven't opened a new pull request related to this yet. I would appreciate it if you could review it and share your thoughts.;requirement_debt
"if all is null --> code will use indexes list
if all is not null --> code will get all the indexes on that table. 
will be implemented in the upcoming PR.";requirement_debt
In the case where all values are null, the code will utilize the indexes list. Conversely, if all values are not null, the code will retrieve all the indexes associated with that table. This functionality will be implemented in the forthcoming pull request.;requirement_debt
If all values are null, the code will employ the indexes list. Conversely, if all values are not null, the code will fetch all the indexes linked to the specified table. The implementation of this feature will be included in an upcoming pull request.;requirement_debt
When all values are null, the code will utilize the indexes list. On the other hand, if all values are not null, the code will retrieve all the indexes associated with the table. This enhancement is planned to be implemented in an upcoming pull request.;requirement_debt
In the event that all values are null, the code will make use of the indexes list. However, if all values are not null, the code will fetch all the indexes belonging to the specified table. This implementation will be included in a future pull request.;requirement_debt
When all values are null, the code will rely on the indexes list. However, if all values are not null, the code will obtain all the indexes associated with the table. The necessary changes to support this functionality will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will use the indexes list. Conversely, if all values are not null, the code will retrieve all the indexes pertaining to the specified table. These modifications will be introduced in an upcoming pull request.;requirement_debt
When all values are null, the code will utilize the indexes list. In contrast, if all values are not null, the code will fetch all the indexes associated with the table. This enhancement will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will employ the indexes list. On the other hand, if all values are not null, the code will retrieve all the indexes linked to the specified table. This feature will be implemented in an upcoming pull request.;requirement_debt
In the case where all values are null, the code will make use of the indexes list. Conversely, if all values are not null, the code will fetch all the indexes associated with the table. The implementation of this functionality is planned for an upcoming pull request.;requirement_debt
When all values are null, the code will rely on the indexes list. However, if all values are not null, the code will obtain all the indexes belonging to the specified table. These changes will be implemented in a forthcoming pull request.;requirement_debt
If all values are null, the code will use the indexes list. Conversely, if all values are not null, the code will retrieve all the indexes pertaining to the specified table. The necessary modifications for this functionality will be included in an upcoming pull request.;requirement_debt
When all values are null, the code will utilize the indexes list. On the other hand, if all values are not null, the code will fetch all the indexes associated with the table. These updates will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will employ the indexes list. However, if all values are not null, the code will retrieve all the indexes linked to the specified table. The implementation of this feature will be introduced in an upcoming pull request.;requirement_debt
In the event that all values are null, the code will rely on the indexes list. Conversely, if all values are not null, the code will fetch all the indexes belonging to the specified table. This enhancement will be included in a future pull request.;requirement_debt
When all values are null, the code will use the indexes list. Conversely, if all values are not null, the code will obtain all the indexes associated with the table. The necessary changes to support this functionality will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will utilize the indexes list. On the other hand, if all values are not null, the code will retrieve all the indexes pertaining to the specified table. These modifications will be included in an upcoming pull request.;requirement_debt
When all values are null, the code will employ the indexes list. In contrast, if all values are not null, the code will fetch all the indexes associated with the table. This enhancement will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will rely on the indexes list. Conversely, if all values are not null, the code will obtain all the indexes linked to the specified table. This feature will be implemented in an upcoming pull request.;requirement_debt
In the case where all values are null, the code will make use of the indexes list. Conversely, if all values are not null, the code will fetch all the indexes associated with the table. The implementation of this functionality is planned for an upcoming pull request.;requirement_debt
When all values are null, the code will utilize the indexes list. However, if all values are not null, the code will retrieve all the indexes pertaining to the specified table. These changes will be implemented in a forthcoming pull request.;requirement_debt
If all values are null, the code will use the indexes list. Conversely, if all values are not null, the code will retrieve all the indexes belonging to the specified table. The necessary modifications for this functionality will be included in an upcoming pull request.;requirement_debt
When all values are null, the code will rely on the indexes list. On the other hand, if all values are not null, the code will fetch all the indexes associated with the table. These updates will be implemented in an upcoming pull request.;requirement_debt
If all values are null, the code will employ the indexes list. However, if all values are not null, the code will obtain all the indexes linked to the specified table. The implementation of this feature will be introduced in an upcoming pull request.;requirement_debt
In the event that all values are null, the code will rely on the indexes list. Conversely, if all values are not null, the code will fetch all the indexes belonging to the specified table. This enhancement will be included in a future pull request.;requirement_debt
I think `DEL_NOT_SATISFIED` is more safe. Or you'd better change DCHECK to CHECK;requirement_debt
In my opinion, using DEL_NOT_SATISFIED would be a safer option. Alternatively, it would be advisable to replace DCHECK with CHECK.;requirement_debt
From my perspective, opting for DEL_NOT_SATISFIED would be a more secure approach. Alternatively, it would be preferable to switch the DCHECK statement to a CHECK statement.;requirement_debt
In my view, utilizing DEL_NOT_SATISFIED would provide a higher level of safety. Alternatively, it would be advisable to convert the DCHECK to a CHECK statement.;requirement_debt
I believe that choosing DEL_NOT_SATISFIED would be a more prudent choice. Alternatively, it would be wise to replace the DCHECK with a CHECK statement.;requirement_debt
In my perspective, using DEL_NOT_SATISFIED would be a more cautious decision. Alternatively, it would be preferable to modify the DCHECK to a CHECK statement.;requirement_debt
I think opting for DEL_NOT_SATISFIED would be a more secure choice. Alternatively, it would be better to transform the DCHECK into a CHECK statement.;requirement_debt
From my point of view, utilizing DEL_NOT_SATISFIED would be a safer option. Alternatively, it would be advisable to switch the DCHECK to a CHECK statement.;requirement_debt
In my opinion, selecting DEL_NOT_SATISFIED would be a more prudent decision. Alternatively, it would be wise to change the DCHECK to a CHECK statement.;requirement_debt
I believe that using DEL_NOT_SATISFIED is a safer approach. Alternatively, it would be better to replace the DCHECK with a CHECK statement.;requirement_debt
In my perspective, opting for DEL_NOT_SATISFIED would provide a higher level of safety. Alternatively, it would be preferable to convert the DCHECK to a CHECK statement.;requirement_debt
I think choosing DEL_NOT_SATISFIED is a more cautious decision. Alternatively, it would be advisable to transform the DCHECK into a CHECK statement.;requirement_debt
From my point of view, utilizing DEL_NOT_SATISFIED would be a more secure choice. Alternatively, it would be better to switch the DCHECK to a CHECK statement.;requirement_debt
In my opinion, selecting DEL_NOT_SATISFIED would be a more prudent option. Alternatively, it would be wise to change the DCHECK to a CHECK statement.;requirement_debt
I believe that using DEL_NOT_SATISFIED is a safer choice. Alternatively, it would be preferable to replace the DCHECK with a CHECK statement.;requirement_debt
In my perspective, opting for DEL_NOT_SATISFIED would provide a higher level of safety. Alternatively, it would be advisable to convert the DCHECK to a CHECK statement.;requirement_debt
I think choosing DEL_NOT_SATISFIED is a more cautious approach. Alternatively, it would be better to transform the DCHECK into a CHECK statement.;requirement_debt
From my point of view, utilizing DEL_NOT_SATISFIED would be a more secure decision. Alternatively, it would be advisable to switch the DCHECK to a CHECK statement.;requirement_debt
In my opinion, selecting DEL_NOT_SATISFIED would be a more prudent choice. Alternatively, it would be wise to change the DCHECK to a CHECK statement.;requirement_debt
I believe that using DEL_NOT_SATISFIED is a safer option. Alternatively, it would be preferable to replace the DCHECK with a CHECK statement.;requirement_debt
In my perspective, opting for DEL_NOT_SATISFIED would provide a higher level of safety. Alternatively, it would be advisable to convert the DCHECK to a CHECK statement.;requirement_debt
I think choosing DEL_NOT_SATISFIED is a more cautious decision. Alternatively, it would be better to transform the DCHECK into a CHECK statement.;requirement_debt
From my point of view, utilizing DEL_NOT_SATISFIED would be a more secure choice. Alternatively, it would be wise to switch the DCHECK to a CHECK statement.;requirement_debt
In my opinion, selecting DEL_NOT_SATISFIED would be a more prudent approach. Alternatively, it would be advisable to change the DCHECK to a CHECK statement.;requirement_debt
I believe that using DEL_NOT_SATISFIED is a safer choice. Alternatively, it would be preferable to replace the DCHECK with a CHECK statement.;requirement_debt
"Fixes https://github.com/apache/cloudstack/issues/4481
TODO";requirement_debt
Resolves the issue mentioned in https://github.com/apache/cloudstack/issues/4481. TODO: Further improvements may be required.;requirement_debt
Addresses the problem reported in https://github.com/apache/cloudstack/issues/4481. Additional work may be needed. TODO: Review and refine the solution.;requirement_debt
Fixes the issue documented in https://github.com/apache/cloudstack/issues/4481. Please note that there may be outstanding tasks to complete. TODO: Validate and enhance the fix as necessary.;requirement_debt
Resolves the issue identified in https://github.com/apache/cloudstack/issues/4481. It is important to note that there may be additional tasks to be completed. TODO: Verify and enhance the fix as required.;requirement_debt
Fixes the problem reported in https://github.com/apache/cloudstack/issues/4481. However, there may still be some pending work remaining. TODO: Evaluate and improve the solution if needed.;requirement_debt
Resolves the issue described in https://github.com/apache/cloudstack/issues/4481. However, please be aware that there may be outstanding tasks that need attention. TODO: Review and refine the solution as necessary.;requirement_debt
Addresses the issue outlined in https://github.com/apache/cloudstack/issues/4481. Please note that additional work may be required. TODO: Validate and enhance the fix as needed.;requirement_debt
Fixes the problem stated in https://github.com/apache/cloudstack/issues/4481. However, there may be unfinished tasks that need to be completed. TODO: Verify and improve the solution as necessary.;requirement_debt
Resolves the issue reported in https://github.com/apache/cloudstack/issues/4481. It is important to note that there may be remaining tasks to be addressed. TODO: Evaluate and enhance the fix as required.;requirement_debt
Fixes the issue highlighted in https://github.com/apache/cloudstack/issues/4481. However, there may still be some outstanding work to be done. TODO: Review and refine the solution if needed.;requirement_debt
Resolves the problem documented in https://github.com/apache/cloudstack/issues/4481. Please note that there may be additional tasks that need to be completed. TODO: Validate and enhance the fix as necessary.;requirement_debt
Fixes the issue mentioned in https://github.com/apache/cloudstack/issues/4481. However, please be aware that there may be unfinished tasks that require attention. TODO: Verify and improve the solution as needed.;requirement_debt
Addresses the issue reported in https://github.com/apache/cloudstack/issues/4481. Please note that additional work may be necessary. TODO: Validate and enhance the fix as required.;requirement_debt
Fixes the problem outlined in https://github.com/apache/cloudstack/issues/4481. However, there may be pending tasks that need to be completed. TODO: Review and refine the solution if needed.;requirement_debt
Resolves the issue stated in https://github.com/apache/cloudstack/issues/4481. It is important to note that there may be remaining tasks that require attention. TODO: Evaluate and enhance the fix as required.;requirement_debt
Fixes the issue reported in https://github.com/apache/cloudstack/issues/4481. However, there may still be some unfinished work to be done. TODO: Verify and improve the solution as necessary.;requirement_debt
Resolves the problem highlighted in https://github.com/apache/cloudstack/issues/4481. Please note that there may be additional tasks to be completed. TODO: Validate and enhance the fix as required.;requirement_debt
Fixes the issue documented in https://github.com/apache/cloudstack/issues/4481. However, there may be outstanding tasks that need attention. TODO: Review and refine the solution as necessary.;requirement_debt
Addresses the issue described in https://github.com/apache/cloudstack/issues/4481. Please note that additional work may be required. TODO: Validate and enhance the fix as needed.;requirement_debt
Fixes the problem stated in https://github.com/apache/cloudstack/issues/4481. However, there may be unfinished tasks that need to be completed. TODO: Verify and improve the solution as necessary.;requirement_debt
Resolves the issue reported in https://github.com/apache/cloudstack/issues/4481. It is important to note that there may be remaining tasks to be addressed. TODO: Evaluate and enhance the fix as required.;requirement_debt
Fixes the issue highlighted in https://github.com/apache/cloudstack/issues/4481. However, there may still be some outstanding work to be done. TODO: Review and refine the solution if needed.;requirement_debt
Resolves the problem documented in https://github.com/apache/cloudstack/issues/4481. Please note that there may be additional tasks that need to be completed. TODO: Validate and enhance the fix as necessary.;requirement_debt
Fixes the issue mentioned in https://github.com/apache/cloudstack/issues/4481. However, please be aware that there may be unfinished tasks that require attention. TODO: Verify and improve the solution as needed.;requirement_debt
Seems like it doesn't support special characters in the column name now. Can we keep the support?;requirement_debt
It appears that the current implementation does not support special characters in the column name. Is it possible to retain this support?;requirement_debt
It seems that the current functionality does not handle special characters in the column name. Is there a way to maintain support for these special characters?;requirement_debt
The current implementation does not seem to handle special characters in the column name. Is it feasible to preserve the ability to work with such special characters?;requirement_debt
It is evident that the current support does not accommodate special characters in the column name. Is there a possibility of retaining this support for special characters?;requirement_debt
It seems that the current implementation does not include support for special characters in the column name. Can we explore options to maintain this support?;requirement_debt
The existing functionality appears to lack support for special characters in the column name. Is it possible to retain this support to handle such special characters?;requirement_debt
It appears that the current implementation does not account for special characters in the column name. Can we ensure that support for these special characters is maintained?;requirement_debt
The current functionality does not seem to handle special characters in the column name. Is there a way to retain support for such special characters?;requirement_debt
It seems that the current implementation does not support special characters in the column name. Is it feasible to preserve this support?;requirement_debt
The existing implementation does not appear to handle special characters in the column name. Can we explore options to maintain support for these special characters?;requirement_debt
It is evident that the current support does not accommodate special characters in the column name. Can we find a way to retain this support?;requirement_debt
The current implementation lacks support for special characters in the column name. Is it possible to ensure that this support is maintained?;requirement_debt
It appears that the current functionality does not include special characters in the column name. Can we retain the ability to work with these special characters?;requirement_debt
The existing functionality does not seem to handle special characters in the column name. Can we preserve support for such special characters?;requirement_debt
It seems that the current implementation does not account for special characters in the column name. Can we ensure that support for these special characters is maintained?;requirement_debt
The current functionality does not appear to support special characters in the column name. Can we explore options to retain this support?;requirement_debt
It seems that the current implementation does not include support for special characters in the column name. Is it feasible to preserve this support?;requirement_debt
The existing implementation does not seem to handle special characters in the column name. Can we find a way to maintain support for these special characters?;requirement_debt
It is evident that the current support does not accommodate special characters in the column name. Can we retain this support?;requirement_debt
The current implementation lacks support for special characters in the column name. Can we ensure that this support is maintained?;requirement_debt
It appears that the current functionality does not include special characters in the column name. Can we preserve the ability to work with these special characters?;requirement_debt
The existing functionality does not seem to handle special characters in the column name. Can we retain support for such special characters?;requirement_debt
It seems that the current implementation does not account for special characters in the column name. Can we ensure that support for these special characters is maintained?;requirement_debt
The current functionality does not appear to support special characters in the column name. Can we explore options to retain this support?;requirement_debt
Map is not supported yet so thrown unsupported exception for Map;requirement_debt
The current implementation does not yet support Map, resulting in an unsupported exception being thrown for Map operations.;requirement_debt
Map is not yet supported in the current implementation, which results in an unsupported exception being thrown when Map-related operations are performed.;requirement_debt
The current functionality does not include support for Map operations, leading to an unsupported exception being thrown when attempting to perform such operations.;requirement_debt
Map operations are not yet supported in the current implementation, resulting in an unsupported exception being thrown for Map-related actions.;requirement_debt
The current implementation does not support Map, which causes an unsupported exception to be thrown when Map operations are executed.;requirement_debt
Map functionality is not yet implemented in the current version, resulting in an unsupported exception being thrown for Map-related operations.;requirement_debt
The current implementation lacks support for Map operations, leading to an unsupported exception when attempting to perform Map-related actions.;requirement_debt
Map is not currently supported in the implementation, resulting in an unsupported exception being thrown for Map operations.;requirement_debt
The current functionality does not accommodate Map operations, resulting in an unsupported exception being thrown when such operations are attempted.;requirement_debt
Map functionality is not yet available in the current implementation, causing an unsupported exception to be thrown for Map-related actions.;requirement_debt
The current implementation does not provide support for Map, resulting in an unsupported exception when Map operations are performed.;requirement_debt
Map operations are not yet supported in the current implementation, leading to an unsupported exception being thrown for Map-related actions.;requirement_debt
The current functionality does not include support for Map operations, resulting in an unsupported exception when attempting to perform such operations.;requirement_debt
Map is not yet supported in the current implementation, causing an unsupported exception to be thrown when performing Map-related operations.;requirement_debt
The current implementation lacks support for Map, leading to an unsupported exception being thrown for Map-related actions.;requirement_debt
Map functionality is not yet implemented in the current version, resulting in an unsupported exception when trying to perform Map operations.;requirement_debt
The current implementation does not support Map operations, resulting in an unsupported exception when attempting to perform such actions.;requirement_debt
Map is not currently supported in the implementation, leading to an unsupported exception when Map operations are executed.;requirement_debt
The current functionality does not accommodate Map operations, resulting in an unsupported exception when such operations are performed.;requirement_debt
Map functionality is not yet available in the current implementation, causing an unsupported exception to be thrown for Map-related actions.;requirement_debt
The current implementation does not provide support for Map, resulting in an unsupported exception when trying to perform Map operations.;requirement_debt
Map operations are not yet supported in the current implementation, leading to an unsupported exception when performing actions related to Map.;requirement_debt
The current functionality does not include support for Map operations, resulting in an unsupported exception when attempting to perform such operations.;requirement_debt
Map is not yet supported in the current implementation, causing an unsupported exception to be thrown when attempting Map-related actions.;requirement_debt
"I'm sorry for commenting on such an old (and closed) PR.
Is this feature completely implemented?  It seems that the SchedulerJob class uses a completely different method for loading DAGs than the webserver etc. and it will not schedule tasks from a packaged dag since it only considers raw python files.";requirement_debt
Apologies for commenting on an old and closed PR. Is this feature fully implemented? It appears that the SchedulerJob class uses a different approach for loading DAGs compared to the webserver, causing it to overlook packaged DAGs and only consider raw Python files for task scheduling.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, employ different methods for loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs, focusing only on raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It appears that the SchedulerJob class uses a distinct mechanism for loading DAGs compared to the webserver and other components. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, as it solely considers raw Python files.;requirement_debt
My apologies for commenting on an old and closed PR. Is this feature completely implemented? It seems that the SchedulerJob class and other components, like the webserver, adopt different approaches when loading DAGs. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs since it solely focuses on raw Python files.;requirement_debt
I apologize for the untimely comment on a closed PR. Is this feature fully implemented? Based on my observation, the SchedulerJob class follows a distinct methodology for loading DAGs compared to the webserver and similar components. This disparity may prevent the SchedulerJob class from scheduling tasks originating from packaged DAGs, as it exclusively considers raw Python files.;requirement_debt
Sorry for commenting on a closed PR from a while ago. Is this feature fully implemented? It appears that the SchedulerJob class and other relevant components, such as the webserver, utilize different techniques for loading DAGs. This divergence may result in the SchedulerJob class disregarding packaged DAGs and only considering raw Python files for task scheduling.;requirement_debt
Apologies for the delayed comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class employs a distinct method for loading DAGs compared to the webserver and other components. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, as it solely takes into account raw Python files.;requirement_debt
I'm sorry for commenting on an old and closed PR. Has this feature been fully implemented? It appears that the SchedulerJob class and other relevant components follow different approaches when it comes to loading DAGs. This difference in methodology may cause the SchedulerJob class to overlook packaged DAGs and only consider raw Python files for task scheduling.;requirement_debt
Apologies for the late comment on a closed PR. Is this feature fully implemented? Based on my understanding, the SchedulerJob class and other components adopt different methods for loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it focuses solely on raw Python files.;requirement_debt
Sorry for commenting on an old and closed PR. Is this feature fully implemented? It seems that the SchedulerJob class utilizes a completely different approach for loading DAGs compared to the webserver and other components. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, as it only considers raw Python files.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It appears that the SchedulerJob class and other components, such as the webserver, employ different methods for loading DAGs. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, focusing solely on raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, like the webserver, utilize different approaches when loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it exclusively considers raw Python files.;requirement_debt
Apologies for commenting on an old and closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, adopt different techniques for loading DAGs. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, focusing solely on raw Python files.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It appears that the SchedulerJob class and other components, like the webserver, employ different approaches when loading DAGs. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, as it solely considers raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, utilize different methods for loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it exclusively focuses on raw Python files.;requirement_debt
Apologies for commenting on an old and closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, follow different approaches when loading DAGs. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, focusing solely on raw Python files.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It appears that the SchedulerJob class and other components, like the webserver, adopt different techniques for loading DAGs. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, as it solely considers raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, utilize different approaches when loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it exclusively focuses on raw Python files.;requirement_debt
Apologies for commenting on an old and closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, follow different methods for loading DAGs. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, focusing solely on raw Python files.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It appears that the SchedulerJob class and other components, like the webserver, adopt different techniques for loading DAGs. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, as it solely considers raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, utilize different approaches when loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it exclusively focuses on raw Python files.;requirement_debt
Apologies for commenting on an old and closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, follow different techniques for loading DAGs. Consequently, the SchedulerJob class may not schedule tasks from packaged DAGs, focusing solely on raw Python files.;requirement_debt
I apologize for commenting on a closed PR from the past. Is this feature fully implemented? It appears that the SchedulerJob class and other components, like the webserver, adopt different methods for loading DAGs. This discrepancy may prevent the SchedulerJob class from scheduling tasks from packaged DAGs, as it solely considers raw Python files.;requirement_debt
Sorry for the late comment on a closed PR. Is this feature fully implemented? It seems that the SchedulerJob class and other components, such as the webserver, utilize different approaches when loading DAGs. As a result, the SchedulerJob class may not schedule tasks from packaged DAGs since it exclusively focuses on raw Python files.;requirement_debt
oh i know why. The `InMemoryExternalCatalog` hasn't implemented all the interfaces(e.g. some partition related ones), so we did it intentionally.;requirement_debt
The reason for not supporting special characters in the column name is that the InMemoryExternalCatalog intentionally omitted implementing certain interfaces, such as those related to partitions. This intentional decision was made due to the incomplete implementation of these interfaces.;requirement_debt
Special characters in the column name are not supported due to intentional omissions in the implementation of the InMemoryExternalCatalog. Some interfaces, particularly those associated with partitions, have not been implemented as part of this intentional decision.;requirement_debt
The lack of support for special characters in the column name is intentional and stems from the InMemoryExternalCatalog implementation. Certain interfaces, specifically those related to partitions, have intentionally been left unimplemented, leading to this limitation.;requirement_debt
The intentional decision to exclude support for special characters in the column name is driven by the incomplete implementation of certain interfaces in the InMemoryExternalCatalog. Not all interfaces, particularly those pertaining to partitions, have been implemented, which necessitates this intentional limitation.;requirement_debt
The intentional choice to not support special characters in the column name is due to the InMemoryExternalCatalog implementation, which intentionally omits the implementation of certain interfaces. This deliberate decision results in the limitation of supporting special characters.;requirement_debt
Special characters in the column name are intentionally not supported in the InMemoryExternalCatalog due to the omission of implementation for certain interfaces, specifically those associated with partitions. This intentional decision restricts the support for special characters.;requirement_debt
The absence of support for special characters in the column name is a deliberate design choice in the InMemoryExternalCatalog. Certain interfaces, particularly those related to partitions, have been intentionally left unimplemented, resulting in this limitation.;requirement_debt
The intentional decision to not support special characters in the column name is attributed to the incomplete implementation of specific interfaces in the InMemoryExternalCatalog. These unimplemented interfaces, particularly those concerning partitions, led to the intentional exclusion of support for special characters.;requirement_debt
Special characters in the column name are intentionally not supported in the InMemoryExternalCatalog due to the intentional omission of certain interfaces. This intentional design choice limits the support for special characters.;requirement_debt
The intentional exclusion of support for special characters in the column name is a result of the incomplete implementation of certain interfaces in the InMemoryExternalCatalog. Not all interfaces, especially those associated with partitions, have been implemented intentionally, leading to this limitation.;requirement_debt
The reason for not implementing certain interfaces in the InMemoryExternalCatalog, such as the ones related to partitions, is intentional. It is because the catalog hasn't fully implemented all the required functionalities for those interfaces.;requirement_debt
The decision to intentionally exclude certain interfaces, like the ones related to partitions, in the InMemoryExternalCatalog is based on the fact that the catalog does not currently support all the required functionalities for those interfaces.;requirement_debt
The lack of implementation for certain interfaces, specifically those pertaining to partitions, in the InMemoryExternalCatalog is intentional. It is because the catalog is not designed to fully support all the functionalities associated with those interfaces.;requirement_debt
The reason behind intentionally not implementing certain interfaces, including those related to partitions, in the InMemoryExternalCatalog, is due to the catalog's current limitations in supporting all the required functionalities for those interfaces.;requirement_debt
The decision to omit the implementation of certain interfaces, such as the ones associated with partitions, in the InMemoryExternalCatalog, is deliberate. It is because the catalog does not currently possess the capabilities to fully support all the functionalities required by those interfaces.;requirement_debt
The intentional exclusion of certain interfaces, particularly those pertaining to partitions, in the InMemoryExternalCatalog, is based on the catalog's limitations in providing complete support for the functionalities required by those interfaces.;requirement_debt
The decision to intentionally leave out the implementation of certain interfaces, including those related to partitions, in the InMemoryExternalCatalog, is motivated by the catalog's current inability to fully fulfill all the functionalities required by those interfaces.;requirement_debt
The reason for intentionally not implementing certain interfaces, such as the ones associated with partitions, in the InMemoryExternalCatalog, is because the catalog is not yet equipped to support all the functionalities required by those interfaces.;requirement_debt
The deliberate choice to exclude the implementation of certain interfaces, particularly those concerning partitions, in the InMemoryExternalCatalog, is due to the catalog's limited capability in providing complete support for the functionalities associated with those interfaces.;requirement_debt
The decision to intentionally omit the implementation of certain interfaces, including those related to partitions, in the InMemoryExternalCatalog, is driven by the catalog's current lack of support for all the functionalities required by those interfaces.;requirement_debt
The reason for intentionally not implementing certain interfaces, such as the ones pertaining to partitions, in the InMemoryExternalCatalog, is because the catalog is not designed to fully accommodate all the functionalities specified by those interfaces.;requirement_debt
The deliberate exclusion of certain interfaces, particularly those concerning partitions, in the InMemoryExternalCatalog, is based on the catalog's current limitations in providing complete support for the functionalities associated with those interfaces.;requirement_debt
The decision to intentionally leave out the implementation of certain interfaces, including those related to partitions, in the InMemoryExternalCatalog, is motivated by the catalog's inability to fully satisfy all the functionalities required by those interfaces.;requirement_debt
The deliberate choice to exclude the implementation of certain interfaces, particularly those pertaining to partitions, in the InMemoryExternalCatalog, is due to the catalog's limited capability in providing complete support for the functionalities associated with those interfaces.;requirement_debt
"@DaanHoogland I would limit it to ""4.15.0.0 to 4.15.1.0"", because adding it also to 4.14.1 will conflict with the the insert's in table `cloud.guest_os` during upgrade to 4.15.0.
I noticed that there is an `com.cloud.upgrade.dao` package still needed, but I hope, someone other will implement this ðŸ¤ž ðŸ˜„";requirement_debt
"""@DaanHoogland, I suggest limiting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, I noticed that the com.cloud.upgrade.dao package still needs to be implemented, but I hope someone else will take care of that. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my recommendation would be to restrict it to """"4.15.0.0 to 4.15.1.0"""" in order to prevent conflicts with the insertions in the cloud.guest_os table when upgrading to 4.15.0. Furthermore, I observed that the com.cloud.upgrade.dao package still requires implementation, but I'm hopeful that someone else will handle that task. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I would advise limiting it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 might cause conflicts with the insert statements in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, there is still a need for the com.cloud.upgrade.dao package to be implemented, but I'm optimistic that someone else will take on that responsibility. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I suggest restricting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the inserts performed on the cloud.guest_os table when upgrading to 4.15.0. Also, please note that the com.cloud.upgrade.dao package still requires implementation, but I believe someone else will handle that task. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my recommendation is to limit it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 could potentially cause conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, please be aware that the com.cloud.upgrade.dao package still needs to be implemented, but I have confidence that someone else will take care of it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I propose restricting it to """"4.15.0.0 to 4.15.1.0"""" to prevent conflicts with the insert statements in the cloud.guest_os table when upgrading to 4.15.0. Moreover, please note that the implementation of the com.cloud.upgrade.dao package is still pending, but I trust that someone else will address it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I would recommend limiting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. Furthermore, it is worth noting that the com.cloud.upgrade.dao package still needs to be implemented, but I'm hopeful that someone else will take care of it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my suggestion is to restrict it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 might cause conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, please be aware that the implementation of the com.cloud.upgrade.dao package is still pending, but I believe someone else will handle it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I propose limiting it to """"4.15.0.0 to 4.15.1.0"""" to prevent conflicts with the inserts performed on the cloud.guest_os table when upgrading to 4.15.0. Also, note that the com.cloud.upgrade.dao package still needs to be implemented, but I trust that someone else will take care of it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I suggest restricting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the insert statements in the cloud.guest_os table during the upgrade to 4.15.0. Furthermore, it is worth mentioning that the com.cloud.upgrade.dao package still requires implementation, but I'm optimistic that someone else will address it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my recommendation would be to limit it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 might result in conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, please note that the implementation of the com.cloud.upgrade.dao package is still pending, but I have confidence that someone else will handle it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I suggest restricting it to """"4.15.0.0 to 4.15.1.0"""" to prevent conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. Moreover, please be aware that the implementation of the com.cloud.upgrade.dao package is still pending, but I trust that someone else will address it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I would recommend limiting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the insert statements in the cloud.guest_os table during the upgrade to 4.15.0. Furthermore, it is worth noting that the com.cloud.upgrade.dao package still needs to be implemented, but I'm hopeful that someone else will take care of it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my suggestion is to restrict it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 might cause conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, please be aware that the implementation of the com.cloud.upgrade.dao package is still pending, but I believe someone else will handle it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I propose limiting it to """"4.15.0.0 to 4.15.1.0"""" to prevent conflicts with the inserts performed on the cloud.guest_os table when upgrading to 4.15.0. Also, note that the com.cloud.upgrade.dao package still needs to be implemented, but I trust that someone else will take care of it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, I suggest restricting it to """"4.15.0.0 to 4.15.1.0"""" to avoid conflicts with the insert statements in the cloud.guest_os table during the upgrade to 4.15.0. Furthermore, it is worth mentioning that the com.cloud.upgrade.dao package still requires implementation, but I'm optimistic that someone else will address it. ðŸ¤ž ðŸ˜„""";requirement_debt
"""@DaanHoogland, my recommendation would be to limit it to """"4.15.0.0 to 4.15.1.0"""" as including it in 4.14.1 might result in conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, please note that the implementation of the com.cloud.upgrade.dao package is still pending, but I have confidence that someone else will handle it. ðŸ¤ž ðŸ˜„""";requirement_debt
"@DaanHoogland, I suggest limiting it to ""4.15.0.0 to 4.15.1.0"" to avoid conflicts with the insertions in table cloud.guest_os during the upgrade to 4.15.0. Also, I noticed that there is still a requirement for the com.cloud.upgrade.dao package, but I hope someone else will handle its implementation. 😄";requirement_debt
"@DaanHoogland, my recommendation is to restrict it to ""4.15.0.0 to 4.15.1.0"" in order to prevent conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. Additionally, I noticed that there is still a need for the com.cloud.upgrade.dao package, but I'm hopeful that someone else will take care of its implementation. 😄";requirement_debt
"@DaanHoogland, I would advise limiting it to ""4.15.0.0 to 4.15.1.0"" to avoid conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. By the way, I noticed that the com.cloud.upgrade.dao package is still required, but I'm optimistic that someone else will handle its implementation. 😄";requirement_debt
"@DaanHoogland, my suggestion would be to only include ""4.15.0.0 to 4.15.1.0"" to prevent any conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. On another note, I observed that there is still a need for the com.cloud.upgrade.dao package, but I'm hopeful that someone else will take care of its implementation. 😄";requirement_debt
"@DaanHoogland, I recommend limiting it to ""4.15.0.0 to 4.15.1.0"" to avoid conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. By the way, I noticed that the com.cloud.upgrade.dao package is still required, but I'm optimistic that someone else will handle its implementation. 😄";requirement_debt
"@DaanHoogland, I suggest restricting it to ""4.15.0.0 to 4.15.1.0"" to prevent any conflicts with the insertions in the cloud.guest_os table during the upgrade to 4.15.0. Also, it seems that the com.cloud.upgrade.dao package is still needed, but I'm hopeful that someone else will take care of its implementation. 😄";requirement_debt
"@DaanHoogland, my recommendation is to only include ""4.15.0.0 to 4.15.1.0"" to avoid conflicts with the inserts in the cloud.guest_os table during the upgrade to 4.15.0. On another note, I observed that there is still a need for the com.cloud.upgrade.dao package, but I'm optimistic that someone else will handle its implementation. 😄";requirement_debt
"Fixed Length Byte Array makes sense to me (as the interpretation of the bytes for decimal is different than either i32 or i64). 
Reasons I could imagine using i32 or i64 to write decimals into Parquet would be 
1. ecosystem compatibility (aka that the pandas parquet reader assumed decimals were stored using those types) 
2.possibly so better / more performant encodings could be used.
But I am just SWAG'ing it here";requirement_debt
"""Fixed Length Byte Array seems appropriate to me for storing decimals in Parquet, considering that the interpretation of the bytes differs from that of i32 or i64. There could be a couple of reasons for using i32 or i64 to write decimals into Parquet. First, it could be for the sake of ecosystem compatibility, such as ensuring that the pandas Parquet reader assumes decimals are stored using those types. Second, it's possible that using i32 or i64 allows for better or more performant encodings to be employed. However, this is just my speculation.""";requirement_debt
"""I find the choice of Fixed Length Byte Array logical for representing decimals in Parquet, given that the byte interpretation for decimals differs from that of i32 or i64. There are a few reasons why one might consider using i32 or i64 to write decimals into Parquet. One reason could be to maintain compatibility with the ecosystem, particularly if the pandas Parquet reader assumes decimals are stored using those types. Additionally, using i32 or i64 might enable the utilization of more efficient or performant encodings. However, these are merely educated guesses on my part.""";requirement_debt
"""The usage of Fixed Length Byte Array for storing decimals in Parquet makes sense to me, considering that the byte interpretation for decimals is distinct from that of i32 or i64. There could be a couple of reasons why i32 or i64 might be chosen for writing decimals into Parquet. One possibility is to ensure compatibility with the ecosystem, particularly if the pandas Parquet reader expects decimals to be stored using those types. Another reason could be to explore the potential for employing more optimized or high-performance encodings. However, please note that these are speculative ideas.""";requirement_debt
"""I agree with the decision to utilize Fixed Length Byte Array for decimal storage in Parquet, as the byte representation for decimals differs from that of i32 or i64. The use of i32 or i64 for writing decimals into Parquet may be motivated by ecosystem compatibility, especially if the pandas Parquet reader assumes decimals are stored using those types. It's also plausible that using i32 or i64 allows for more efficient or faster encodings. However, I must emphasize that these are merely conjectures on my part.""";requirement_debt
"""The adoption of Fixed Length Byte Array as the representation for decimals in Parquet appears logical, considering the distinct byte interpretation compared to i32 or i64. The choice of i32 or i64 for writing decimals into Parquet might be driven by the need for ecosystem compatibility, particularly if the pandas Parquet reader assumes decimals are encoded using those types. Additionally, using i32 or i64 could potentially facilitate the utilization of more optimized or performant encodings. However, these assumptions should be taken as informed guesses.""";requirement_debt
"""I find the use of Fixed Length Byte Array suitable for storing decimals in Parquet, given the different byte interpretation from i32 or i64. The decision to potentially employ i32 or i64 for writing decimals into Parquet might be motivated by considerations such as ecosystem compatibility, particularly if the pandas Parquet reader expects decimals to be stored using those types. It's also plausible that using i32 or i64 allows for the utilization of more efficient or faster encoding techniques. However, please note that these are speculative ideas.""";requirement_debt
"""I understand the rationale behind using Fixed Length Byte Array for representing decimals in Parquet, as the byte interpretation for decimals deviates from that of i32 or i64. The choice to potentially use i32 or i64 for writing decimals into Parquet could stem from the desire for ecosystem compatibility, especially if the pandas Parquet reader assumes decimals are encoded using those types. Furthermore, leveraging i32 or i64 might offer opportunities for employing optimized or high-performance encodings. However, I must emphasize that these are mere estimations on my part.""";requirement_debt
"The use of Fixed Length Byte Array for decimals makes sense to me because the interpretation of the bytes for decimal values is different compared to i32 or i64. There could be reasons to use i32 or i64 to write decimals into Parquet, such as:

Ecosystem compatibility, where the pandas Parquet reader assumes that decimals are stored using those types.

Potentially, to enable better or more performant encodings for decimals.

However, these are just speculative guesses (SWAG) based on the context and possibilities.";requirement_debt
"""Fixed Length Byte Array seems reasonable to me for storing decimals, as the interpretation of the bytes for decimals differs from that of i32 or i64. There could be a couple of reasons for using i32 or i64 to write decimals into Parquet. Firstly, it could be for the sake of ecosystem compatibility, such as ensuring compatibility with the pandas parquet reader that assumes decimals are stored using those types. Secondly, it might be done to explore better or more performant encodings for decimals. However, I must admit that these reasons are purely speculative.""";requirement_debt
"""I find the choice of Fixed Length Byte Array suitable for representing decimals, considering that the byte interpretation for decimals is distinct from i32 or i64. Nonetheless, there are potential reasons why one might consider using i32 or i64 to store decimals in Parquet. One reason could be ecosystem compatibility, ensuring that the pandas parquet reader, which assumes decimals are stored using those types, remains compatible. Another reason could be to explore alternative, more efficient encodings for decimals. However, please note that these reasons are based on my conjecture.""";requirement_debt
"""Using Fixed Length Byte Array for decimals makes sense to me, given the difference in byte interpretation compared to i32 or i64. However, there could be instances where one would choose i32 or i64 for writing decimals in Parquet. One possible reason is to maintain compatibility with the ecosystem, specifically with the pandas parquet reader, which expects decimals to be stored using those types. Another reason could be the potential for leveraging more optimized or performant encodings for decimals. It's important to note that these reasons are speculative in nature.""";requirement_debt
"""I agree that utilizing Fixed Length Byte Array for decimals is a logical choice, considering the unique byte interpretation for decimals as opposed to i32 or i64. However, there could be scenarios where one might opt for i32 or i64 to store decimals in Parquet. One such reason could be to ensure compatibility with the broader ecosystem, particularly the pandas parquet reader, which assumes decimals are stored using those types. Additionally, the choice of i32 or i64 might enable the exploration of more efficient or performant encodings for decimals. It's worth mentioning that these reasons are speculative.""";requirement_debt
"""The adoption of Fixed Length Byte Array for decimals appears to be a suitable approach, given the distinct byte interpretation compared to i32 or i64. Nevertheless, there are potential reasons why i32 or i64 could be used for writing decimals in Parquet. One possibility is to maintain compatibility with the wider ecosystem, especially the pandas parquet reader, which expects decimals to be stored using those types. Another reason could be to explore alternative encoding schemes that offer better performance or efficiency for decimals. However, it's important to note that these reasons are speculative in nature.""";requirement_debt
"""Fixed Length Byte Array seems appropriate to me since the interpretation of the bytes for decimal values is different from both i32 and i64. There are a couple of reasons why one might consider using i32 or i64 to store decimals in Parquet. Firstly, it could be for ecosystem compatibility, such as ensuring compatibility with the pandas parquet reader, which assumes decimals are stored using those types. Secondly, it could be to explore the possibility of utilizing better or more performant encodings. However, I must admit that these are just my educated guesses.""";requirement_debt
"""I find the choice of Fixed Length Byte Array reasonable because the byte interpretation for decimal values differs from that of i32 or i64. There are a couple of reasons why one might opt for i32 or i64 to store decimals in Parquet. One reason could be to ensure compatibility with the ecosystem, particularly with the pandas parquet reader, which expects decimals to be stored using those types. Another possibility is to explore the potential for leveraging improved or more efficient encodings. However, I must clarify that these are just my speculations.""";requirement_debt
"""I agree that Fixed Length Byte Array is a suitable choice because the byte representation for decimal values is distinct from that of i32 or i64. It's possible that i32 or i64 were considered for storing decimals in Parquet due to ecosystem compatibility, specifically to align with the assumptions made by the pandas parquet reader. Another rationale could be to explore alternative encodings that offer better performance or efficiency. However, I want to acknowledge that these are mere conjectures.""";requirement_debt
"""The selection of Fixed Length Byte Array makes sense to me as the byte interpretation for decimal values differs from i32 or i64. There are a couple of reasons why i32 or i64 might have been considered for storing decimals in Parquet. One reason could be ecosystem compatibility, especially with the pandas parquet reader that expects decimals to be stored using those types. Another possibility is to explore the potential for more optimized or performant encodings. However, I want to emphasize that these are just my assumptions.""";requirement_debt
"""I understand the rationale behind choosing Fixed Length Byte Array, considering the distinct byte representation required for decimals compared to i32 or i64. It's conceivable that i32 or i64 were initially considered for storing decimals in Parquet for the sake of ecosystem compatibility, particularly with the pandas parquet reader's assumptions. Additionally, it's plausible that using i32 or i64 would enable the utilization of more efficient or improved encoding techniques. However, please note that these are my personal estimations.""";requirement_debt
"The use of Fixed Length Byte Array for decimals makes sense to me because the interpretation of the bytes differs from that of i32 or i64. There could be a couple of reasons for using i32 or i64 to store decimals in Parquet:

Ecosystem compatibility: It's possible that the pandas parquet reader assumes decimals are stored using those types, so using i32 or i64 would ensure compatibility with the existing ecosystem.
Performance considerations: It's also possible that using i32 or i64 allows for better or more performant encodings to be utilized. However, this is just an estimation on my part.";requirement_debt
"I agree with using Fixed Length Byte Array for decimals as the byte interpretation for decimals differs from that of i32 or i64. While i32 or i64 could be used to write decimals into Parquet, there are a couple of reasons why it may not be the preferred choice:

Ecosystem compatibility: It's possible that the pandas parquet reader assumes decimals are stored using i32 or i64, so using these types would ensure compatibility with the existing ecosystem.
Performance optimizations: It's possible that using i32 or i64 may allow for better or more efficient encodings to be used. However, this is just speculation at this point.";requirement_debt
"I understand the rationale behind using Fixed Length Byte Array for decimals, as the byte representation for decimals is different from i32 or i64. While it's possible to use i32 or i64 to store decimals in Parquet, there may be a couple of reasons why this approach is not adopted:

Ecosystem compatibility: The assumption here is that the pandas parquet reader expects decimals to be stored using i32 or i64, so using these types would ensure compatibility with the existing ecosystem.
Performance considerations: It's conceivable that using i32 or i64 could allow for more efficient encodings or better performance. However, this is just a speculative assumption on my part.";requirement_debt
"The decision to use Fixed Length Byte Array for decimals seems reasonable to me, given that the byte interpretation differs from that of i32 or i64. Although using i32 or i64 could be an option for writing decimals in Parquet, there are a couple of potential reasons for not choosing this approach:

Ecosystem compatibility: It is possible that the pandas parquet reader assumes decimals are stored using i32 or i64, so using these types would ensure compatibility with the existing ecosystem.
Performance optimization: It is also possible that using i32 or i64 could enable better or more efficient encodings for decimals. However, this is purely speculative and would require further investigation.";requirement_debt
"I agree that using Fixed Length Byte Array for decimals makes sense, considering that the byte interpretation differs from i32 or i64. While it is possible to use i32 or i64 to store decimals in Parquet, there could be a couple of reasons why this approach is not chosen:

Ecosystem compatibility: It is possible that the pandas parquet reader assumes decimals are stored using i32 or i64, so using these types ensures compatibility with the existing ecosystem.
Performance considerations: It is conceivable that using i32 or i64 could provide better performance or more efficient encodings for decimals. However, this is just an educated guess and would require further investigation to confirm.";requirement_debt
"The choice of using Fixed Length Byte Array for decimals seems reasonable, as the byte representation for decimals differs from that of i32 or i64. Although i32 or i64 could potentially be used to write decimals into Parquet, there are a couple of factors to consider:

Ecosystem compatibility: It is possible that the pandas parquet reader assumes decimals are stored using i32 or i64, so using these types would ensure compatibility with the existing ecosystem.
Performance optimization: It is also possible that using i32 or i64 could allow for better or more efficient encodings for decimals. However, this is just a speculative assumption and would require further investigation.";requirement_debt
Just tried. That function for FP32 convolution seems not implemented appropriately. The data member `layout` of struct `ConvolutionParam` is a `option<int>` type, but it was used with the assumption that `layout` contains a real value in that function. This will cause my test case to fail. Making that function properly implemented needs another PR. For now, I will just keep the shape inference for quantized_conv op. Already deleted the TODO comment.;requirement_debt
I just tested it, and it appears that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but it was used with the assumption that layout contains an actual value in that function. As a result, my test case fails. Properly implementing that function will require a separate PR. For now, I will retain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I've given it a try, and it seems that the function for FP32 convolution is not appropriately implemented. The layout data member in the ConvolutionParam struct is of type option<int>, but it was used with the assumption that it holds a valid value within that function. As a consequence, my test case fails. To rectify this issue, a separate PR will be needed to properly implement the function. In the meantime, I will maintain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I recently tested the code, and it seems that the function for FP32 convolution is not correctly implemented. The layout data member in the ConvolutionParam struct is of type option<int>, but it was utilized with the assumption that it contains a valid value within that particular function. Unfortunately, this leads to a failure in my test case. To address this issue properly, another PR will be required. For now, I will preserve the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
Based on my recent testing, it appears that the function for FP32 convolution is not implemented appropriately. The layout data member within the ConvolutionParam struct is of type option<int>, but it was assumed to hold a valid value within that function. Consequently, this causes my test case to fail. A proper implementation of the function will necessitate a separate PR. In the meantime, I will retain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I've just conducted a test, and it seems that the function for FP32 convolution is not implemented correctly. The layout data member in the ConvolutionParam struct is of type option<int>, but it was used with the assumption that it contains a valid value in that function. As a result, my test case fails. To ensure the proper implementation of this function, another PR will be needed. In the meantime, I will maintain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I just tested it, and it appears that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but the function assumes that layout contains a real value. This inconsistency is causing my test case to fail. Fixing this issue requires a separate PR. In the meantime, I will retain the shape inference for the quantized_conv op and remove the TODO comment.;requirement_debt
I've tested it, and it seems that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but the function assumes that layout holds an actual value. As a result, my test case is failing. To address this issue properly, another PR will be needed. For now, I will focus on maintaining the shape inference for the quantized_conv op and remove the TODO comment.;requirement_debt
I gave it a try, and it appears that the function for FP32 convolution is not appropriately implemented. The layout data member of the ConvolutionParam struct is defined as option<int>, but the function assumes that layout contains a valid value. This discrepancy is causing my test case to fail. To resolve this issue correctly, a separate PR will be necessary. In the meantime, I will concentrate on preserving the shape inference for the quantized_conv op and remove the TODO comment.;requirement_debt
After testing, it seems that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but the function assumes that layout holds a valid value. As a result, my test case is failing. Properly addressing this issue will require another PR. However, for now, I will focus on maintaining the shape inference for the quantized_conv op and remove the TODO comment.;requirement_debt
I've just tested it, and it seems that the function for FP32 convolution is not implemented appropriately. The layout data member of the ConvolutionParam struct is defined as option<int>, but the function assumes that layout contains a real value. This discrepancy is causing my test case to fail. To rectify this issue correctly, a separate PR will be necessary. Meanwhile, I will ensure the shape inference for the quantized_conv op remains intact and remove the TODO comment.;requirement_debt
I've tested it, and it seems that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but it was used assuming that layout contains a real value in that function. As a result, my test case failed. Properly implementing that function will require another PR. For now, I'll focus on keeping the shape inference for the quantized_conv op. I've already removed the TODO comment.;requirement_debt
I've given it a try, and it appears that the function for FP32 convolution is not properly implemented. The layout data member in the ConvolutionParam struct is of type option<int>, but it was used with the assumption that layout holds a real value within that function. Consequently, my test case failed. To address this issue and ensure proper implementation, we'll need to create a separate PR. However, for now, I will focus on maintaining the shape inference for the quantized_conv op. I have already removed the TODO comment.;requirement_debt
I've conducted a test, and it seems that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but it was mistakenly assumed to contain a valid value within that function. As a result, my test case failed. To rectify this issue and achieve proper implementation, we will need to submit another PR. However, for the time being, I will concentrate on preserving the shape inference for the quantized_conv op. I have already addressed the TODO comment by removing it.;requirement_debt
I've given it a try, and it appears that the function for FP32 convolution is not appropriately implemented. The layout data member in the ConvolutionParam struct is of type option<int>, but it was used with the assumption that layout holds a valid value within that function. As a consequence, my test case failed. To ensure proper implementation, we will need to submit a separate PR. However, for now, I will focus on maintaining the shape inference for the quantized_conv op. The TODO comment has already been removed.;requirement_debt
I've tested it, and it seems that the function for FP32 convolution is not implemented properly. The layout data member of the ConvolutionParam struct is of type option<int>, but it was used with the expectation that layout contains a valid value in that function. This resulted in a failure of my test case. To address this issue and achieve proper implementation, we will need to create a separate PR. However, for now, my focus will be on preserving the shape inference for the quantized_conv op. The TODO comment has already been removed.;requirement_debt
Just tried it out, and it appears that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but it seems to have been used with the assumption that layout contains an actual value in that function. This inconsistency will cause my test case to fail. To address this issue properly, we would need to submit another pull request. However, for now, I will proceed with keeping the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I just tested it, and it seems that the function for FP32 convolution is not implemented correctly. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being used with the assumption that it contains a real value in that function. As a result, my test case failed. Properly implementing that function will require another PR. For now, I will maintain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
I've tried it, and it appears that the function for FP32 convolution is not implemented correctly. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being utilized with the assumption that it holds a valid value within that function. This will cause my test case to fail. To ensure proper implementation, a separate PR will be required. However, for the time being, I will retain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
After testing it, I found that the function for FP32 convolution is not properly implemented. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being used under the assumption that it contains a valid value in that function. This discrepancy caused my test case to fail. To address this issue correctly, a separate PR will be necessary. In the meantime, I will keep the shape inference for the quantized_conv op intact and have already removed the TODO comment.;requirement_debt
It appears that the function for FP32 convolution is not implemented correctly upon testing. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being utilized with the expectation that it holds a valid value within that function. Consequently, my test case failed due to this inconsistency. To rectify the situation, a separate PR will be required to ensure the proper implementation. However, for now, I will maintain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
Based on my test, it seems that the function for FP32 convolution is not implemented appropriately. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being used with the assumption that it contains a valid value in that function. This inconsistency resulted in the failure of my test case. To resolve this issue properly, a separate PR will be needed. However, I have already preserved the shape inference for the quantized_conv op and removed the TODO comment.;requirement_debt
During my test, I discovered that the function for FP32 convolution is not implemented correctly. The layout data member in the ConvolutionParam struct is of type option<int>, but it is being utilized under the assumption that it holds a real value within that function. As a consequence, my test case failed to produce the expected results. A separate PR will be required to address this issue and ensure the proper implementation. Nevertheless, I have already retained the shape inference for the quantized_conv op and removed the TODO comment.;requirement_debt
I've tested it, and it appears that the function for FP32 convolution is not implemented correctly. The layout data member of the ConvolutionParam struct is of type option<int>, but it is being used with the assumption that it contains an actual value in that function. As a result, my test case fails. Properly implementing that function will require another PR. In the meantime, I will maintain the shape inference for the quantized_conv op and have already removed the TODO comment.;requirement_debt
After trying it out, it seems that the function for FP32 convolution is not appropriately implemented. The layout data member of the ConvolutionParam struct is of type option<int>, but the function assumes that layout holds a valid value. As a result, my test case fails. Fixing this issue will require a separate PR to ensure the function is properly implemented. For now, I will focus on preserving the shape inference for the quantized_conv op, and I have already removed the TODO comment.;requirement_debt
"Looks like you're removing max spout pending and also configuration for backpressure. Does this patch also touch the mechanism of backpressure?
And let's address this in patch or file an issue and remove TODO.";requirement_debt
It appears that you are removing the configuration for max spout pending and backpressure in this patch. Does this patch also modify the backpressure mechanism itself? Let's discuss and address this in the patch or file a separate issue to handle it. Additionally, we should remove the TODO comment.;requirement_debt
I noticed that you are removing the configuration for max spout pending and backpressure in this patch. Does this patch also involve changes to the backpressure mechanism itself? Let's address this in the patch or create a new issue to handle it. Furthermore, we should remove the TODO comment to ensure proper documentation.;requirement_debt
It seems that you are removing the configuration for max spout pending and backpressure in this patch. I would like to confirm whether this patch also includes modifications to the backpressure mechanism. If it does, let's discuss it in the patch or create a new issue to address this concern. Additionally, we should remove the TODO comment to ensure thorough documentation.;requirement_debt
I've noticed that this patch involves removing the configuration for max spout pending and backpressure. However, I would like to know if it also affects the underlying backpressure mechanism itself. Let's discuss this in the patch or consider creating a new issue to address any necessary changes. Additionally, we should remove the TODO comment as part of the documentation process.;requirement_debt
It appears that in this patch, you are removing the configuration for max spout pending and backpressure. I'm curious if this patch also modifies the backpressure mechanism itself. We should discuss this in the patch or consider creating a separate issue to address any necessary adjustments. Furthermore, let's remove the TODO comment to ensure comprehensive documentation.;requirement_debt
It appears that you are removing the configuration for max spout pending and the mechanism of backpressure. Does this patch also modify the backpressure mechanism? Let's address this in the patch itself or file an issue and remove the TODO.;requirement_debt
It seems that in this patch, you're removing the configuration for max spout pending and the backpressure mechanism. I'm wondering if this patch also touches the implementation of the backpressure mechanism itself. To ensure clarity, let's address this either in the patch or file a separate issue and remove the TODO comment.;requirement_debt
I noticed that you're removing the configuration for max spout pending and the backpressure mechanism in this patch. I'm curious if this patch also includes changes to the backpressure mechanism itself. It would be best to address this directly in the patch or create a separate issue and remove the TODO note.;requirement_debt
I see that the patch is removing the configuration for max spout pending and altering the backpressure mechanism. However, I'm wondering if this patch specifically modifies the backpressure mechanism itself. To clarify, let's discuss this in the patch or open a separate issue and remove the TODO remark.;requirement_debt
It seems that as part of this patch, you are removing the configuration for max spout pending and making changes to the backpressure mechanism. I'm interested to know if the patch directly modifies the backpressure mechanism or if the changes are only related to configuration. Let's address this in the patch or consider creating a separate issue and removing the TODO statement.;requirement_debt
It appears that you are removing the configuration for max spout pending and backpressure in this patch. Does this patch also modify the backpressure mechanism itself? It would be best to address this either in the patch or file a separate issue and remove the TODO.;requirement_debt
I noticed that you are removing the max spout pending configuration and the configuration for backpressure in this patch. I'm curious if this patch also involves any changes to the backpressure mechanism. It would be helpful to address this directly in the patch or create a new issue to track the modifications and remove the TODO comment.;requirement_debt
I see that you're removing the configuration for max spout pending and backpressure in this patch. However, I'm wondering if this patch also modifies the underlying backpressure mechanism. It would be great to clarify this in the patch itself or create a new issue to discuss the changes and resolve the TODO comment.;requirement_debt
It seems that you're removing the max spout pending configuration and the configuration for backpressure in this patch. I'd like to confirm if this patch also includes changes to the backpressure mechanism. It would be beneficial to address this directly in the patch or create a separate issue to handle the modifications and remove the TODO note.;requirement_debt
From my observation, it seems that you're eliminating the configuration for max spout pending and backpressure in this patch. I'm curious if this patch also modifies the underlying mechanism of backpressure. It would be helpful to either address this directly in the patch or open a new issue to discuss the changes and resolve the TODO comment.;requirement_debt
It appears that you're removing the configuration for max spout pending and the mechanism for backpressure. Does this patch also involve changes to the backpressure mechanism? It would be best to address this in the patch itself or file an issue and remove the TODO.;requirement_debt
From what I can see, you're removing the max spout pending configuration and the backpressure mechanism. Does this patch also modify the way backpressure works? It would be beneficial to address this matter either within the patch or by creating an issue and removing the TODO comment.;requirement_debt
It seems that the patch includes the removal of the max spout pending configuration and the backpressure mechanism. However, I'm curious if this patch also affects the underlying backpressure mechanism. To ensure clarity, it would be ideal to address this either in the patch itself or by filing an issue and removing the TODO.;requirement_debt
It appears that in this patch, you're removing the configuration for max spout pending and the backpressure mechanism. I'd like to clarify if this patch also modifies the way backpressure is handled. It would be helpful to address this concern within the patch or by opening an issue and removing the TODO comment.;requirement_debt
It seems that this patch involves removing the configuration for max spout pending and the backpressure mechanism. However, I'd like to confirm if there are any changes to the underlying backpressure mechanism as well. It would be advisable to address this either within the patch or by raising an issue and eliminating the TODO comment.;requirement_debt
It appears that you are removing the configuration for max spout pending and the mechanism of backpressure. Does this patch also modify the backpressure mechanism? It would be helpful to address this in the patch or file an issue and remove the TODO.;requirement_debt
It seems that this patch is removing the configuration for max spout pending and the mechanism of backpressure. I'm curious if the backpressure mechanism is also being modified in this patch. It would be beneficial to address this either in the patch itself or by filing an issue and removing the TODO comment.;requirement_debt
From what I can see, you're removing the configuration for max spout pending and the backpressure mechanism. I'd like to confirm if the backpressure mechanism is being altered by this patch as well. It would be ideal to address this either in the patch or by creating an issue and resolving the TODO.;requirement_debt
It appears that both the configuration for max spout pending and the backpressure mechanism are being removed in this patch. I'm wondering if the backpressure mechanism itself is being modified as part of this patch. It would be helpful to address this concern within the patch or by opening an issue and removing the TODO comment.;requirement_debt
"@shuttie Got it, makes a lot of sense to me.
I was wondering at some point if it would be worthwhile to just ""unsafe arraycopy"" the char[] from the string to the byte[] in the memory segments or stream buffers. So basically no byte-wise logic at all in the serialization. That would increase the state size (all chars would have two bytes), but might save CPU resources.
Have you ever experimented with something that?";requirement_debt
"I understand your point, and it seems reasonable to me. I have also considered whether it would be beneficial to directly ""unsafe arraycopy"" the char[] from the string to the byte[] in the memory segments or stream buffers. By doing so, we could avoid byte-wise logic in the serialization process. However, it's important to note that this approach would increase the state size, as each char would occupy two bytes. Nevertheless, it could potentially save CPU resources. Have you ever experimented with such an approach?";requirement_debt
"I appreciate your perspective, and I find it quite logical. I have also contemplated the idea of ""unsafe arraycopy"" to directly transfer the char[] from the string to the byte[] in the memory segments or stream buffers. This approach would eliminate the need for byte-wise logic during serialization, which could potentially reduce CPU overhead. However, it's worth considering that adopting this method would result in increased state size since each char would occupy two bytes. Have you ever explored or experimented with this approach?";requirement_debt
"Your suggestion makes a lot of sense to me, and I have actually pondered the idea of using ""unsafe arraycopy"" to transfer the char[] from the string directly to the byte[] in the memory segments or stream buffers. By doing so, we could eliminate the byte-wise logic in the serialization process, potentially leading to CPU savings. However, it's important to consider that this approach would increase the state size, as each char would occupy two bytes. Have you ever conducted any experiments or investigations related to this approach?";requirement_debt
"I completely understand your point, and I find it quite compelling. I have also contemplated the possibility of employing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers. This approach would bypass the need for byte-wise logic during serialization, potentially resulting in CPU resource savings. However, it's important to note that this would increase the state size since each char would occupy two bytes. Have you ever explored or experimented with this concept?";requirement_debt
"Your suggestion resonates with me, and I have indeed considered the idea of using ""unsafe arraycopy"" to directly transfer the char[] from the string to the byte[] in the memory segments or stream buffers. This approach would eliminate the need for byte-wise logic during serialization, potentially reducing CPU utilization. However, it's crucial to recognize that adopting this approach would result in increased state size, as each char would occupy two bytes. I'm curious if you have ever conducted any experiments or research on this topic.";requirement_debt
"I understand your point, and it seems reasonable to consider using ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in memory segments or stream buffers. This approach would eliminate the need for byte-wise logic in serialization. Although it would increase the state size due to the two-byte representation of each char, it could potentially save CPU resources. I'm curious if you have ever experimented with such an approach.";requirement_debt
"I comprehend your idea, and it's worth exploring the option of ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in memory segments or stream buffers. This approach would eliminate the byte-wise logic required for serialization. While it would increase the state size due to the two-byte representation of each char, it could potentially result in CPU resource savings. I'm interested to know if you have conducted any experiments involving this approach.";requirement_debt
"I see your point, and it's an interesting idea to consider utilizing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in memory segments or stream buffers. This approach would eliminate the need for byte-wise logic in serialization. Although the state size would increase due to the two-byte representation of each char, it could potentially reduce CPU resource usage. I'm curious if you have ever experimented with this approach to see its impact.";requirement_debt
"I comprehend your suggestion, and it's worth exploring the possibility of employing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in memory segments or stream buffers. This approach would remove the byte-wise logic involved in serialization. While it would increase the state size due to the two-byte representation of each char, it might result in CPU resource savings. I'm curious if you have conducted any experiments to evaluate the feasibility of this approach.";requirement_debt
"I understand your point, and it's an intriguing idea to consider using ""unsafe arraycopy"" for directly copying the char[] from the string to the byte[] in memory segments or stream buffers. By doing so, the byte-wise logic in serialization would be eliminated. Although the state size would increase due to the two-byte representation of each char, it could potentially lead to CPU resource savings. I'm curious if you have ever experimented with this approach to assess its viability.";requirement_debt
"I understand your point, and it makes a lot of sense to me. I have also contemplated the idea of ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers. By doing so, we could eliminate the byte-wise logic in the serialization process. However, it is worth noting that this approach would increase the state size, as each char would occupy two bytes. Nonetheless, it might result in savings in CPU resources. I'm curious if you have ever experimented with such an approach.";requirement_debt
"I comprehend your perspective, and it resonates with me. It crossed my mind whether it would be worthwhile to employ an ""unsafe arraycopy"" technique to directly transfer the char[] from the string to the byte[] in the memory segments or stream buffers, eliminating the need for byte-wise logic in serialization. This approach would likely lead to an increase in the state size, as each char would occupy two bytes. However, it could potentially reduce CPU resource consumption. I'm curious if you have explored this idea or conducted any experiments in this regard.";requirement_debt
"I see the rationale behind your suggestion, and it aligns with my understanding. I have contemplated the idea of employing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers, bypassing the need for byte-wise logic in serialization. This approach would result in an increase in the state size, as each char would require two bytes. However, it might offer advantages in terms of CPU resource utilization. I'm curious if you have ever experimented with this approach or have any insights to share.";requirement_debt
"Your point is well taken, and it resonates with me. I have also considered the possibility of utilizing ""unsafe arraycopy"" to directly transfer the char[] from the string to the byte[] in the memory segments or stream buffers, eliminating the byte-wise logic in serialization. This approach would entail a larger state size, as each char would occupy two bytes. However, it has the potential to reduce CPU resource usage. I'm interested to know if you have ever explored such an approach or conducted any experiments in this area.";requirement_debt
"I completely understand your perspective, and I find it highly logical. I have also pondered the idea of employing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers, thereby eliminating the need for byte-wise logic in serialization. This approach would result in a larger state size, as each char would require two bytes. However, it could potentially offer CPU resource savings. I'm curious if you have ever conducted any experiments or investigations regarding this approach.";requirement_debt
"@shuttie, I understand your point, and it aligns with my thinking as well. I've also considered whether it would be beneficial to directly ""unsafe arraycopy"" the char[] from the string to the byte[] in the memory segments or stream buffers. By doing so, we could eliminate the byte-wise logic in serialization. While this approach would increase the state size (since each char would occupy two bytes), it has the potential to save CPU resources. I'm curious if you have ever experimented with such an approach.";requirement_debt
"@shuttie, I appreciate your perspective, and it resonates with my thoughts on the matter. I've contemplated whether it would be advantageous to employ an ""unsafe arraycopy"" technique to transfer the char[] from the string directly to the byte[] in the memory segments or stream buffers. This would essentially eliminate the need for byte-wise logic in serialization. Although this approach would result in an increased state size (as each char would occupy two bytes), it could potentially reduce CPU resource utilization. I'm curious if you have ever experimented with this approach or have any insights to share.";requirement_debt
"@shuttie, I completely understand and agree with your point. It crossed my mind whether it would be worthwhile to perform an ""unsafe arraycopy"" operation, directly transferring the char[] from the string to the byte[] in the memory segments or stream buffers. This would effectively eliminate the need for byte-wise logic during serialization. While it would increase the state size (as each char would occupy two bytes), it could potentially yield CPU resource savings. I'm curious if you have ever explored such an approach or have any insights to share.";requirement_debt
"@shuttie, your perspective is well taken, and it aligns with my own considerations. I have pondered the idea of employing an ""unsafe arraycopy"" mechanism to directly transfer the char[] from the string to the byte[] in the memory segments or stream buffers, thereby eliminating byte-wise logic from the serialization process. Although this would result in an increased state size (with each char occupying two bytes), it could potentially lead to CPU resource savings. I'm curious if you have ever experimented with this approach or have any insights to offer.";requirement_debt
"@shuttie, I understand your point, and it resonates with my own thoughts on the matter. I have contemplated whether it would be worthwhile to utilize an ""unsafe arraycopy"" approach to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers. This would eliminate the need for byte-wise logic in serialization. While it would result in an increased state size (as each char would occupy two bytes), it could potentially reduce CPU resource consumption. I'm curious if you have ever explored this approach or have any insights to share.";requirement_debt
"I understand your point, and it definitely makes sense. I also considered the possibility of using ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers. This approach would eliminate the need for byte-wise logic in the serialization process. However, it's worth noting that this would increase the state size since each char would require two bytes. It could potentially save CPU resources, though. Have you ever experimented with such an approach?";requirement_debt
"I see where you're coming from, and it makes perfect sense to me. I also had the thought of utilizing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers. By doing so, we could eliminate the byte-wise logic in the serialization process. However, it's important to consider that this would increase the state size as each char would now occupy two bytes. On the flip side, it might help conserve CPU resources. I'm curious if you have ever experimented with this approach?";requirement_debt
"I completely understand your perspective, and it resonates with me. The idea of employing ""unsafe arraycopy"" to directly copy the char[] from the string to the byte[] in the memory segments or stream buffers crossed my mind as well. By adopting this approach, we could eliminate the need for byte-wise logic in the serialization process. However, it's crucial to acknowledge that this would result in an increase in the state size, as each char would consume two bytes. On the other hand, it could potentially optimize CPU resources. I'm curious if you have ever conducted any experiments with this approach?";requirement_debt
"Your point is well taken, and I find it highly logical. I also contemplated the possibility of utilizing ""unsafe arraycopy"" to perform a direct copy of the char[] from the string to the byte[] in the memory segments or stream buffers. This approach would eradicate the byte-wise logic involved in serialization. Nevertheless, it's important to consider that this would lead to an increase in the state size since each char would occupy two bytes. However, it could potentially result in CPU resource savings. I'm curious if you have ever conducted any experimentation with this idea?";requirement_debt
"Thank you for investigating this, @HeartSaVioR .
The original logic looks not safe. Do you think if there is any other better way?";requirement_debt
I appreciate your investigation into this matter, @HeartSaVioR. The original logic indeed appears to be unsafe. I'm curious if you have any suggestions for a better approach?;requirement_debt
Thank you, @HeartSaVioR, for looking into this. It's evident that the original logic is not safe. I'm interested to know if you have any alternative suggestions that could improve the situation?;requirement_debt
Your investigation into this issue is greatly appreciated, @HeartSaVioR. It's clear that the original logic is not secure. I'm wondering if you have any ideas for a better solution?;requirement_debt
I want to express my gratitude for taking the time to investigate this, @HeartSaVioR. It's apparent that the original logic is not reliable. Do you have any thoughts on how we can improve it?;requirement_debt
Thank you, @HeartSaVioR, for your diligent investigation. It's clear that the original logic is flawed. I'm curious if you have any recommendations for a more effective approach?;requirement_debt
I appreciate your effort in investigating this, @HeartSaVioR. The original logic indeed appears to be unsafe. I'm curious if you have any suggestions for an alternative and possibly better approach?;requirement_debt
Thank you for delving into this matter, @HeartSaVioR. It's clear that the original logic is not secure. I'm interested to know if you have any ideas for an alternative method that could potentially improve the situation.;requirement_debt
I want to express my gratitude for your investigation, @HeartSaVioR. It's evident that the original logic is not reliable. I'm wondering if you have any thoughts on an alternative approach that could be safer and more effective.;requirement_debt
Thank you, @HeartSaVioR, for taking the time to look into this. It's evident that the original logic is flawed. I'm curious if you have any suggestions for a different approach that might be more suitable.;requirement_debt
I want to extend my thanks to you, @HeartSaVioR, for your thorough investigation. It's clear that the original logic is not sound. I'm interested to hear if you have any ideas for a better alternative.;requirement_debt
I appreciate your effort in investigating this, @HeartSaVioR. The original logic indeed seems unsafe. I'm curious if you have any suggestions for a better alternative approach.;requirement_debt
Thank you for looking into this, @HeartSaVioR. It appears that the original logic is not secure. I'm interested to know if you have any ideas for a better solution.;requirement_debt
I'm grateful for your investigation, @HeartSaVioR. It seems that the original logic is not reliable. Can you think of any other approaches that might be better?;requirement_debt
Thank you, @HeartSaVioR, for delving into this issue. It appears that the original logic is flawed. I'm curious if you have any thoughts on how we could improve it.;requirement_debt
I appreciate your thorough investigation, @HeartSaVioR. It's evident that the original logic is not robust. I'm wondering if you have any suggestions for a more effective approach.;requirement_debt
Thank you for taking the time to investigate this, @HeartSaVioR. The original logic indeed appears to be unsafe. I'm curious if you have any suggestions for an alternative and potentially better approach?;requirement_debt
I appreciate your efforts in investigating this matter, @HeartSaVioR. It's evident that the original logic is not safe. I'm interested in hearing your thoughts on whether there might be a better way to address this issue.;requirement_debt
Many thanks for your investigation, @HeartSaVioR. It's clear that the original logic is not safe. I'm curious to know if you have any ideas on how we can improve the situation and find a better solution.;requirement_debt
I want to express my gratitude for delving into this, @HeartSaVioR. It's apparent that the original logic is not secure. I would appreciate your insights on whether there might be a better approach that we can consider.;requirement_debt
Thank you, @HeartSaVioR, for your thorough investigation. It's evident that the original logic is not reliable. I'm interested to hear if you have any suggestions for an alternative that might be more effective.;requirement_debt
I appreciate your efforts in investigating this, @HeartSaVioR. The original logic indeed seems unsafe. Do you have any suggestions for a better alternative?;requirement_debt
Thank you, @HeartSaVioR, for looking into this matter. It's evident that the original logic is not safe. I'm curious if you have any ideas for a better approach.;requirement_debt
@HeartSaVioR, thank you for delving into this issue. It's clear that the original logic is not secure. I'm interested to know if you have any suggestions for an improved solution.;requirement_debt
I want to express my gratitude to @HeartSaVioR for taking the time to investigate this. It's apparent that the original logic is not reliable. Can you think of any other approaches that might be better suited for this situation?;requirement_debt
"Why is propertyMap marked volatile?  The value is only ever set once at construction time.
If the answer is because of thread safety, the contents of the HashMap are not ""protected"" just because the reference to the map is marked volatile.  puts/gets to the map do not inherit the memory barrier protections associated to the volatile reference.  c.f.  http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap
Maybe a review of the concurrency issues of this processor is in order before accepting this merge request?  I'm pretty sure that, even though the class will mostly behave correctly since values are set during OnScheduled and OnStopped, these are not ""safely published"" to the map.  While unlikely, other threads could potentially see stale values in this map.
Either this class should likely be using ConcurrentHashMap here, or it should republish an entirely fresh map by calling ""new HashMap()"" instead of ""clear()"".";requirement_debt
"The reason the propertyMap is marked as volatile is likely for thread safety. However, it's important to note that marking the reference to the map as volatile does not automatically provide thread-safe access to the contents of the map. This means that puts and gets on the map do not inherit the memory barrier protections associated with the volatile reference. You can refer to this Stack Overflow thread for more details: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap. Before accepting this merge request, it might be worth reviewing the concurrency issues of this processor. Although the class may behave correctly in most cases since the values are set during OnScheduled and OnStopped, these values are not ""safely published"" to the map. While unlikely, other threads may potentially see stale values in this map. It may be advisable for this class to use ConcurrentHashMap instead or to republish an entirely fresh map by calling new HashMap() instead of clear().";requirement_debt
The propertyMap is marked as volatile, presumably for thread safety considerations. However, it's important to understand that simply marking the reference as volatile does not ensure thread-safe access to the contents of the map. In this case, puts and gets on the map do not inherit the memory barrier protections associated with the volatile reference. I recommend reviewing the concurrency issues of this processor before accepting the merge request. While the class may behave correctly in most cases since the values are set during OnScheduled and OnStopped, there is a possibility that other threads may observe stale values in the map. To address this, it may be necessary to use ConcurrentHashMap instead of a regular HashMap or to create an entirely fresh map by calling new HashMap() instead of using clear().;requirement_debt
The decision to mark propertyMap as volatile is likely due to concerns about thread safety. However, it's important to understand that volatile only guarantees visibility of the reference itself and not the contents of the map. This means that puts and gets on the map do not inherit the memory barrier protections provided by the volatile keyword. Before accepting this merge request, it would be prudent to review the concurrency issues of this processor. While the class may exhibit correct behavior in most cases since values are set during OnScheduled and OnStopped, these values are not safely published to the map. There is a possibility that other threads may observe stale values in the map. To address this, it might be necessary to use ConcurrentHashMap or create a fresh map by calling new HashMap() instead of using clear().;requirement_debt
The use of the volatile keyword for propertyMap is likely intended to ensure thread safety. However, it's worth noting that marking the reference as volatile does not provide inherent thread safety for the map's contents. This means that operations like puts and gets on the map do not receive the memory barrier protections associated with the volatile keyword. Before accepting this merge request, it would be advisable to review the concurrency concerns of this processor. While the class may generally behave correctly since values are set during OnScheduled and OnStopped, there is a possibility that other threads might observe stale values in the map. It may be necessary to consider using ConcurrentHashMap or creating a fresh map by calling new HashMap() instead of using clear() to address these concerns.;requirement_debt
"The decision to mark propertyMap as volatile is likely driven by the need for thread safety. However, it's important to note that marking the reference as volatile does not automatically make the operations on the map itself thread-safe. This means that the contents of the map are not ""protected"" solely by the volatile reference. In the context of this merge request, it may be prudent to conduct a review of the concurrency issues related to this processor. While the class may generally behave correctly since values are set during OnScheduled and OnStopped, there is a possibility that other threads could observe stale values in the map. To address this, it may be necessary to consider using ConcurrentHashMap or creating a fresh map by calling new HashMap() instead of using clear().";requirement_debt
The propertyMap is marked volatile to ensure visibility of the reference across multiple threads. Although the value is set only once at construction time, marking it as volatile allows other threads to see the updated value without potential stale data.;requirement_debt
If the reason for marking propertyMap as volatile is thread safety, it's important to note that simply marking the reference as volatile does not provide thread-safe access to the contents of the HashMap. The individual puts and gets on the map do not inherit the memory barrier protections associated with the volatile reference. You can refer to this Stack Overflow thread for more information: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap;requirement_debt
"It might be worth reviewing the concurrency issues of this processor before accepting the merge request. While the class may mostly behave correctly since the values are set during OnScheduled and OnStopped, there is a concern about safely publishing these values to the map. Stale values could potentially be seen by other threads, although it is unlikely. One potential solution could be to use ConcurrentHashMap instead of HashMap to ensure thread-safe access. Another option could be to republish an entirely fresh map by calling ""new HashMap()"" instead of using ""clear()"".";requirement_debt
"The volatile modifier on the propertyMap is used to ensure visibility across threads. While the value is set only once at construction time, marking it as volatile allows other threads to see the updated value without encountering stale data. However, it's important to consider the concurrency issues of this processor before merging the request. Although the class may behave correctly in most cases due to values being set during OnScheduled and OnStopped, there is a possibility of other threads seeing stale values in the map. To address this, it may be necessary to use ConcurrentHashMap or create a fresh map by calling ""new HashMap()"" instead of using ""clear()"".";requirement_debt
"The decision to mark the propertyMap as volatile is aimed at providing visibility of the reference across threads. Even though the value is set only once at construction time, using volatile ensures that other threads can see the updated value without encountering stale data. However, it's crucial to review the concurrency issues associated with this processor before accepting the merge request. While the class may generally behave correctly due to values being set during OnScheduled and OnStopped, there is a potential for other threads to observe stale values in the map. To mitigate this, alternatives such as using ConcurrentHashMap or creating a fresh map using ""new HashMap()"" instead of ""clear()"" should be considered.";requirement_debt
The propertyMap is marked as volatile because it ensures that changes made to the propertyMap by one thread are visible to other threads. Although the value is set only once at construction time, marking it as volatile guarantees the visibility of the propertyMap across threads.;requirement_debt
If the reason for marking the propertyMap as volatile is thread safety, it's important to note that the contents of the HashMap are not automatically protected just because the reference to the map is volatile. The puts and gets operations on the map do not inherit the memory barrier protections associated with the volatile reference. You can refer to this Stack Overflow post for more details: [link to Stack Overflow post].;requirement_debt
"It might be worth reviewing the concurrency issues of this processor before accepting the merge request. While the class will mostly behave correctly since values are set during OnScheduled and OnStopped, it's possible that these values are not ""safely published"" to the map. Other threads could potentially see stale values in the map, although it is unlikely. To address this, the class could consider using ConcurrentHashMap instead of HashMap, or it could republish an entirely fresh map by creating a new instance of HashMap instead of using the clear() method.";requirement_debt
The propertyMap is marked as volatile to ensure visibility of changes made to it across threads. However, it's worth considering a review of the concurrency issues in this processor before accepting the merge request. While the class may mostly behave correctly since the values are set during OnScheduled and OnStopped, there is a potential for other threads to see stale values in the map. One option to address this is to use ConcurrentHashMap instead of HashMap, or to create a new instance of HashMap instead of using the clear() method to ensure a fresh map is published.;requirement_debt
The decision to mark the propertyMap as volatile is to guarantee the visibility of changes made to it across threads. However, it would be beneficial to review the concurrency issues in this processor before merging the request. While the class may appear to behave correctly since the values are set during OnScheduled and OnStopped, there is a possibility of other threads seeing stale values in the map. To address this, the class could consider using ConcurrentHashMap or creating a new instance of HashMap instead of using the clear() method to ensure safe publication of a fresh map.;requirement_debt
The reason the propertyMap is marked as volatile is likely due to concerns about thread safety. However, it's important to note that marking the reference to the map as volatile does not provide the same memory barrier protections for the contents of the HashMap. This means that puts and gets on the map do not inherently inherit the memory barrier guarantees associated with the volatile reference. For more information on this topic, you can refer to the discussion on Stack Overflow: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap Given these considerations, it may be worth reviewing the concurrency issues of this processor before accepting the merge request. Although the class may mostly behave correctly since the values are set during OnScheduled and OnStopped, there is a possibility that other threads could observe stale values in the map. To address this, the class could potentially switch to using ConcurrentHashMap or republish an entirely fresh map by calling new HashMap() instead of using clear().;requirement_debt
The use of volatile for the propertyMap might be driven by concerns about thread safety. However, it's important to recognize that simply marking the reference as volatile does not provide the necessary memory barrier protections for the contents of the HashMap. This means that operations like puts and gets on the map do not automatically benefit from the memory synchronization guarantees associated with volatile references. A discussion on Stack Overflow provides additional insights on this topic: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap Considering the potential concurrency issues, it would be prudent to conduct a review of this processor's behavior before accepting the merge request. While the class may generally behave correctly due to values being set during OnScheduled and OnStopped, there is still a risk of other threads observing stale values in the map. To address this, the class could consider using ConcurrentHashMap or creating an entirely fresh map by invoking new HashMap() instead of using clear().;requirement_debt
The decision to mark the propertyMap as volatile is likely driven by concerns surrounding thread safety. However, it's crucial to recognize that the memory synchronization guarantees provided by volatile only apply to the reference itself, not the contents of the HashMap. Consequently, operations like puts and gets on the map do not inherently benefit from the memory barrier protections associated with volatile. You can refer to a discussion on Stack Overflow for more insights: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap Considering the concurrency issues at hand, it would be wise to conduct a thorough review of this processor before accepting the merge request. Although the class may function correctly in most cases due to the values being set during OnScheduled and OnStopped, there is still a possibility of other threads observing stale values in the map. To mitigate this risk, the class could consider using ConcurrentHashMap or creating a completely fresh map by invoking new HashMap() instead of using clear().;requirement_debt
The usage of volatile for the propertyMap is likely motivated by concerns about thread safety. However, it's important to note that applying volatile to the reference does not extend the memory barrier protections to the contents of the HashMap. This means that operations such as puts and gets on the map do not automatically benefit from the memory synchronization guarantees associated with volatile. You can refer to a discussion on Stack Overflow for further insights: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap Considering the potential concurrency issues, it would be prudent to conduct a comprehensive review of this processor prior to accepting the merge request. While the class may generally exhibit correct behavior due to the values being set during OnScheduled and OnStopped, there remains a possibility for other threads to observe stale values in the map. To address this concern, the class could consider utilizing ConcurrentHashMap or creating a fresh map entirely by invoking new HashMap() instead of using clear().;requirement_debt
The decision to mark the propertyMap as volatile is likely driven by concerns regarding thread safety. However, it's important to note that the memory barrier protections provided by volatile apply only to the reference itself, not the contents of the HashMap. Consequently, operations such as puts and gets on the map do not automatically inherit the memory synchronization guarantees associated with volatile. You can find further discussions on this topic on Stack Overflow: http://stackoverflow.com/questions/10357823/volatile-hashmap-vs-concurrenthashmap Given the potential concurrency issues, it would be advisable to conduct a thorough review of this processor before accepting the merge request. While the class may generally behave correctly due to the values being set during OnScheduled and OnStopped, there is a possibility of other threads observing stale values in the map. To address this concern, the class could consider leveraging ConcurrentHashMap or creating an entirely fresh map by using new HashMap() instead of clear().;requirement_debt
"The reason for marking the propertyMap as volatile is not clear to me. The value is only set once during construction, so volatility may not be necessary. If the purpose is to ensure thread safety, it's important to note that marking the reference to the map as volatile does not provide inherent protection to the contents of the HashMap. The puts and gets performed on the map do not inherit the memory barrier protections associated with the volatile reference. You can refer to this Stack Overflow post for more information: [link to Stack Overflow post]
It might be worth reviewing the concurrency issues of this processor before accepting the merge request. While the class may mostly behave correctly since the values are set during OnScheduled and OnStopped, they are not necessarily ""safely published"" to the map. Although it's unlikely, other threads could potentially observe stale values in this map.
To address this, the class could consider using ConcurrentHashMap instead of HashMap or republish an entirely fresh map by creating a new instance with ""new HashMap()"" instead of calling ""clear()"".";requirement_debt
"I'm uncertain about the reason for marking the propertyMap as volatile. Since the value is only set once during construction, it's possible that volatility may not be necessary. If the intention is to ensure thread safety, it's essential to understand that marking the reference to the map as volatile does not inherently protect the contents of the HashMap. Operations such as puts and gets on the map do not benefit from the memory barrier protections associated with the volatile reference. I recommend referring to this Stack Overflow post for more insights: [link to Stack Overflow post]
Before accepting this merge request, it might be prudent to review the concurrency issues of this processor. Although the class may mostly behave correctly since the values are set during OnScheduled and OnStopped, it does not guarantee ""safe publication"" to the map. While it's unlikely, there is a possibility that other threads could observe stale values in this map.
To address this concern, the class could consider using ConcurrentHashMap instead of HashMap or ensure the publication of an entirely fresh map by creating a new instance with ""new HashMap()"" instead of using ""clear()"".";requirement_debt
"I'm unsure of the rationale behind marking the propertyMap as volatile. Given that the value is only set once during construction, it's possible that volatility is not required. If the intention is to ensure thread safety, it's important to note that marking the reference to the map as volatile does not automatically protect the contents of the HashMap. Operations such as puts and gets on the map do not inherit the memory barrier protections provided by the volatile reference. You can find further information in this Stack Overflow post: [link to Stack Overflow post]
Before accepting this merge request, it might be beneficial to review the concurrency issues related to this processor. Although the class may generally behave correctly since the values are set during OnScheduled and OnStopped, it does not guarantee the ""safe publication"" of values to the map. While it's unlikely, it's conceivable that other threads may observe stale values in this map.
To address these concerns, the class could consider using ConcurrentHashMap instead of HashMap or ensure the publication of an entirely fresh map by creating a new instance using ""new HashMap()"" rather than relying on ""clear()"".";requirement_debt
"The reason behind marking the propertyMap as volatile is not apparent to me. Since the value is set only once during construction, it's possible that volatility may not be necessary. If the purpose is to ensure thread safety, it's important to understand that marking the reference to the map as volatile does not inherently protect the contents of the HashMap. Operations such as puts and gets on the map do not automatically receive the memory barrier protections provided by the volatile reference. You can find more information in this Stack Overflow post: [link to Stack Overflow post]
Before accepting this merge request, it would be prudent to review the concurrency issues associated with this processor. Although the class may mostly behave correctly due to the values being set during OnScheduled and OnStopped, it does not guarantee ""safe publication"" of values to the map. While it's unlikely, there is a possibility that other threads could observe stale values in this map.
To address this concern, the class could consider using ConcurrentHashMap instead of HashMap or ensure the publication of an entirely fresh map by creating a new instance with ""new HashMap()"" instead of relying on ""clear()"".";requirement_debt
"I think you are right that in order to break up the cyclic dependency we would need to decouple the `SchedulingResultPartition` from the `SchedulingExecutionVertex` (via the `ExecutionVertexID` for example).
I'm wondering how bad this cyclic dependency and the need for mutable state is, though. From a user's perspective, the existing interfaces are a bit easier and more convenient to use. On the down side, it makes the implementation a bit harder and harder to test in isolation. However, do we want to test the `Scheduling*` implementations in isolation? Moreover, you always have this problem in graph structures which have bidirectional edges or loops.";build_debt
Carbon-core module should not depend on spark, this PR removes this dependency;build_debt
I'm pretty sure the answer is yes: we need to include all transitive dependencies' licenses and notices. Otherwise it would be trivial to circumvent these requirements by creating a wrapper project that depends on the software and exposes all of its components.;build_debt
"This patch includes the following bug fixes:
- TopNColumnSelectorStrategyFactory: Cast dimension values to the output type
  during dimExtractionScanAndAggregate instead of updateDimExtractionResults.
  This fixes a bug where, for example, grouping on doubles-cast-to-longs would
  fail to merge two doubles that should have been combined into the same long value.
- TopNQueryEngine: Use DimExtractionTopNAlgorithm when treating string columns
  as numeric dimensions. This fixes a similar bug: grouping on string-cast-to-long
  would fail to merge two strings that should have been combined.
- GroupByQuery: Cast numeric types to the expected output type before comparing them
  in compareDimsForLimitPushDown. This fixes #6123.
- GroupByQueryQueryToolChest: Convert Jackson-deserialized dimension values into
  the proper output type. This fixes an inconsistency between results that came
  from cache vs. not-cache: for example, Jackson sometimes deserializes integers
  as Integers and sometimes as Longs.
And the following code-cleanup changes, related to the fixes above:
- DimensionHandlerUtils: Introduce convertObjectToType, compareObjectsAsType,
  and converterFromTypeToType to make it easier to handle casting operations.
- TopN in general: Rename various ""dimName"" variables to ""dimValue"" where they
  actually represent dimension values. The old names were confusing.
* Remove unused imports.";build_debt
Unused import.;build_debt
@jihoonson If you mean the actual "CompactionTask" etc classes, I think probably moving something so heavy from druid-indexing-service all the way down to druid-api would probably require collapsing a ton of druid modules into one giant druid-core module. I guess we could do that but it seems like a big change. Do you think it's worth it?;architecture_debt
Could be moved to `UpdateFeaturesResponse` as a utility.;architecture_debt
Currently in flink connector we are depending only on aws-sdk-kinesis and not on aws-java-sdk-bundle and also don't depend on kinesisvideo. So by default the dependency on kinesisvideo is not included in the connector which means we don't have to exclude any dependencies. I also verified that there is no unwanted netty dependencies by running mvn dependency:tree. The only instance of netty is this: `[INFO] |  +- org.apache.flink:flink-shaded-netty:jar:4.0.27.Final-2.0:provided` in accordance to the value in flink-parent pom.;build_debt
Temprary failures - it's good to go.;defect_debt
"@vanzin : The followup to this is #21066; I could move the compile time changes there but if you are going to have POMs playing with dependencies, seems best to have it all in one place...the other one just setting up the compile and tests
@jerryshao what do you suggest? It was your proposal to split things into pom and source for ease of reviewal, after all?";architecture_debt
We already have a test plugin in `tests/plugins/`. This is less complete than that one and should probably not be added here. We can consider moving the other one (in a separate PR) to serve as a sample.;architecture_debt
Yea, since this topic is important for some users, I mean we better move the doc into `./docs/` ( I feel novices dont seem to check the code documents).;architecture_debt
remove. this import already exists.;build_debt
Can you restructure this to use `val` - it helps to have a single block on the RHS that encapsulates the full assignment logic (as opposed to being exposed to the method's entire scope).;architecture_debt
"This PR is for code review, **this should not be merged yet.** It still needs to be debugged and the actual deployment of entities is not completed yet.  Also, it **may be missing some files** since I tore everything down and put it back together.  I'll add back the missing files (like report.go and version.go).  Sorry about that :-)
The goal here is to refactor the code to modularize it better so we can add ""big"" features.  
The major refactors are:
- refactor utils.go into separate classes
- refactor manifest and deployment functions into parsers and readers
- refactor classes into packages that are more descriptive of their functions, e.g. parsers, utils, deployers
The features added in this code:
- support for multiple packages in the service deployer.  This is mainly targeted towards multiple packages in the deployment.yaml, not the manifest.yaml
- add parameter and annotation binding from the deployment file into the deployment plan.
- add support for sequence notation per use case` openstack.`
- add placeholder for dependency specification in a package.";architecture_debt
Are you using any functions in this library? if not please get rid of this line.;build_debt
It doesn't seem necessary to put this in `RecordBatchStreamReader`, you can move it to the implementation class.;architecture_debt
Do me a favor and move both `MemSpan` and `MemArena` so the test source files are in alphabetically order.;architecture_debt
"IMO, the JMS pool from 5.x should not be migrated to Artemis.  It belongs in it's own project with it's own release cycle.  Also, it makes sense for it to *not* be in the ActiveMQ project to make clear that the pool is generic and isn't tied to any ActiveMQ broker.
The pool on messaginghub has JMS 2.0 support.";architecture_debt